{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested on python 3.8 only!\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset wmt14_translate/de-en/1.0.0 (download: 1.58 GiB, generated: Unknown size, total: 1.58 GiB) to /home/rottmann/tensorflow_datasets/wmt14_translate/de-en/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea2b8d3b1a4eb29e7d51373a42225b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ccf223489a4c5ebec933b5184c9ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2d8cb4389442f3bca9d1200885764d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ec19c26ae94a399bba273e7e1f40bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6242b79a194d9ea5d563f6b7b86dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/rottmann/tensorflow_datasets/wmt14_translate/de-en/1.0.0.incomplete82OJKW/wmt14_translate-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891dec59aab64a6d9a696364cf88e20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4508785.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbb4a14e77e4635a13ff4413bc36ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/rottmann/tensorflow_datasets/wmt14_translate/de-en/1.0.0.incomplete82OJKW/wmt14_translate-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153a013abd574004a33f5fa8c2b91c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbd068855614e798edd82f07797e443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/rottmann/tensorflow_datasets/wmt14_translate/de-en/1.0.0.incomplete82OJKW/wmt14_translate-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9d52938f744c67bd48f4dd32346e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3003.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset wmt14_translate downloaded and prepared to /home/rottmann/tensorflow_datasets/wmt14_translate/de-en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# getting training data first\n",
    "\n",
    "examples, metadata = tfds.load('wmt14_translate/de-en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for _, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_de = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (de.numpy() for de, _ in train_examples), target_vocab_size=2**13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [3970, 1803, 9, 2804, 94, 4050, 7971]\n",
      "The original string: Transformer is awesome.\n",
      "3970 ----> Trans\n",
      "1803 ----> former \n",
      "9 ----> is \n",
      "2804 ----> aw\n",
      "94 ----> es\n",
      "4050 ----> ome\n",
      "7971 ----> .\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_de.vocab_size] + tokenizer_de.encode(\n",
    "      lang1.numpy()) + [tokenizer_de.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(de, en):\n",
    "  result_de, result_en = tf.py_function(encode, [de, en], [tf.int64, tf.int64])\n",
    "  result_de.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_de, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
       " array([[8262,  329,   49, ...,    0,    0,    0],\n",
       "        [8262,   64,   75, ...,    0,    0,    0],\n",
       "        [8262, 4522,   15, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8262,  290, 4862, ...,    0,    0,    0],\n",
       "        [8262,  329,  102, ...,    0,    0,    0],\n",
       "        [8262, 1120, 2883, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
       " array([[8181, 1299,  436, ...,    0,    0,    0],\n",
       "        [8181,   52, 7860, ...,    0,    0,    0],\n",
       "        [8181, 2971,    1, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8181,   14, 1664, ...,    0,    0,    0],\n",
       "        [8181, 1299,   38, ...,    0,    0,    0],\n",
       "        [8181, 4836, 7299, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE(pos, 2i) = sin(pos/(10000^{2i/d_model}))\n",
    "# PE(pos, 2i+1)= cos(pos/(10000^{2i/d_model}))\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def test_positional_encoding():\n",
    "    POSITIONS = 80\n",
    "    DIMS = 256\n",
    "    pos_encodings = positional_encoding(POSITIONS, DIMS)\n",
    "    POS = 10\n",
    "    DIM = 11\n",
    "    assert(pos_encodings[0, 10, 11] == np.cos(POS * 1./np.power(10000, (2 * (DIM//2) / np.float32(DIMS)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "test_positional_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 0. 0. 0. 1.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# masking for padded sequence\n",
    "\n",
    "def create_padding_mask(sequence):\n",
    "    mask = tf.cast(tf.math.equal(sequence, 0), dtype=tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def test_padding_mask():\n",
    "    test_sequence = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 4, 0], [0, 1, 2, 3, 0]])\n",
    "    padding_mask = create_padding_mask(test_sequence)\n",
    "    print(padding_mask)\n",
    "    assert (tf.reduce_all(tf.math.equal(padding_mask, tf.convert_to_tensor([\n",
    "        [[[0, 0, 1, 1, 0]]],\n",
    "        [[[0, 0, 0, 0, 1]]],\n",
    "        [[[1, 0, 0, 0, 1]]]], dtype=tf.float32))))\n",
    "    \n",
    "\n",
    "test_padding_mask()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# masking for look_ahead (we don't want to look into the future!)\n",
    "def create_look_ahead_mask(size):\n",
    "    matrix = tf.ones((size,size), dtype=tf.float32)\n",
    "    return 1 - tf.linalg.band_part(matrix, -1, 0)\n",
    "\n",
    "def test_lookahead_mask():\n",
    "    lookahead_mask = create_look_ahead_mask(4)\n",
    "    print(lookahead_mask)\n",
    "    assert(tf.reduce_all(tf.math.equal(lookahead_mask, tf.convert_to_tensor([\n",
    "        [0, 1, 1, 1],\n",
    "        [0, 0, 1, 1],\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 0, 0, 0]], dtype=tf.float32))))\n",
    "\n",
    "test_lookahead_mask()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full transformer!\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_de.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "inp = None\n",
    "tar = None\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0097 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.9493 Accuracy 0.0091\n",
      "Epoch 1 Batch 100 Loss 8.8611 Accuracy 0.0305\n",
      "Epoch 1 Batch 150 Loss 8.7690 Accuracy 0.0378\n",
      "Epoch 1 Batch 200 Loss 8.6580 Accuracy 0.0416\n",
      "Epoch 1 Batch 250 Loss 8.5246 Accuracy 0.0444\n",
      "Epoch 1 Batch 300 Loss 8.3710 Accuracy 0.0507\n",
      "Epoch 1 Batch 350 Loss 8.2106 Accuracy 0.0566\n",
      "Epoch 1 Batch 400 Loss 8.0498 Accuracy 0.0612\n",
      "Epoch 1 Batch 450 Loss 7.9028 Accuracy 0.0655\n",
      "Epoch 1 Batch 500 Loss 7.7680 Accuracy 0.0703\n",
      "Epoch 1 Batch 550 Loss 7.6438 Accuracy 0.0749\n",
      "Epoch 1 Batch 600 Loss 7.5240 Accuracy 0.0794\n",
      "Epoch 1 Batch 650 Loss 7.4093 Accuracy 0.0837\n",
      "Epoch 1 Batch 700 Loss 7.3034 Accuracy 0.0876\n",
      "Epoch 1 Batch 750 Loss 7.2031 Accuracy 0.0916\n",
      "Epoch 1 Batch 800 Loss 7.1063 Accuracy 0.0957\n",
      "Epoch 1 Batch 850 Loss 7.0151 Accuracy 0.0996\n",
      "Epoch 1 Batch 900 Loss 6.9281 Accuracy 0.1034\n",
      "Epoch 1 Batch 950 Loss 6.8464 Accuracy 0.1070\n",
      "Epoch 1 Batch 1000 Loss 6.7691 Accuracy 0.1105\n",
      "Epoch 1 Batch 1050 Loss 6.6952 Accuracy 0.1140\n",
      "Epoch 1 Batch 1100 Loss 6.6265 Accuracy 0.1174\n",
      "Epoch 1 Batch 1150 Loss 6.5603 Accuracy 0.1206\n",
      "Epoch 1 Batch 1200 Loss 6.4981 Accuracy 0.1236\n",
      "Epoch 1 Batch 1250 Loss 6.4389 Accuracy 0.1264\n",
      "Epoch 1 Batch 1300 Loss 6.3823 Accuracy 0.1292\n",
      "Epoch 1 Batch 1350 Loss 6.3280 Accuracy 0.1318\n",
      "Epoch 1 Batch 1400 Loss 6.2760 Accuracy 0.1344\n",
      "Epoch 1 Batch 1450 Loss 6.2257 Accuracy 0.1369\n",
      "Epoch 1 Batch 1500 Loss 6.1781 Accuracy 0.1392\n",
      "Epoch 1 Batch 1550 Loss 6.1327 Accuracy 0.1414\n",
      "Epoch 1 Batch 1600 Loss 6.0893 Accuracy 0.1436\n",
      "Epoch 1 Batch 1650 Loss 6.0473 Accuracy 0.1457\n",
      "Epoch 1 Batch 1700 Loss 6.0071 Accuracy 0.1479\n",
      "Epoch 1 Batch 1750 Loss 5.9678 Accuracy 0.1499\n",
      "Epoch 1 Batch 1800 Loss 5.9308 Accuracy 0.1518\n",
      "Epoch 1 Batch 1850 Loss 5.8953 Accuracy 0.1537\n",
      "Epoch 1 Batch 1900 Loss 5.8598 Accuracy 0.1555\n",
      "Epoch 1 Batch 1950 Loss 5.8269 Accuracy 0.1573\n",
      "Epoch 1 Batch 2000 Loss 5.7948 Accuracy 0.1590\n",
      "Epoch 1 Batch 2050 Loss 5.7635 Accuracy 0.1606\n",
      "Epoch 1 Batch 2100 Loss 5.7328 Accuracy 0.1623\n",
      "Epoch 1 Batch 2150 Loss 5.7027 Accuracy 0.1639\n",
      "Epoch 1 Batch 2200 Loss 5.6742 Accuracy 0.1654\n",
      "Epoch 1 Batch 2250 Loss 5.6464 Accuracy 0.1669\n",
      "Epoch 1 Batch 2300 Loss 5.6200 Accuracy 0.1683\n",
      "Epoch 1 Batch 2350 Loss 5.5933 Accuracy 0.1697\n",
      "Epoch 1 Batch 2400 Loss 5.5667 Accuracy 0.1712\n",
      "Epoch 1 Batch 2450 Loss 5.5425 Accuracy 0.1726\n",
      "Epoch 1 Batch 2500 Loss 5.5179 Accuracy 0.1740\n",
      "Epoch 1 Batch 2550 Loss 5.4946 Accuracy 0.1752\n",
      "Epoch 1 Batch 2600 Loss 5.4706 Accuracy 0.1766\n",
      "Epoch 1 Batch 2650 Loss 5.4481 Accuracy 0.1780\n",
      "Epoch 1 Batch 2700 Loss 5.4256 Accuracy 0.1792\n",
      "Epoch 1 Batch 2750 Loss 5.4032 Accuracy 0.1805\n",
      "Epoch 1 Batch 2800 Loss 5.3823 Accuracy 0.1817\n",
      "Epoch 1 Batch 2850 Loss 5.3618 Accuracy 0.1828\n",
      "Epoch 1 Batch 2900 Loss 5.3408 Accuracy 0.1840\n",
      "Epoch 1 Batch 2950 Loss 5.3215 Accuracy 0.1850\n",
      "Epoch 1 Batch 3000 Loss 5.3023 Accuracy 0.1861\n",
      "Epoch 1 Batch 3050 Loss 5.2835 Accuracy 0.1872\n",
      "Epoch 1 Batch 3100 Loss 5.2644 Accuracy 0.1883\n",
      "Epoch 1 Batch 3150 Loss 5.2466 Accuracy 0.1893\n",
      "Epoch 1 Batch 3200 Loss 5.2282 Accuracy 0.1904\n",
      "Epoch 1 Batch 3250 Loss 5.2103 Accuracy 0.1915\n",
      "Epoch 1 Batch 3300 Loss 5.1923 Accuracy 0.1926\n",
      "Epoch 1 Batch 3350 Loss 5.1753 Accuracy 0.1936\n",
      "Epoch 1 Batch 3400 Loss 5.1580 Accuracy 0.1947\n",
      "Epoch 1 Batch 3450 Loss 5.1416 Accuracy 0.1957\n",
      "Epoch 1 Batch 3500 Loss 5.1244 Accuracy 0.1967\n",
      "Epoch 1 Batch 3550 Loss 5.1079 Accuracy 0.1977\n",
      "Epoch 1 Batch 3600 Loss 5.0920 Accuracy 0.1986\n",
      "Epoch 1 Batch 3650 Loss 5.0759 Accuracy 0.1995\n",
      "Epoch 1 Batch 3700 Loss 5.0602 Accuracy 0.2005\n",
      "Epoch 1 Batch 3750 Loss 5.0448 Accuracy 0.2015\n",
      "Epoch 1 Batch 3800 Loss 5.0296 Accuracy 0.2024\n",
      "Epoch 1 Batch 3850 Loss 5.0150 Accuracy 0.2033\n",
      "Epoch 1 Batch 3900 Loss 4.9997 Accuracy 0.2043\n",
      "Epoch 1 Batch 3950 Loss 4.9852 Accuracy 0.2051\n",
      "Epoch 1 Batch 4000 Loss 4.9712 Accuracy 0.2060\n",
      "Epoch 1 Batch 4050 Loss 4.9566 Accuracy 0.2069\n",
      "Epoch 1 Batch 4100 Loss 4.9429 Accuracy 0.2078\n",
      "Epoch 1 Batch 4150 Loss 4.9296 Accuracy 0.2086\n",
      "Epoch 1 Batch 4200 Loss 4.9162 Accuracy 0.2094\n",
      "Epoch 1 Batch 4250 Loss 4.9022 Accuracy 0.2104\n",
      "Epoch 1 Batch 4300 Loss 4.8887 Accuracy 0.2112\n",
      "Epoch 1 Batch 4350 Loss 4.8752 Accuracy 0.2121\n",
      "Epoch 1 Batch 4400 Loss 4.8615 Accuracy 0.2129\n",
      "Epoch 1 Batch 4450 Loss 4.8483 Accuracy 0.2138\n",
      "Epoch 1 Batch 4500 Loss 4.8353 Accuracy 0.2146\n",
      "Epoch 1 Batch 4550 Loss 4.8226 Accuracy 0.2154\n",
      "Epoch 1 Batch 4600 Loss 4.8100 Accuracy 0.2163\n",
      "Epoch 1 Batch 4650 Loss 4.7970 Accuracy 0.2171\n",
      "Epoch 1 Batch 4700 Loss 4.7844 Accuracy 0.2180\n",
      "Epoch 1 Batch 4750 Loss 4.7717 Accuracy 0.2188\n",
      "Epoch 1 Batch 4800 Loss 4.7589 Accuracy 0.2196\n",
      "Epoch 1 Batch 4850 Loss 4.7462 Accuracy 0.2205\n",
      "Epoch 1 Batch 4900 Loss 4.7341 Accuracy 0.2212\n",
      "Epoch 1 Batch 4950 Loss 4.7222 Accuracy 0.2220\n",
      "Epoch 1 Batch 5000 Loss 4.7100 Accuracy 0.2228\n",
      "Epoch 1 Batch 5050 Loss 4.6984 Accuracy 0.2236\n",
      "Epoch 1 Batch 5100 Loss 4.6864 Accuracy 0.2244\n",
      "Epoch 1 Batch 5150 Loss 4.6746 Accuracy 0.2252\n",
      "Epoch 1 Batch 5200 Loss 4.6629 Accuracy 0.2260\n",
      "Epoch 1 Batch 5250 Loss 4.6514 Accuracy 0.2268\n",
      "Epoch 1 Batch 5300 Loss 4.6396 Accuracy 0.2276\n",
      "Epoch 1 Batch 5350 Loss 4.6284 Accuracy 0.2283\n",
      "Epoch 1 Batch 5400 Loss 4.6166 Accuracy 0.2291\n",
      "Epoch 1 Batch 5450 Loss 4.6055 Accuracy 0.2299\n",
      "Epoch 1 Batch 5500 Loss 4.5949 Accuracy 0.2306\n",
      "Epoch 1 Batch 5550 Loss 4.5840 Accuracy 0.2313\n",
      "Epoch 1 Batch 5600 Loss 4.5730 Accuracy 0.2321\n",
      "Epoch 1 Batch 5650 Loss 4.5620 Accuracy 0.2329\n",
      "Epoch 1 Batch 5700 Loss 4.5512 Accuracy 0.2336\n",
      "Epoch 1 Batch 5750 Loss 4.5406 Accuracy 0.2343\n",
      "Epoch 1 Batch 5800 Loss 4.5298 Accuracy 0.2350\n",
      "Epoch 1 Batch 5850 Loss 4.5193 Accuracy 0.2358\n",
      "Epoch 1 Batch 5900 Loss 4.5092 Accuracy 0.2365\n",
      "Epoch 1 Batch 5950 Loss 4.4986 Accuracy 0.2372\n",
      "Epoch 1 Batch 6000 Loss 4.4886 Accuracy 0.2380\n",
      "Epoch 1 Batch 6050 Loss 4.4788 Accuracy 0.2387\n",
      "Epoch 1 Batch 6100 Loss 4.4687 Accuracy 0.2394\n",
      "Epoch 1 Batch 6150 Loss 4.4589 Accuracy 0.2401\n",
      "Epoch 1 Batch 6200 Loss 4.4491 Accuracy 0.2408\n",
      "Epoch 1 Batch 6250 Loss 4.4396 Accuracy 0.2414\n",
      "Epoch 1 Batch 6300 Loss 4.4298 Accuracy 0.2421\n",
      "Epoch 1 Batch 6350 Loss 4.4202 Accuracy 0.2428\n",
      "Epoch 1 Batch 6400 Loss 4.4102 Accuracy 0.2435\n",
      "Epoch 1 Batch 6450 Loss 4.4006 Accuracy 0.2441\n",
      "Epoch 1 Batch 6500 Loss 4.3910 Accuracy 0.2448\n",
      "Epoch 1 Batch 6550 Loss 4.3816 Accuracy 0.2455\n",
      "Epoch 1 Batch 6600 Loss 4.3722 Accuracy 0.2462\n",
      "Epoch 1 Batch 6650 Loss 4.3630 Accuracy 0.2468\n",
      "Epoch 1 Batch 6700 Loss 4.3538 Accuracy 0.2474\n",
      "Epoch 1 Batch 6750 Loss 4.3447 Accuracy 0.2481\n",
      "Epoch 1 Batch 6800 Loss 4.3356 Accuracy 0.2487\n",
      "Epoch 1 Batch 6850 Loss 4.3267 Accuracy 0.2494\n",
      "Epoch 1 Batch 6900 Loss 4.3180 Accuracy 0.2500\n",
      "Epoch 1 Batch 6950 Loss 4.3093 Accuracy 0.2506\n",
      "Epoch 1 Batch 7000 Loss 4.3004 Accuracy 0.2512\n",
      "Epoch 1 Batch 7050 Loss 4.2920 Accuracy 0.2519\n",
      "Epoch 1 Batch 7100 Loss 4.2835 Accuracy 0.2525\n",
      "Epoch 1 Batch 7150 Loss 4.2748 Accuracy 0.2531\n",
      "Epoch 1 Batch 7200 Loss 4.2660 Accuracy 0.2537\n",
      "Epoch 1 Batch 7250 Loss 4.2575 Accuracy 0.2543\n",
      "Epoch 1 Batch 7300 Loss 4.2491 Accuracy 0.2549\n",
      "Epoch 1 Batch 7350 Loss 4.2409 Accuracy 0.2555\n",
      "Epoch 1 Batch 7400 Loss 4.2327 Accuracy 0.2561\n",
      "Epoch 1 Batch 7450 Loss 4.2245 Accuracy 0.2567\n",
      "Epoch 1 Batch 7500 Loss 4.2164 Accuracy 0.2573\n",
      "Epoch 1 Batch 7550 Loss 4.2082 Accuracy 0.2579\n",
      "Epoch 1 Batch 7600 Loss 4.2002 Accuracy 0.2585\n",
      "Epoch 1 Batch 7650 Loss 4.1922 Accuracy 0.2590\n",
      "Epoch 1 Batch 7700 Loss 4.1844 Accuracy 0.2596\n",
      "Epoch 1 Batch 7750 Loss 4.1763 Accuracy 0.2602\n",
      "Epoch 1 Batch 7800 Loss 4.1685 Accuracy 0.2607\n",
      "Epoch 1 Batch 7850 Loss 4.1608 Accuracy 0.2613\n",
      "Epoch 1 Batch 7900 Loss 4.1533 Accuracy 0.2619\n",
      "Epoch 1 Batch 7950 Loss 4.1456 Accuracy 0.2624\n",
      "Epoch 1 Batch 8000 Loss 4.1383 Accuracy 0.2630\n",
      "Epoch 1 Batch 8050 Loss 4.1309 Accuracy 0.2635\n",
      "Epoch 1 Batch 8100 Loss 4.1236 Accuracy 0.2640\n",
      "Epoch 1 Batch 8150 Loss 4.1164 Accuracy 0.2646\n",
      "Epoch 1 Batch 8200 Loss 4.1087 Accuracy 0.2651\n",
      "Epoch 1 Batch 8250 Loss 4.1018 Accuracy 0.2656\n",
      "Epoch 1 Batch 8300 Loss 4.0945 Accuracy 0.2661\n",
      "Epoch 1 Batch 8350 Loss 4.0875 Accuracy 0.2666\n",
      "Epoch 1 Batch 8400 Loss 4.0805 Accuracy 0.2672\n",
      "Epoch 1 Batch 8450 Loss 4.0734 Accuracy 0.2677\n",
      "Epoch 1 Batch 8500 Loss 4.0666 Accuracy 0.2682\n",
      "Epoch 1 Batch 8550 Loss 4.0596 Accuracy 0.2687\n",
      "Epoch 1 Batch 8600 Loss 4.0530 Accuracy 0.2692\n",
      "Epoch 1 Batch 8650 Loss 4.0462 Accuracy 0.2697\n",
      "Epoch 1 Batch 8700 Loss 4.0393 Accuracy 0.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 8750 Loss 4.0324 Accuracy 0.2707\n",
      "Epoch 1 Batch 8800 Loss 4.0258 Accuracy 0.2712\n",
      "Epoch 1 Batch 8850 Loss 4.0193 Accuracy 0.2717\n",
      "Epoch 1 Batch 8900 Loss 4.0129 Accuracy 0.2721\n",
      "Epoch 1 Batch 8950 Loss 4.0063 Accuracy 0.2726\n",
      "Epoch 1 Batch 9000 Loss 4.0000 Accuracy 0.2731\n",
      "Epoch 1 Batch 9050 Loss 3.9937 Accuracy 0.2736\n",
      "Epoch 1 Batch 9100 Loss 3.9873 Accuracy 0.2741\n",
      "Epoch 1 Batch 9150 Loss 3.9809 Accuracy 0.2745\n",
      "Epoch 1 Batch 9200 Loss 3.9746 Accuracy 0.2750\n",
      "Epoch 1 Batch 9250 Loss 3.9682 Accuracy 0.2755\n",
      "Epoch 1 Batch 9300 Loss 3.9618 Accuracy 0.2759\n",
      "Epoch 1 Batch 9350 Loss 3.9555 Accuracy 0.2764\n",
      "Epoch 1 Batch 9400 Loss 3.9496 Accuracy 0.2769\n",
      "Epoch 1 Batch 9450 Loss 3.9438 Accuracy 0.2773\n",
      "Epoch 1 Batch 9500 Loss 3.9379 Accuracy 0.2777\n",
      "Epoch 1 Batch 9550 Loss 3.9316 Accuracy 0.2782\n",
      "Epoch 1 Batch 9600 Loss 3.9256 Accuracy 0.2786\n",
      "Epoch 1 Batch 9650 Loss 3.9197 Accuracy 0.2791\n",
      "Epoch 1 Batch 9700 Loss 3.9138 Accuracy 0.2795\n",
      "Epoch 1 Batch 9750 Loss 3.9079 Accuracy 0.2799\n",
      "Epoch 1 Batch 9800 Loss 3.9021 Accuracy 0.2803\n",
      "Epoch 1 Batch 9850 Loss 3.8964 Accuracy 0.2808\n",
      "Epoch 1 Batch 9900 Loss 3.8906 Accuracy 0.2812\n",
      "Epoch 1 Batch 9950 Loss 3.8851 Accuracy 0.2816\n",
      "Epoch 1 Batch 10000 Loss 3.8796 Accuracy 0.2820\n",
      "Epoch 1 Batch 10050 Loss 3.8740 Accuracy 0.2824\n",
      "Epoch 1 Batch 10100 Loss 3.8683 Accuracy 0.2828\n",
      "Epoch 1 Batch 10150 Loss 3.8628 Accuracy 0.2833\n",
      "Epoch 1 Batch 10200 Loss 3.8575 Accuracy 0.2837\n",
      "Epoch 1 Loss 3.8564 Accuracy 0.2837\n",
      "Time taken for 1 epoch: 1782.8434081077576 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.9663 Accuracy 0.3388\n",
      "Epoch 2 Batch 50 Loss 2.7004 Accuracy 0.3691\n",
      "Epoch 2 Batch 100 Loss 2.7275 Accuracy 0.3687\n",
      "Epoch 2 Batch 150 Loss 2.7202 Accuracy 0.3705\n",
      "Epoch 2 Batch 200 Loss 2.7175 Accuracy 0.3708\n",
      "Epoch 2 Batch 250 Loss 2.7198 Accuracy 0.3705\n",
      "Epoch 2 Batch 300 Loss 2.7196 Accuracy 0.3702\n",
      "Epoch 2 Batch 350 Loss 2.7216 Accuracy 0.3699\n",
      "Epoch 2 Batch 400 Loss 2.7262 Accuracy 0.3695\n",
      "Epoch 2 Batch 450 Loss 2.7257 Accuracy 0.3692\n",
      "Epoch 2 Batch 500 Loss 2.7243 Accuracy 0.3693\n",
      "Epoch 2 Batch 550 Loss 2.7255 Accuracy 0.3692\n",
      "Epoch 2 Batch 600 Loss 2.7257 Accuracy 0.3692\n",
      "Epoch 2 Batch 650 Loss 2.7236 Accuracy 0.3695\n",
      "Epoch 2 Batch 700 Loss 2.7224 Accuracy 0.3694\n",
      "Epoch 2 Batch 750 Loss 2.7204 Accuracy 0.3694\n",
      "Epoch 2 Batch 800 Loss 2.7171 Accuracy 0.3698\n",
      "Epoch 2 Batch 850 Loss 2.7150 Accuracy 0.3699\n",
      "Epoch 2 Batch 900 Loss 2.7139 Accuracy 0.3699\n",
      "Epoch 2 Batch 950 Loss 2.7108 Accuracy 0.3700\n",
      "Epoch 2 Batch 1000 Loss 2.7098 Accuracy 0.3701\n",
      "Epoch 2 Batch 1050 Loss 2.7070 Accuracy 0.3705\n",
      "Epoch 2 Batch 1100 Loss 2.7048 Accuracy 0.3707\n",
      "Epoch 2 Batch 1150 Loss 2.7035 Accuracy 0.3707\n",
      "Epoch 2 Batch 1200 Loss 2.7026 Accuracy 0.3708\n",
      "Epoch 2 Batch 1250 Loss 2.7002 Accuracy 0.3709\n",
      "Epoch 2 Batch 1300 Loss 2.6979 Accuracy 0.3712\n",
      "Epoch 2 Batch 1350 Loss 2.6956 Accuracy 0.3714\n",
      "Epoch 2 Batch 1400 Loss 2.6943 Accuracy 0.3714\n",
      "Epoch 2 Batch 1450 Loss 2.6926 Accuracy 0.3716\n",
      "Epoch 2 Batch 1500 Loss 2.6914 Accuracy 0.3717\n",
      "Epoch 2 Batch 1550 Loss 2.6894 Accuracy 0.3720\n",
      "Epoch 2 Batch 1600 Loss 2.6879 Accuracy 0.3721\n",
      "Epoch 2 Batch 1650 Loss 2.6864 Accuracy 0.3722\n",
      "Epoch 2 Batch 1700 Loss 2.6848 Accuracy 0.3724\n",
      "Epoch 2 Batch 1750 Loss 2.6831 Accuracy 0.3726\n",
      "Epoch 2 Batch 1800 Loss 2.6803 Accuracy 0.3728\n",
      "Epoch 2 Batch 1850 Loss 2.6778 Accuracy 0.3731\n",
      "Epoch 2 Batch 1900 Loss 2.6771 Accuracy 0.3732\n",
      "Epoch 2 Batch 1950 Loss 2.6748 Accuracy 0.3733\n",
      "Epoch 2 Batch 2000 Loss 2.6739 Accuracy 0.3732\n",
      "Epoch 2 Batch 2050 Loss 2.6726 Accuracy 0.3734\n",
      "Epoch 2 Batch 2100 Loss 2.6706 Accuracy 0.3735\n",
      "Epoch 2 Batch 2150 Loss 2.6686 Accuracy 0.3737\n",
      "Epoch 2 Batch 2200 Loss 2.6663 Accuracy 0.3740\n",
      "Epoch 2 Batch 2250 Loss 2.6654 Accuracy 0.3740\n",
      "Epoch 2 Batch 2300 Loss 2.6642 Accuracy 0.3741\n",
      "Epoch 2 Batch 2350 Loss 2.6623 Accuracy 0.3743\n",
      "Epoch 2 Batch 2400 Loss 2.6616 Accuracy 0.3744\n",
      "Epoch 2 Batch 2450 Loss 2.6601 Accuracy 0.3746\n",
      "Epoch 2 Batch 2500 Loss 2.6586 Accuracy 0.3747\n",
      "Epoch 2 Batch 2550 Loss 2.6566 Accuracy 0.3749\n",
      "Epoch 2 Batch 2600 Loss 2.6554 Accuracy 0.3750\n",
      "Epoch 2 Batch 2650 Loss 2.6540 Accuracy 0.3751\n",
      "Epoch 2 Batch 2700 Loss 2.6528 Accuracy 0.3752\n",
      "Epoch 2 Batch 2750 Loss 2.6516 Accuracy 0.3753\n",
      "Epoch 2 Batch 2800 Loss 2.6504 Accuracy 0.3754\n",
      "Epoch 2 Batch 2850 Loss 2.6497 Accuracy 0.3755\n",
      "Epoch 2 Batch 2900 Loss 2.6480 Accuracy 0.3756\n",
      "Epoch 2 Batch 2950 Loss 2.6467 Accuracy 0.3757\n",
      "Epoch 2 Batch 3000 Loss 2.6454 Accuracy 0.3758\n",
      "Epoch 2 Batch 3050 Loss 2.6438 Accuracy 0.3760\n",
      "Epoch 2 Batch 3100 Loss 2.6419 Accuracy 0.3761\n",
      "Epoch 2 Batch 3150 Loss 2.6411 Accuracy 0.3762\n",
      "Epoch 2 Batch 3200 Loss 2.6401 Accuracy 0.3763\n",
      "Epoch 2 Batch 3250 Loss 2.6385 Accuracy 0.3765\n",
      "Epoch 2 Batch 3300 Loss 2.6375 Accuracy 0.3766\n",
      "Epoch 2 Batch 3350 Loss 2.6367 Accuracy 0.3767\n",
      "Epoch 2 Batch 3400 Loss 2.6351 Accuracy 0.3768\n",
      "Epoch 2 Batch 3450 Loss 2.6333 Accuracy 0.3769\n",
      "Epoch 2 Batch 3500 Loss 2.6320 Accuracy 0.3771\n",
      "Epoch 2 Batch 3550 Loss 2.6309 Accuracy 0.3771\n",
      "Epoch 2 Batch 3600 Loss 2.6295 Accuracy 0.3772\n",
      "Epoch 2 Batch 3650 Loss 2.6281 Accuracy 0.3773\n",
      "Epoch 2 Batch 3700 Loss 2.6273 Accuracy 0.3774\n",
      "Epoch 2 Batch 3750 Loss 2.6257 Accuracy 0.3776\n",
      "Epoch 2 Batch 3800 Loss 2.6247 Accuracy 0.3776\n",
      "Epoch 2 Batch 3850 Loss 2.6239 Accuracy 0.3776\n",
      "Epoch 2 Batch 3900 Loss 2.6224 Accuracy 0.3777\n",
      "Epoch 2 Batch 3950 Loss 2.6215 Accuracy 0.3778\n",
      "Epoch 2 Batch 4000 Loss 2.6206 Accuracy 0.3779\n",
      "Epoch 2 Batch 4050 Loss 2.6196 Accuracy 0.3780\n",
      "Epoch 2 Batch 4100 Loss 2.6181 Accuracy 0.3781\n",
      "Epoch 2 Batch 4150 Loss 2.6168 Accuracy 0.3782\n",
      "Epoch 2 Batch 4200 Loss 2.6157 Accuracy 0.3783\n",
      "Epoch 2 Batch 4250 Loss 2.6142 Accuracy 0.3784\n",
      "Epoch 2 Batch 4300 Loss 2.6129 Accuracy 0.3785\n",
      "Epoch 2 Batch 4350 Loss 2.6115 Accuracy 0.3786\n",
      "Epoch 2 Batch 4400 Loss 2.6098 Accuracy 0.3788\n",
      "Epoch 2 Batch 4450 Loss 2.6087 Accuracy 0.3789\n",
      "Epoch 2 Batch 4500 Loss 2.6076 Accuracy 0.3790\n",
      "Epoch 2 Batch 4550 Loss 2.6068 Accuracy 0.3790\n",
      "Epoch 2 Batch 4600 Loss 2.6059 Accuracy 0.3791\n",
      "Epoch 2 Batch 4650 Loss 2.6050 Accuracy 0.3792\n",
      "Epoch 2 Batch 4700 Loss 2.6038 Accuracy 0.3793\n",
      "Epoch 2 Batch 4750 Loss 2.6026 Accuracy 0.3795\n",
      "Epoch 2 Batch 4800 Loss 2.6017 Accuracy 0.3795\n",
      "Epoch 2 Batch 4850 Loss 2.6003 Accuracy 0.3796\n",
      "Epoch 2 Batch 4900 Loss 2.5994 Accuracy 0.3797\n",
      "Epoch 2 Batch 4950 Loss 2.5985 Accuracy 0.3798\n",
      "Epoch 2 Batch 5000 Loss 2.5973 Accuracy 0.3799\n",
      "Epoch 2 Batch 5050 Loss 2.5963 Accuracy 0.3800\n",
      "Epoch 2 Batch 5100 Loss 2.5951 Accuracy 0.3801\n",
      "Epoch 2 Batch 5150 Loss 2.5939 Accuracy 0.3802\n",
      "Epoch 2 Batch 5200 Loss 2.5926 Accuracy 0.3803\n",
      "Epoch 2 Batch 5250 Loss 2.5911 Accuracy 0.3804\n",
      "Epoch 2 Batch 5300 Loss 2.5897 Accuracy 0.3805\n",
      "Epoch 2 Batch 5350 Loss 2.5884 Accuracy 0.3806\n",
      "Epoch 2 Batch 5400 Loss 2.5872 Accuracy 0.3807\n",
      "Epoch 2 Batch 5450 Loss 2.5859 Accuracy 0.3808\n",
      "Epoch 2 Batch 5500 Loss 2.5848 Accuracy 0.3809\n",
      "Epoch 2 Batch 5550 Loss 2.5836 Accuracy 0.3810\n",
      "Epoch 2 Batch 5600 Loss 2.5824 Accuracy 0.3811\n",
      "Epoch 2 Batch 5650 Loss 2.5815 Accuracy 0.3812\n",
      "Epoch 2 Batch 5700 Loss 2.5805 Accuracy 0.3813\n",
      "Epoch 2 Batch 5750 Loss 2.5796 Accuracy 0.3813\n",
      "Epoch 2 Batch 5800 Loss 2.5785 Accuracy 0.3814\n",
      "Epoch 2 Batch 5850 Loss 2.5776 Accuracy 0.3815\n",
      "Epoch 2 Batch 5900 Loss 2.5767 Accuracy 0.3816\n",
      "Epoch 2 Batch 5950 Loss 2.5755 Accuracy 0.3817\n",
      "Epoch 2 Batch 6000 Loss 2.5742 Accuracy 0.3818\n",
      "Epoch 2 Batch 6050 Loss 2.5729 Accuracy 0.3819\n",
      "Epoch 2 Batch 6100 Loss 2.5719 Accuracy 0.3820\n",
      "Epoch 2 Batch 6150 Loss 2.5708 Accuracy 0.3821\n",
      "Epoch 2 Batch 6200 Loss 2.5698 Accuracy 0.3822\n",
      "Epoch 2 Batch 6250 Loss 2.5691 Accuracy 0.3823\n",
      "Epoch 2 Batch 6300 Loss 2.5680 Accuracy 0.3824\n",
      "Epoch 2 Batch 6350 Loss 2.5669 Accuracy 0.3825\n",
      "Epoch 2 Batch 6400 Loss 2.5656 Accuracy 0.3826\n",
      "Epoch 2 Batch 6450 Loss 2.5643 Accuracy 0.3827\n",
      "Epoch 2 Batch 6500 Loss 2.5634 Accuracy 0.3828\n",
      "Epoch 2 Batch 6550 Loss 2.5622 Accuracy 0.3829\n",
      "Epoch 2 Batch 6600 Loss 2.5609 Accuracy 0.3830\n",
      "Epoch 2 Batch 6650 Loss 2.5598 Accuracy 0.3831\n",
      "Epoch 2 Batch 6700 Loss 2.5587 Accuracy 0.3832\n",
      "Epoch 2 Batch 6750 Loss 2.5576 Accuracy 0.3833\n",
      "Epoch 2 Batch 6800 Loss 2.5564 Accuracy 0.3834\n",
      "Epoch 2 Batch 6850 Loss 2.5555 Accuracy 0.3835\n",
      "Epoch 2 Batch 6900 Loss 2.5546 Accuracy 0.3836\n",
      "Epoch 2 Batch 6950 Loss 2.5536 Accuracy 0.3837\n",
      "Epoch 2 Batch 7000 Loss 2.5526 Accuracy 0.3837\n",
      "Epoch 2 Batch 7050 Loss 2.5519 Accuracy 0.3838\n",
      "Epoch 2 Batch 7100 Loss 2.5509 Accuracy 0.3839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 7150 Loss 2.5497 Accuracy 0.3840\n",
      "Epoch 2 Batch 7200 Loss 2.5487 Accuracy 0.3840\n",
      "Epoch 2 Batch 7250 Loss 2.5479 Accuracy 0.3841\n",
      "Epoch 2 Batch 7300 Loss 2.5471 Accuracy 0.3842\n",
      "Epoch 2 Batch 7350 Loss 2.5464 Accuracy 0.3843\n",
      "Epoch 2 Batch 7400 Loss 2.5455 Accuracy 0.3844\n",
      "Epoch 2 Batch 7450 Loss 2.5446 Accuracy 0.3845\n",
      "Epoch 2 Batch 7500 Loss 2.5437 Accuracy 0.3846\n",
      "Epoch 2 Batch 7550 Loss 2.5426 Accuracy 0.3847\n",
      "Epoch 2 Batch 7600 Loss 2.5417 Accuracy 0.3848\n",
      "Epoch 2 Batch 7650 Loss 2.5408 Accuracy 0.3848\n",
      "Epoch 2 Batch 7700 Loss 2.5396 Accuracy 0.3849\n",
      "Epoch 2 Batch 7750 Loss 2.5387 Accuracy 0.3850\n",
      "Epoch 2 Batch 7800 Loss 2.5381 Accuracy 0.3851\n",
      "Epoch 2 Batch 7850 Loss 2.5370 Accuracy 0.3852\n",
      "Epoch 2 Batch 7900 Loss 2.5361 Accuracy 0.3853\n",
      "Epoch 2 Batch 7950 Loss 2.5348 Accuracy 0.3854\n",
      "Epoch 2 Batch 8000 Loss 2.5338 Accuracy 0.3855\n",
      "Epoch 2 Batch 8050 Loss 2.5329 Accuracy 0.3855\n",
      "Epoch 2 Batch 8100 Loss 2.5318 Accuracy 0.3856\n",
      "Epoch 2 Batch 8150 Loss 2.5309 Accuracy 0.3857\n",
      "Epoch 2 Batch 8200 Loss 2.5299 Accuracy 0.3858\n",
      "Epoch 2 Batch 8250 Loss 2.5291 Accuracy 0.3859\n",
      "Epoch 2 Batch 8300 Loss 2.5283 Accuracy 0.3860\n",
      "Epoch 2 Batch 8350 Loss 2.5276 Accuracy 0.3860\n",
      "Epoch 2 Batch 8400 Loss 2.5266 Accuracy 0.3861\n",
      "Epoch 2 Batch 8450 Loss 2.5258 Accuracy 0.3862\n",
      "Epoch 2 Batch 8500 Loss 2.5247 Accuracy 0.3863\n",
      "Epoch 2 Batch 8550 Loss 2.5235 Accuracy 0.3864\n",
      "Epoch 2 Batch 8600 Loss 2.5228 Accuracy 0.3865\n",
      "Epoch 2 Batch 8650 Loss 2.5221 Accuracy 0.3866\n",
      "Epoch 2 Batch 8700 Loss 2.5211 Accuracy 0.3866\n",
      "Epoch 2 Batch 8750 Loss 2.5203 Accuracy 0.3867\n",
      "Epoch 2 Batch 8800 Loss 2.5195 Accuracy 0.3868\n",
      "Epoch 2 Batch 8850 Loss 2.5188 Accuracy 0.3869\n",
      "Epoch 2 Batch 8900 Loss 2.5177 Accuracy 0.3870\n",
      "Epoch 2 Batch 8950 Loss 2.5169 Accuracy 0.3870\n",
      "Epoch 2 Batch 9000 Loss 2.5161 Accuracy 0.3871\n",
      "Epoch 2 Batch 9050 Loss 2.5154 Accuracy 0.3872\n",
      "Epoch 2 Batch 9100 Loss 2.5147 Accuracy 0.3872\n",
      "Epoch 2 Batch 9150 Loss 2.5138 Accuracy 0.3873\n",
      "Epoch 2 Batch 9200 Loss 2.5130 Accuracy 0.3874\n",
      "Epoch 2 Batch 9250 Loss 2.5122 Accuracy 0.3874\n",
      "Epoch 2 Batch 9300 Loss 2.5115 Accuracy 0.3875\n",
      "Epoch 2 Batch 9350 Loss 2.5107 Accuracy 0.3876\n",
      "Epoch 2 Batch 9400 Loss 2.5099 Accuracy 0.3877\n",
      "Epoch 2 Batch 9450 Loss 2.5093 Accuracy 0.3877\n",
      "Epoch 2 Batch 9500 Loss 2.5085 Accuracy 0.3878\n",
      "Epoch 2 Batch 9550 Loss 2.5077 Accuracy 0.3879\n",
      "Epoch 2 Batch 9600 Loss 2.5070 Accuracy 0.3879\n",
      "Epoch 2 Batch 9650 Loss 2.5061 Accuracy 0.3880\n",
      "Epoch 2 Batch 9700 Loss 2.5053 Accuracy 0.3881\n",
      "Epoch 2 Batch 9750 Loss 2.5044 Accuracy 0.3881\n",
      "Epoch 2 Batch 9800 Loss 2.5036 Accuracy 0.3882\n",
      "Epoch 2 Batch 9850 Loss 2.5029 Accuracy 0.3883\n",
      "Epoch 2 Batch 9900 Loss 2.5020 Accuracy 0.3884\n",
      "Epoch 2 Batch 9950 Loss 2.5012 Accuracy 0.3884\n",
      "Epoch 2 Batch 10000 Loss 2.5006 Accuracy 0.3885\n",
      "Epoch 2 Batch 10050 Loss 2.4997 Accuracy 0.3886\n",
      "Epoch 2 Batch 10100 Loss 2.4988 Accuracy 0.3886\n",
      "Epoch 2 Batch 10150 Loss 2.4982 Accuracy 0.3887\n",
      "Epoch 2 Batch 10200 Loss 2.4977 Accuracy 0.3887\n",
      "Epoch 2 Loss 2.4975 Accuracy 0.3887\n",
      "Time taken for 1 epoch: 431.3853554725647 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.3305 Accuracy 0.4128\n",
      "Epoch 3 Batch 50 Loss 2.3409 Accuracy 0.4074\n",
      "Epoch 3 Batch 100 Loss 2.3262 Accuracy 0.4064\n",
      "Epoch 3 Batch 150 Loss 2.3306 Accuracy 0.4060\n",
      "Epoch 3 Batch 200 Loss 2.3357 Accuracy 0.4061\n",
      "Epoch 3 Batch 250 Loss 2.3350 Accuracy 0.4052\n",
      "Epoch 3 Batch 300 Loss 2.3329 Accuracy 0.4055\n",
      "Epoch 3 Batch 350 Loss 2.3360 Accuracy 0.4050\n",
      "Epoch 3 Batch 400 Loss 2.3329 Accuracy 0.4055\n",
      "Epoch 3 Batch 450 Loss 2.3326 Accuracy 0.4052\n",
      "Epoch 3 Batch 500 Loss 2.3296 Accuracy 0.4054\n",
      "Epoch 3 Batch 550 Loss 2.3275 Accuracy 0.4055\n",
      "Epoch 3 Batch 600 Loss 2.3306 Accuracy 0.4051\n",
      "Epoch 3 Batch 650 Loss 2.3302 Accuracy 0.4048\n",
      "Epoch 3 Batch 700 Loss 2.3283 Accuracy 0.4050\n",
      "Epoch 3 Batch 750 Loss 2.3288 Accuracy 0.4050\n",
      "Epoch 3 Batch 800 Loss 2.3299 Accuracy 0.4050\n",
      "Epoch 3 Batch 850 Loss 2.3307 Accuracy 0.4049\n",
      "Epoch 3 Batch 900 Loss 2.3304 Accuracy 0.4048\n",
      "Epoch 3 Batch 950 Loss 2.3305 Accuracy 0.4048\n",
      "Epoch 3 Batch 1000 Loss 2.3308 Accuracy 0.4045\n",
      "Epoch 3 Batch 1050 Loss 2.3305 Accuracy 0.4045\n",
      "Epoch 3 Batch 1100 Loss 2.3291 Accuracy 0.4046\n",
      "Epoch 3 Batch 1150 Loss 2.3285 Accuracy 0.4045\n",
      "Epoch 3 Batch 1200 Loss 2.3273 Accuracy 0.4046\n",
      "Epoch 3 Batch 1250 Loss 2.3277 Accuracy 0.4045\n",
      "Epoch 3 Batch 1300 Loss 2.3271 Accuracy 0.4047\n",
      "Epoch 3 Batch 1350 Loss 2.3265 Accuracy 0.4047\n",
      "Epoch 3 Batch 1400 Loss 2.3259 Accuracy 0.4047\n",
      "Epoch 3 Batch 1450 Loss 2.3255 Accuracy 0.4047\n",
      "Epoch 3 Batch 1500 Loss 2.3257 Accuracy 0.4047\n",
      "Epoch 3 Batch 1550 Loss 2.3245 Accuracy 0.4049\n",
      "Epoch 3 Batch 1600 Loss 2.3213 Accuracy 0.4050\n",
      "Epoch 3 Batch 1650 Loss 2.3212 Accuracy 0.4050\n",
      "Epoch 3 Batch 1700 Loss 2.3197 Accuracy 0.4052\n",
      "Epoch 3 Batch 1750 Loss 2.3183 Accuracy 0.4053\n",
      "Epoch 3 Batch 1800 Loss 2.3178 Accuracy 0.4054\n",
      "Epoch 3 Batch 1850 Loss 2.3170 Accuracy 0.4054\n",
      "Epoch 3 Batch 1900 Loss 2.3164 Accuracy 0.4054\n",
      "Epoch 3 Batch 1950 Loss 2.3164 Accuracy 0.4055\n",
      "Epoch 3 Batch 2000 Loss 2.3157 Accuracy 0.4056\n",
      "Epoch 3 Batch 2050 Loss 2.3154 Accuracy 0.4056\n",
      "Epoch 3 Batch 2100 Loss 2.3153 Accuracy 0.4057\n",
      "Epoch 3 Batch 2150 Loss 2.3154 Accuracy 0.4057\n",
      "Epoch 3 Batch 2200 Loss 2.3157 Accuracy 0.4057\n",
      "Epoch 3 Batch 2250 Loss 2.3150 Accuracy 0.4058\n",
      "Epoch 3 Batch 2300 Loss 2.3143 Accuracy 0.4058\n",
      "Epoch 3 Batch 2350 Loss 2.3146 Accuracy 0.4058\n",
      "Epoch 3 Batch 2400 Loss 2.3143 Accuracy 0.4058\n",
      "Epoch 3 Batch 2450 Loss 2.3142 Accuracy 0.4059\n",
      "Epoch 3 Batch 2500 Loss 2.3137 Accuracy 0.4059\n",
      "Epoch 3 Batch 2550 Loss 2.3130 Accuracy 0.4060\n",
      "Epoch 3 Batch 2600 Loss 2.3130 Accuracy 0.4060\n",
      "Epoch 3 Batch 2650 Loss 2.3130 Accuracy 0.4060\n",
      "Epoch 3 Batch 2700 Loss 2.3127 Accuracy 0.4060\n",
      "Epoch 3 Batch 2750 Loss 2.3124 Accuracy 0.4060\n",
      "Epoch 3 Batch 2800 Loss 2.3115 Accuracy 0.4061\n",
      "Epoch 3 Batch 2850 Loss 2.3113 Accuracy 0.4062\n",
      "Epoch 3 Batch 2900 Loss 2.3105 Accuracy 0.4062\n",
      "Epoch 3 Batch 2950 Loss 2.3100 Accuracy 0.4063\n",
      "Epoch 3 Batch 3000 Loss 2.3094 Accuracy 0.4064\n",
      "Epoch 3 Batch 3050 Loss 2.3091 Accuracy 0.4064\n",
      "Epoch 3 Batch 3100 Loss 2.3087 Accuracy 0.4065\n",
      "Epoch 3 Batch 3150 Loss 2.3089 Accuracy 0.4065\n",
      "Epoch 3 Batch 3200 Loss 2.3089 Accuracy 0.4065\n",
      "Epoch 3 Batch 3250 Loss 2.3086 Accuracy 0.4065\n",
      "Epoch 3 Batch 3300 Loss 2.3077 Accuracy 0.4066\n",
      "Epoch 3 Batch 3350 Loss 2.3075 Accuracy 0.4066\n",
      "Epoch 3 Batch 3400 Loss 2.3070 Accuracy 0.4066\n",
      "Epoch 3 Batch 3450 Loss 2.3065 Accuracy 0.4066\n",
      "Epoch 3 Batch 3500 Loss 2.3066 Accuracy 0.4066\n",
      "Epoch 3 Batch 3550 Loss 2.3064 Accuracy 0.4067\n",
      "Epoch 3 Batch 3600 Loss 2.3058 Accuracy 0.4067\n",
      "Epoch 3 Batch 3650 Loss 2.3053 Accuracy 0.4067\n",
      "Epoch 3 Batch 3700 Loss 2.3051 Accuracy 0.4067\n",
      "Epoch 3 Batch 3750 Loss 2.3047 Accuracy 0.4068\n",
      "Epoch 3 Batch 3800 Loss 2.3047 Accuracy 0.4067\n",
      "Epoch 3 Batch 3850 Loss 2.3043 Accuracy 0.4067\n",
      "Epoch 3 Batch 3900 Loss 2.3039 Accuracy 0.4068\n",
      "Epoch 3 Batch 3950 Loss 2.3034 Accuracy 0.4068\n",
      "Epoch 3 Batch 4000 Loss 2.3026 Accuracy 0.4069\n",
      "Epoch 3 Batch 4050 Loss 2.3022 Accuracy 0.4069\n",
      "Epoch 3 Batch 4100 Loss 2.3019 Accuracy 0.4069\n",
      "Epoch 3 Batch 4150 Loss 2.3016 Accuracy 0.4069\n",
      "Epoch 3 Batch 4200 Loss 2.3011 Accuracy 0.4070\n",
      "Epoch 3 Batch 4250 Loss 2.3001 Accuracy 0.4070\n",
      "Epoch 3 Batch 4300 Loss 2.3000 Accuracy 0.4070\n",
      "Epoch 3 Batch 4350 Loss 2.2998 Accuracy 0.4070\n",
      "Epoch 3 Batch 4400 Loss 2.2999 Accuracy 0.4071\n",
      "Epoch 3 Batch 4450 Loss 2.2996 Accuracy 0.4071\n",
      "Epoch 3 Batch 4500 Loss 2.2993 Accuracy 0.4071\n",
      "Epoch 3 Batch 4550 Loss 2.2994 Accuracy 0.4071\n",
      "Epoch 3 Batch 4600 Loss 2.2988 Accuracy 0.4072\n",
      "Epoch 3 Batch 4650 Loss 2.2982 Accuracy 0.4073\n",
      "Epoch 3 Batch 4700 Loss 2.2974 Accuracy 0.4074\n",
      "Epoch 3 Batch 4750 Loss 2.2967 Accuracy 0.4075\n",
      "Epoch 3 Batch 4800 Loss 2.2961 Accuracy 0.4075\n",
      "Epoch 3 Batch 4850 Loss 2.2957 Accuracy 0.4075\n",
      "Epoch 3 Batch 4900 Loss 2.2951 Accuracy 0.4076\n",
      "Epoch 3 Batch 4950 Loss 2.2948 Accuracy 0.4076\n",
      "Epoch 3 Batch 5000 Loss 2.2944 Accuracy 0.4076\n",
      "Epoch 3 Batch 5050 Loss 2.2940 Accuracy 0.4076\n",
      "Epoch 3 Batch 5100 Loss 2.2940 Accuracy 0.4076\n",
      "Epoch 3 Batch 5150 Loss 2.2932 Accuracy 0.4077\n",
      "Epoch 3 Batch 5200 Loss 2.2927 Accuracy 0.4077\n",
      "Epoch 3 Batch 5250 Loss 2.2924 Accuracy 0.4077\n",
      "Epoch 3 Batch 5300 Loss 2.2916 Accuracy 0.4078\n",
      "Epoch 3 Batch 5350 Loss 2.2912 Accuracy 0.4078\n",
      "Epoch 3 Batch 5400 Loss 2.2908 Accuracy 0.4078\n",
      "Epoch 3 Batch 5450 Loss 2.2902 Accuracy 0.4079\n",
      "Epoch 3 Batch 5500 Loss 2.2897 Accuracy 0.4079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 5550 Loss 2.2896 Accuracy 0.4079\n",
      "Epoch 3 Batch 5600 Loss 2.2894 Accuracy 0.4079\n",
      "Epoch 3 Batch 5650 Loss 2.2890 Accuracy 0.4080\n",
      "Epoch 3 Batch 5700 Loss 2.2885 Accuracy 0.4080\n",
      "Epoch 3 Batch 5750 Loss 2.2880 Accuracy 0.4081\n",
      "Epoch 3 Batch 5800 Loss 2.2874 Accuracy 0.4081\n",
      "Epoch 3 Batch 5850 Loss 2.2875 Accuracy 0.4081\n",
      "Epoch 3 Batch 5900 Loss 2.2870 Accuracy 0.4081\n",
      "Epoch 3 Batch 5950 Loss 2.2862 Accuracy 0.4082\n",
      "Epoch 3 Batch 6000 Loss 2.2859 Accuracy 0.4082\n",
      "Epoch 3 Batch 6050 Loss 2.2851 Accuracy 0.4083\n",
      "Epoch 3 Batch 6100 Loss 2.2847 Accuracy 0.4084\n",
      "Epoch 3 Batch 6150 Loss 2.2842 Accuracy 0.4084\n",
      "Epoch 3 Batch 6200 Loss 2.2842 Accuracy 0.4084\n",
      "Epoch 3 Batch 6250 Loss 2.2838 Accuracy 0.4085\n",
      "Epoch 3 Batch 6300 Loss 2.2834 Accuracy 0.4085\n",
      "Epoch 3 Batch 6350 Loss 2.2828 Accuracy 0.4086\n",
      "Epoch 3 Batch 6400 Loss 2.2821 Accuracy 0.4086\n",
      "Epoch 3 Batch 6450 Loss 2.2818 Accuracy 0.4087\n",
      "Epoch 3 Batch 6500 Loss 2.2813 Accuracy 0.4087\n",
      "Epoch 3 Batch 6550 Loss 2.2809 Accuracy 0.4087\n",
      "Epoch 3 Batch 6600 Loss 2.2802 Accuracy 0.4088\n",
      "Epoch 3 Batch 6650 Loss 2.2796 Accuracy 0.4089\n",
      "Epoch 3 Batch 6700 Loss 2.2794 Accuracy 0.4089\n",
      "Epoch 3 Batch 6750 Loss 2.2789 Accuracy 0.4090\n",
      "Epoch 3 Batch 6800 Loss 2.2786 Accuracy 0.4090\n",
      "Epoch 3 Batch 6850 Loss 2.2780 Accuracy 0.4091\n",
      "Epoch 3 Batch 6900 Loss 2.2775 Accuracy 0.4091\n",
      "Epoch 3 Batch 6950 Loss 2.2773 Accuracy 0.4091\n",
      "Epoch 3 Batch 7000 Loss 2.2771 Accuracy 0.4092\n",
      "Epoch 3 Batch 7050 Loss 2.2765 Accuracy 0.4092\n",
      "Epoch 3 Batch 7100 Loss 2.2762 Accuracy 0.4092\n",
      "Epoch 3 Batch 7150 Loss 2.2758 Accuracy 0.4093\n",
      "Epoch 3 Batch 7200 Loss 2.2754 Accuracy 0.4093\n",
      "Epoch 3 Batch 7250 Loss 2.2751 Accuracy 0.4093\n",
      "Epoch 3 Batch 7300 Loss 2.2750 Accuracy 0.4093\n",
      "Epoch 3 Batch 7350 Loss 2.2744 Accuracy 0.4093\n",
      "Epoch 3 Batch 7400 Loss 2.2740 Accuracy 0.4093\n",
      "Epoch 3 Batch 7450 Loss 2.2735 Accuracy 0.4094\n",
      "Epoch 3 Batch 7500 Loss 2.2733 Accuracy 0.4094\n",
      "Epoch 3 Batch 7550 Loss 2.2728 Accuracy 0.4095\n",
      "Epoch 3 Batch 7600 Loss 2.2726 Accuracy 0.4095\n",
      "Epoch 3 Batch 7650 Loss 2.2722 Accuracy 0.4095\n",
      "Epoch 3 Batch 7700 Loss 2.2720 Accuracy 0.4095\n",
      "Epoch 3 Batch 7750 Loss 2.2716 Accuracy 0.4096\n",
      "Epoch 3 Batch 7800 Loss 2.2710 Accuracy 0.4097\n",
      "Epoch 3 Batch 7850 Loss 2.2706 Accuracy 0.4097\n",
      "Epoch 3 Batch 7900 Loss 2.2702 Accuracy 0.4098\n",
      "Epoch 3 Batch 7950 Loss 2.2699 Accuracy 0.4098\n",
      "Epoch 3 Batch 8000 Loss 2.2694 Accuracy 0.4099\n",
      "Epoch 3 Batch 8050 Loss 2.2689 Accuracy 0.4099\n",
      "Epoch 3 Batch 8100 Loss 2.2684 Accuracy 0.4099\n",
      "Epoch 3 Batch 8150 Loss 2.2679 Accuracy 0.4100\n",
      "Epoch 3 Batch 8200 Loss 2.2676 Accuracy 0.4100\n",
      "Epoch 3 Batch 8250 Loss 2.2670 Accuracy 0.4100\n",
      "Epoch 3 Batch 8300 Loss 2.2667 Accuracy 0.4101\n",
      "Epoch 3 Batch 8350 Loss 2.2663 Accuracy 0.4101\n",
      "Epoch 3 Batch 8400 Loss 2.2659 Accuracy 0.4101\n",
      "Epoch 3 Batch 8450 Loss 2.2656 Accuracy 0.4102\n",
      "Epoch 3 Batch 8500 Loss 2.2652 Accuracy 0.4102\n",
      "Epoch 3 Batch 8550 Loss 2.2650 Accuracy 0.4102\n",
      "Epoch 3 Batch 8600 Loss 2.2647 Accuracy 0.4102\n",
      "Epoch 3 Batch 8650 Loss 2.2642 Accuracy 0.4103\n",
      "Epoch 3 Batch 8700 Loss 2.2639 Accuracy 0.4103\n",
      "Epoch 3 Batch 8750 Loss 2.2635 Accuracy 0.4103\n",
      "Epoch 3 Batch 8800 Loss 2.2632 Accuracy 0.4104\n",
      "Epoch 3 Batch 8850 Loss 2.2627 Accuracy 0.4104\n",
      "Epoch 3 Batch 8900 Loss 2.2626 Accuracy 0.4105\n",
      "Epoch 3 Batch 8950 Loss 2.2620 Accuracy 0.4105\n",
      "Epoch 3 Batch 9000 Loss 2.2615 Accuracy 0.4105\n",
      "Epoch 3 Batch 9050 Loss 2.2613 Accuracy 0.4105\n",
      "Epoch 3 Batch 9100 Loss 2.2611 Accuracy 0.4106\n",
      "Epoch 3 Batch 9150 Loss 2.2609 Accuracy 0.4106\n",
      "Epoch 3 Batch 9200 Loss 2.2607 Accuracy 0.4106\n",
      "Epoch 3 Batch 9250 Loss 2.2605 Accuracy 0.4106\n",
      "Epoch 3 Batch 9300 Loss 2.2601 Accuracy 0.4107\n",
      "Epoch 3 Batch 9350 Loss 2.2599 Accuracy 0.4107\n",
      "Epoch 3 Batch 9400 Loss 2.2595 Accuracy 0.4107\n",
      "Epoch 3 Batch 9450 Loss 2.2591 Accuracy 0.4108\n",
      "Epoch 3 Batch 9500 Loss 2.2589 Accuracy 0.4108\n",
      "Epoch 3 Batch 9550 Loss 2.2587 Accuracy 0.4108\n",
      "Epoch 3 Batch 9600 Loss 2.2585 Accuracy 0.4108\n",
      "Epoch 3 Batch 9650 Loss 2.2582 Accuracy 0.4109\n",
      "Epoch 3 Batch 9700 Loss 2.2578 Accuracy 0.4109\n",
      "Epoch 3 Batch 9750 Loss 2.2575 Accuracy 0.4109\n",
      "Epoch 3 Batch 9800 Loss 2.2573 Accuracy 0.4109\n",
      "Epoch 3 Batch 9850 Loss 2.2567 Accuracy 0.4110\n",
      "Epoch 3 Batch 9900 Loss 2.2564 Accuracy 0.4110\n",
      "Epoch 3 Batch 9950 Loss 2.2560 Accuracy 0.4110\n",
      "Epoch 3 Batch 10000 Loss 2.2558 Accuracy 0.4111\n",
      "Epoch 3 Batch 10050 Loss 2.2554 Accuracy 0.4111\n",
      "Epoch 3 Batch 10100 Loss 2.2552 Accuracy 0.4111\n",
      "Epoch 3 Batch 10150 Loss 2.2548 Accuracy 0.4112\n",
      "Epoch 3 Batch 10200 Loss 2.2546 Accuracy 0.4112\n",
      "Epoch 3 Loss 2.2545 Accuracy 0.4112\n",
      "Time taken for 1 epoch: 431.95190620422363 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.1277 Accuracy 0.4350\n",
      "Epoch 4 Batch 50 Loss 2.1831 Accuracy 0.4166\n",
      "Epoch 4 Batch 100 Loss 2.1644 Accuracy 0.4183\n",
      "Epoch 4 Batch 150 Loss 2.1701 Accuracy 0.4186\n",
      "Epoch 4 Batch 200 Loss 2.1722 Accuracy 0.4185\n",
      "Epoch 4 Batch 250 Loss 2.1746 Accuracy 0.4190\n",
      "Epoch 4 Batch 300 Loss 2.1751 Accuracy 0.4188\n",
      "Epoch 4 Batch 350 Loss 2.1757 Accuracy 0.4191\n",
      "Epoch 4 Batch 400 Loss 2.1791 Accuracy 0.4186\n",
      "Epoch 4 Batch 450 Loss 2.1839 Accuracy 0.4181\n",
      "Epoch 4 Batch 500 Loss 2.1873 Accuracy 0.4182\n",
      "Epoch 4 Batch 550 Loss 2.1853 Accuracy 0.4182\n",
      "Epoch 4 Batch 600 Loss 2.1856 Accuracy 0.4182\n",
      "Epoch 4 Batch 650 Loss 2.1860 Accuracy 0.4182\n",
      "Epoch 4 Batch 700 Loss 2.1858 Accuracy 0.4183\n",
      "Epoch 4 Batch 750 Loss 2.1855 Accuracy 0.4181\n",
      "Epoch 4 Batch 800 Loss 2.1846 Accuracy 0.4185\n",
      "Epoch 4 Batch 850 Loss 2.1846 Accuracy 0.4185\n",
      "Epoch 4 Batch 900 Loss 2.1844 Accuracy 0.4182\n",
      "Epoch 4 Batch 950 Loss 2.1863 Accuracy 0.4179\n",
      "Epoch 4 Batch 1000 Loss 2.1842 Accuracy 0.4183\n",
      "Epoch 4 Batch 1050 Loss 2.1834 Accuracy 0.4185\n",
      "Epoch 4 Batch 1100 Loss 2.1824 Accuracy 0.4187\n",
      "Epoch 4 Batch 1150 Loss 2.1823 Accuracy 0.4188\n",
      "Epoch 4 Batch 1200 Loss 2.1836 Accuracy 0.4186\n",
      "Epoch 4 Batch 1250 Loss 2.1843 Accuracy 0.4184\n",
      "Epoch 4 Batch 1300 Loss 2.1828 Accuracy 0.4186\n",
      "Epoch 4 Batch 1350 Loss 2.1824 Accuracy 0.4185\n",
      "Epoch 4 Batch 1400 Loss 2.1807 Accuracy 0.4187\n",
      "Epoch 4 Batch 1450 Loss 2.1800 Accuracy 0.4187\n",
      "Epoch 4 Batch 1500 Loss 2.1794 Accuracy 0.4188\n",
      "Epoch 4 Batch 1550 Loss 2.1799 Accuracy 0.4188\n",
      "Epoch 4 Batch 1600 Loss 2.1793 Accuracy 0.4188\n",
      "Epoch 4 Batch 1650 Loss 2.1780 Accuracy 0.4189\n",
      "Epoch 4 Batch 1700 Loss 2.1766 Accuracy 0.4190\n",
      "Epoch 4 Batch 1750 Loss 2.1770 Accuracy 0.4189\n",
      "Epoch 4 Batch 1800 Loss 2.1775 Accuracy 0.4189\n",
      "Epoch 4 Batch 1850 Loss 2.1762 Accuracy 0.4190\n",
      "Epoch 4 Batch 1900 Loss 2.1752 Accuracy 0.4191\n",
      "Epoch 4 Batch 1950 Loss 2.1744 Accuracy 0.4192\n",
      "Epoch 4 Batch 2000 Loss 2.1741 Accuracy 0.4192\n",
      "Epoch 4 Batch 2050 Loss 2.1740 Accuracy 0.4192\n",
      "Epoch 4 Batch 2100 Loss 2.1734 Accuracy 0.4192\n",
      "Epoch 4 Batch 2150 Loss 2.1733 Accuracy 0.4192\n",
      "Epoch 4 Batch 2200 Loss 2.1729 Accuracy 0.4193\n",
      "Epoch 4 Batch 2250 Loss 2.1729 Accuracy 0.4193\n",
      "Epoch 4 Batch 2300 Loss 2.1718 Accuracy 0.4193\n",
      "Epoch 4 Batch 2350 Loss 2.1716 Accuracy 0.4194\n",
      "Epoch 4 Batch 2400 Loss 2.1721 Accuracy 0.4194\n",
      "Epoch 4 Batch 2450 Loss 2.1725 Accuracy 0.4194\n",
      "Epoch 4 Batch 2500 Loss 2.1733 Accuracy 0.4194\n",
      "Epoch 4 Batch 2550 Loss 2.1726 Accuracy 0.4194\n",
      "Epoch 4 Batch 2600 Loss 2.1729 Accuracy 0.4194\n",
      "Epoch 4 Batch 2650 Loss 2.1728 Accuracy 0.4194\n",
      "Epoch 4 Batch 2700 Loss 2.1730 Accuracy 0.4194\n",
      "Epoch 4 Batch 2750 Loss 2.1731 Accuracy 0.4194\n",
      "Epoch 4 Batch 2800 Loss 2.1730 Accuracy 0.4194\n",
      "Epoch 4 Batch 2850 Loss 2.1732 Accuracy 0.4194\n",
      "Epoch 4 Batch 2900 Loss 2.1726 Accuracy 0.4195\n",
      "Epoch 4 Batch 2950 Loss 2.1721 Accuracy 0.4195\n",
      "Epoch 4 Batch 3000 Loss 2.1717 Accuracy 0.4196\n",
      "Epoch 4 Batch 3050 Loss 2.1719 Accuracy 0.4196\n",
      "Epoch 4 Batch 3100 Loss 2.1717 Accuracy 0.4197\n",
      "Epoch 4 Batch 3150 Loss 2.1715 Accuracy 0.4197\n",
      "Epoch 4 Batch 3200 Loss 2.1713 Accuracy 0.4197\n",
      "Epoch 4 Batch 3250 Loss 2.1707 Accuracy 0.4197\n",
      "Epoch 4 Batch 3300 Loss 2.1708 Accuracy 0.4197\n",
      "Epoch 4 Batch 3350 Loss 2.1708 Accuracy 0.4197\n",
      "Epoch 4 Batch 3400 Loss 2.1706 Accuracy 0.4197\n",
      "Epoch 4 Batch 3450 Loss 2.1704 Accuracy 0.4198\n",
      "Epoch 4 Batch 3500 Loss 2.1705 Accuracy 0.4198\n",
      "Epoch 4 Batch 3550 Loss 2.1705 Accuracy 0.4198\n",
      "Epoch 4 Batch 3600 Loss 2.1701 Accuracy 0.4198\n",
      "Epoch 4 Batch 3650 Loss 2.1703 Accuracy 0.4198\n",
      "Epoch 4 Batch 3700 Loss 2.1701 Accuracy 0.4198\n",
      "Epoch 4 Batch 3750 Loss 2.1698 Accuracy 0.4198\n",
      "Epoch 4 Batch 3800 Loss 2.1693 Accuracy 0.4199\n",
      "Epoch 4 Batch 3850 Loss 2.1694 Accuracy 0.4198\n",
      "Epoch 4 Batch 3900 Loss 2.1689 Accuracy 0.4199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 3950 Loss 2.1687 Accuracy 0.4199\n",
      "Epoch 4 Batch 4000 Loss 2.1687 Accuracy 0.4199\n",
      "Epoch 4 Batch 4050 Loss 2.1686 Accuracy 0.4199\n",
      "Epoch 4 Batch 4100 Loss 2.1684 Accuracy 0.4199\n",
      "Epoch 4 Batch 4150 Loss 2.1682 Accuracy 0.4199\n",
      "Epoch 4 Batch 4200 Loss 2.1680 Accuracy 0.4199\n",
      "Epoch 4 Batch 4250 Loss 2.1677 Accuracy 0.4199\n",
      "Epoch 4 Batch 4300 Loss 2.1671 Accuracy 0.4199\n",
      "Epoch 4 Batch 4350 Loss 2.1668 Accuracy 0.4200\n",
      "Epoch 4 Batch 4400 Loss 2.1668 Accuracy 0.4200\n",
      "Epoch 4 Batch 4450 Loss 2.1669 Accuracy 0.4200\n",
      "Epoch 4 Batch 4500 Loss 2.1668 Accuracy 0.4200\n",
      "Epoch 4 Batch 4550 Loss 2.1662 Accuracy 0.4200\n",
      "Epoch 4 Batch 4600 Loss 2.1659 Accuracy 0.4200\n",
      "Epoch 4 Batch 4650 Loss 2.1661 Accuracy 0.4200\n",
      "Epoch 4 Batch 4700 Loss 2.1662 Accuracy 0.4201\n",
      "Epoch 4 Batch 4750 Loss 2.1657 Accuracy 0.4201\n",
      "Epoch 4 Batch 4800 Loss 2.1651 Accuracy 0.4201\n",
      "Epoch 4 Batch 4850 Loss 2.1651 Accuracy 0.4201\n",
      "Epoch 4 Batch 4900 Loss 2.1648 Accuracy 0.4202\n",
      "Epoch 4 Batch 4950 Loss 2.1645 Accuracy 0.4202\n",
      "Epoch 4 Batch 5000 Loss 2.1643 Accuracy 0.4202\n",
      "Epoch 4 Batch 5050 Loss 2.1640 Accuracy 0.4202\n",
      "Epoch 4 Batch 5100 Loss 2.1638 Accuracy 0.4203\n",
      "Epoch 4 Batch 5150 Loss 2.1633 Accuracy 0.4203\n",
      "Epoch 4 Batch 5200 Loss 2.1627 Accuracy 0.4203\n",
      "Epoch 4 Batch 5250 Loss 2.1626 Accuracy 0.4202\n",
      "Epoch 4 Batch 5300 Loss 2.1625 Accuracy 0.4203\n",
      "Epoch 4 Batch 5350 Loss 2.1620 Accuracy 0.4203\n",
      "Epoch 4 Batch 5400 Loss 2.1616 Accuracy 0.4203\n",
      "Epoch 4 Batch 5450 Loss 2.1613 Accuracy 0.4203\n",
      "Epoch 4 Batch 5500 Loss 2.1612 Accuracy 0.4203\n",
      "Epoch 4 Batch 5550 Loss 2.1609 Accuracy 0.4203\n",
      "Epoch 4 Batch 5600 Loss 2.1606 Accuracy 0.4203\n",
      "Epoch 4 Batch 5650 Loss 2.1606 Accuracy 0.4204\n",
      "Epoch 4 Batch 5700 Loss 2.1606 Accuracy 0.4203\n",
      "Epoch 4 Batch 5750 Loss 2.1602 Accuracy 0.4204\n",
      "Epoch 4 Batch 5800 Loss 2.1599 Accuracy 0.4204\n",
      "Epoch 4 Batch 5850 Loss 2.1595 Accuracy 0.4204\n",
      "Epoch 4 Batch 5900 Loss 2.1593 Accuracy 0.4205\n",
      "Epoch 4 Batch 5950 Loss 2.1589 Accuracy 0.4205\n",
      "Epoch 4 Batch 6000 Loss 2.1587 Accuracy 0.4205\n",
      "Epoch 4 Batch 6050 Loss 2.1584 Accuracy 0.4205\n",
      "Epoch 4 Batch 6100 Loss 2.1581 Accuracy 0.4205\n",
      "Epoch 4 Batch 6150 Loss 2.1579 Accuracy 0.4206\n",
      "Epoch 4 Batch 6200 Loss 2.1576 Accuracy 0.4206\n",
      "Epoch 4 Batch 6250 Loss 2.1573 Accuracy 0.4207\n",
      "Epoch 4 Batch 6300 Loss 2.1570 Accuracy 0.4207\n",
      "Epoch 4 Batch 6350 Loss 2.1568 Accuracy 0.4208\n",
      "Epoch 4 Batch 6400 Loss 2.1565 Accuracy 0.4208\n",
      "Epoch 4 Batch 6450 Loss 2.1564 Accuracy 0.4208\n",
      "Epoch 4 Batch 6500 Loss 2.1559 Accuracy 0.4209\n",
      "Epoch 4 Batch 6550 Loss 2.1557 Accuracy 0.4209\n",
      "Epoch 4 Batch 6600 Loss 2.1553 Accuracy 0.4209\n",
      "Epoch 4 Batch 6650 Loss 2.1552 Accuracy 0.4209\n",
      "Epoch 4 Batch 6700 Loss 2.1552 Accuracy 0.4210\n",
      "Epoch 4 Batch 6750 Loss 2.1549 Accuracy 0.4210\n",
      "Epoch 4 Batch 6800 Loss 2.1548 Accuracy 0.4210\n",
      "Epoch 4 Batch 6850 Loss 2.1543 Accuracy 0.4211\n",
      "Epoch 4 Batch 6900 Loss 2.1540 Accuracy 0.4211\n",
      "Epoch 4 Batch 6950 Loss 2.1539 Accuracy 0.4211\n",
      "Epoch 4 Batch 7000 Loss 2.1539 Accuracy 0.4211\n",
      "Epoch 4 Batch 7050 Loss 2.1538 Accuracy 0.4210\n",
      "Epoch 4 Batch 7100 Loss 2.1536 Accuracy 0.4211\n",
      "Epoch 4 Batch 7150 Loss 2.1533 Accuracy 0.4211\n",
      "Epoch 4 Batch 7200 Loss 2.1533 Accuracy 0.4211\n",
      "Epoch 4 Batch 7250 Loss 2.1532 Accuracy 0.4211\n",
      "Epoch 4 Batch 7300 Loss 2.1528 Accuracy 0.4211\n",
      "Epoch 4 Batch 7350 Loss 2.1526 Accuracy 0.4212\n",
      "Epoch 4 Batch 7400 Loss 2.1524 Accuracy 0.4212\n",
      "Epoch 4 Batch 7450 Loss 2.1521 Accuracy 0.4213\n",
      "Epoch 4 Batch 7500 Loss 2.1516 Accuracy 0.4213\n",
      "Epoch 4 Batch 7550 Loss 2.1514 Accuracy 0.4213\n",
      "Epoch 4 Batch 7600 Loss 2.1513 Accuracy 0.4214\n",
      "Epoch 4 Batch 7650 Loss 2.1509 Accuracy 0.4214\n",
      "Epoch 4 Batch 7700 Loss 2.1507 Accuracy 0.4214\n",
      "Epoch 4 Batch 7750 Loss 2.1504 Accuracy 0.4214\n",
      "Epoch 4 Batch 7800 Loss 2.1501 Accuracy 0.4214\n",
      "Epoch 4 Batch 7850 Loss 2.1499 Accuracy 0.4215\n",
      "Epoch 4 Batch 7900 Loss 2.1496 Accuracy 0.4215\n",
      "Epoch 4 Batch 7950 Loss 2.1493 Accuracy 0.4216\n",
      "Epoch 4 Batch 8000 Loss 2.1488 Accuracy 0.4216\n",
      "Epoch 4 Batch 8050 Loss 2.1487 Accuracy 0.4216\n",
      "Epoch 4 Batch 8100 Loss 2.1482 Accuracy 0.4217\n",
      "Epoch 4 Batch 8150 Loss 2.1482 Accuracy 0.4217\n",
      "Epoch 4 Batch 8200 Loss 2.1480 Accuracy 0.4217\n",
      "Epoch 4 Batch 8250 Loss 2.1477 Accuracy 0.4217\n",
      "Epoch 4 Batch 8300 Loss 2.1475 Accuracy 0.4217\n",
      "Epoch 4 Batch 8350 Loss 2.1473 Accuracy 0.4217\n",
      "Epoch 4 Batch 8400 Loss 2.1472 Accuracy 0.4217\n",
      "Epoch 4 Batch 8450 Loss 2.1466 Accuracy 0.4218\n",
      "Epoch 4 Batch 8500 Loss 2.1463 Accuracy 0.4218\n",
      "Epoch 4 Batch 8550 Loss 2.1461 Accuracy 0.4218\n",
      "Epoch 4 Batch 8600 Loss 2.1460 Accuracy 0.4218\n",
      "Epoch 4 Batch 8650 Loss 2.1457 Accuracy 0.4219\n",
      "Epoch 4 Batch 8700 Loss 2.1454 Accuracy 0.4219\n",
      "Epoch 4 Batch 8750 Loss 2.1450 Accuracy 0.4220\n",
      "Epoch 4 Batch 8800 Loss 2.1450 Accuracy 0.4220\n",
      "Epoch 4 Batch 8850 Loss 2.1451 Accuracy 0.4220\n",
      "Epoch 4 Batch 8900 Loss 2.1451 Accuracy 0.4220\n",
      "Epoch 4 Batch 8950 Loss 2.1448 Accuracy 0.4220\n",
      "Epoch 4 Batch 9000 Loss 2.1444 Accuracy 0.4220\n",
      "Epoch 4 Batch 9050 Loss 2.1444 Accuracy 0.4220\n",
      "Epoch 4 Batch 9100 Loss 2.1441 Accuracy 0.4220\n",
      "Epoch 4 Batch 9150 Loss 2.1442 Accuracy 0.4220\n",
      "Epoch 4 Batch 9200 Loss 2.1438 Accuracy 0.4221\n",
      "Epoch 4 Batch 9250 Loss 2.1437 Accuracy 0.4221\n",
      "Epoch 4 Batch 9300 Loss 2.1435 Accuracy 0.4221\n",
      "Epoch 4 Batch 9350 Loss 2.1434 Accuracy 0.4221\n",
      "Epoch 4 Batch 9400 Loss 2.1430 Accuracy 0.4221\n",
      "Epoch 4 Batch 9450 Loss 2.1429 Accuracy 0.4221\n",
      "Epoch 4 Batch 9500 Loss 2.1426 Accuracy 0.4222\n",
      "Epoch 4 Batch 9550 Loss 2.1425 Accuracy 0.4222\n",
      "Epoch 4 Batch 9600 Loss 2.1424 Accuracy 0.4222\n",
      "Epoch 4 Batch 9650 Loss 2.1424 Accuracy 0.4222\n",
      "Epoch 4 Batch 9700 Loss 2.1423 Accuracy 0.4222\n",
      "Epoch 4 Batch 9750 Loss 2.1422 Accuracy 0.4222\n",
      "Epoch 4 Batch 9800 Loss 2.1422 Accuracy 0.4222\n",
      "Epoch 4 Batch 9850 Loss 2.1419 Accuracy 0.4222\n",
      "Epoch 4 Batch 9900 Loss 2.1419 Accuracy 0.4222\n",
      "Epoch 4 Batch 9950 Loss 2.1417 Accuracy 0.4222\n",
      "Epoch 4 Batch 10000 Loss 2.1412 Accuracy 0.4223\n",
      "Epoch 4 Batch 10050 Loss 2.1412 Accuracy 0.4223\n",
      "Epoch 4 Batch 10100 Loss 2.1410 Accuracy 0.4223\n",
      "Epoch 4 Batch 10150 Loss 2.1406 Accuracy 0.4223\n",
      "Epoch 4 Batch 10200 Loss 2.1404 Accuracy 0.4224\n",
      "Epoch 4 Loss 2.1404 Accuracy 0.4224\n",
      "Time taken for 1 epoch: 432.56174206733704 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.1645 Accuracy 0.3997\n",
      "Epoch 5 Batch 50 Loss 2.1090 Accuracy 0.4282\n",
      "Epoch 5 Batch 100 Loss 2.1046 Accuracy 0.4269\n",
      "Epoch 5 Batch 150 Loss 2.0977 Accuracy 0.4266\n",
      "Epoch 5 Batch 200 Loss 2.0959 Accuracy 0.4264\n",
      "Epoch 5 Batch 250 Loss 2.0940 Accuracy 0.4269\n",
      "Epoch 5 Batch 300 Loss 2.0968 Accuracy 0.4265\n",
      "Epoch 5 Batch 350 Loss 2.0939 Accuracy 0.4268\n",
      "Epoch 5 Batch 400 Loss 2.0967 Accuracy 0.4270\n",
      "Epoch 5 Batch 450 Loss 2.0962 Accuracy 0.4269\n",
      "Epoch 5 Batch 500 Loss 2.0997 Accuracy 0.4269\n",
      "Epoch 5 Batch 550 Loss 2.0959 Accuracy 0.4268\n",
      "Epoch 5 Batch 600 Loss 2.0984 Accuracy 0.4268\n",
      "Epoch 5 Batch 650 Loss 2.0973 Accuracy 0.4268\n",
      "Epoch 5 Batch 700 Loss 2.0985 Accuracy 0.4265\n",
      "Epoch 5 Batch 750 Loss 2.0993 Accuracy 0.4265\n",
      "Epoch 5 Batch 800 Loss 2.0979 Accuracy 0.4269\n",
      "Epoch 5 Batch 850 Loss 2.0987 Accuracy 0.4267\n",
      "Epoch 5 Batch 900 Loss 2.0984 Accuracy 0.4266\n",
      "Epoch 5 Batch 950 Loss 2.0994 Accuracy 0.4268\n",
      "Epoch 5 Batch 1000 Loss 2.0987 Accuracy 0.4269\n",
      "Epoch 5 Batch 1050 Loss 2.1007 Accuracy 0.4267\n",
      "Epoch 5 Batch 1100 Loss 2.1005 Accuracy 0.4269\n",
      "Epoch 5 Batch 1150 Loss 2.0998 Accuracy 0.4269\n",
      "Epoch 5 Batch 1200 Loss 2.0979 Accuracy 0.4270\n",
      "Epoch 5 Batch 1250 Loss 2.0992 Accuracy 0.4269\n",
      "Epoch 5 Batch 1300 Loss 2.0989 Accuracy 0.4268\n",
      "Epoch 5 Batch 1350 Loss 2.0989 Accuracy 0.4269\n",
      "Epoch 5 Batch 1400 Loss 2.0979 Accuracy 0.4269\n",
      "Epoch 5 Batch 1450 Loss 2.0960 Accuracy 0.4270\n",
      "Epoch 5 Batch 1500 Loss 2.0955 Accuracy 0.4271\n",
      "Epoch 5 Batch 1550 Loss 2.0959 Accuracy 0.4270\n",
      "Epoch 5 Batch 1600 Loss 2.0951 Accuracy 0.4272\n",
      "Epoch 5 Batch 1650 Loss 2.0934 Accuracy 0.4275\n",
      "Epoch 5 Batch 1700 Loss 2.0924 Accuracy 0.4275\n",
      "Epoch 5 Batch 1750 Loss 2.0914 Accuracy 0.4275\n",
      "Epoch 5 Batch 1800 Loss 2.0909 Accuracy 0.4276\n",
      "Epoch 5 Batch 1850 Loss 2.0915 Accuracy 0.4275\n",
      "Epoch 5 Batch 1900 Loss 2.0913 Accuracy 0.4275\n",
      "Epoch 5 Batch 1950 Loss 2.0910 Accuracy 0.4275\n",
      "Epoch 5 Batch 2000 Loss 2.0916 Accuracy 0.4275\n",
      "Epoch 5 Batch 2050 Loss 2.0912 Accuracy 0.4275\n",
      "Epoch 5 Batch 2100 Loss 2.0910 Accuracy 0.4276\n",
      "Epoch 5 Batch 2150 Loss 2.0906 Accuracy 0.4277\n",
      "Epoch 5 Batch 2200 Loss 2.0912 Accuracy 0.4277\n",
      "Epoch 5 Batch 2250 Loss 2.0918 Accuracy 0.4276\n",
      "Epoch 5 Batch 2300 Loss 2.0918 Accuracy 0.4276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 2350 Loss 2.0912 Accuracy 0.4276\n",
      "Epoch 5 Batch 2400 Loss 2.0907 Accuracy 0.4276\n",
      "Epoch 5 Batch 2450 Loss 2.0908 Accuracy 0.4277\n",
      "Epoch 5 Batch 2500 Loss 2.0910 Accuracy 0.4277\n",
      "Epoch 5 Batch 2550 Loss 2.0909 Accuracy 0.4278\n",
      "Epoch 5 Batch 2600 Loss 2.0913 Accuracy 0.4277\n",
      "Epoch 5 Batch 2650 Loss 2.0916 Accuracy 0.4277\n",
      "Epoch 5 Batch 2700 Loss 2.0915 Accuracy 0.4277\n",
      "Epoch 5 Batch 2750 Loss 2.0913 Accuracy 0.4278\n",
      "Epoch 5 Batch 2800 Loss 2.0913 Accuracy 0.4278\n",
      "Epoch 5 Batch 2850 Loss 2.0915 Accuracy 0.4278\n",
      "Epoch 5 Batch 2900 Loss 2.0910 Accuracy 0.4277\n",
      "Epoch 5 Batch 2950 Loss 2.0912 Accuracy 0.4277\n",
      "Epoch 5 Batch 3000 Loss 2.0909 Accuracy 0.4277\n",
      "Epoch 5 Batch 3050 Loss 2.0909 Accuracy 0.4277\n",
      "Epoch 5 Batch 3100 Loss 2.0911 Accuracy 0.4277\n",
      "Epoch 5 Batch 3150 Loss 2.0912 Accuracy 0.4277\n",
      "Epoch 5 Batch 3200 Loss 2.0910 Accuracy 0.4278\n",
      "Epoch 5 Batch 3250 Loss 2.0909 Accuracy 0.4278\n",
      "Epoch 5 Batch 3300 Loss 2.0909 Accuracy 0.4279\n",
      "Epoch 5 Batch 3350 Loss 2.0904 Accuracy 0.4279\n",
      "Epoch 5 Batch 3400 Loss 2.0900 Accuracy 0.4278\n",
      "Epoch 5 Batch 3450 Loss 2.0897 Accuracy 0.4279\n",
      "Epoch 5 Batch 3500 Loss 2.0894 Accuracy 0.4279\n",
      "Epoch 5 Batch 3550 Loss 2.0892 Accuracy 0.4280\n",
      "Epoch 5 Batch 3600 Loss 2.0888 Accuracy 0.4279\n",
      "Epoch 5 Batch 3650 Loss 2.0884 Accuracy 0.4279\n",
      "Epoch 5 Batch 3700 Loss 2.0886 Accuracy 0.4279\n",
      "Epoch 5 Batch 3750 Loss 2.0888 Accuracy 0.4279\n",
      "Epoch 5 Batch 3800 Loss 2.0890 Accuracy 0.4278\n",
      "Epoch 5 Batch 3850 Loss 2.0892 Accuracy 0.4278\n",
      "Epoch 5 Batch 3900 Loss 2.0888 Accuracy 0.4279\n",
      "Epoch 5 Batch 3950 Loss 2.0888 Accuracy 0.4279\n",
      "Epoch 5 Batch 4000 Loss 2.0882 Accuracy 0.4280\n",
      "Epoch 5 Batch 4050 Loss 2.0885 Accuracy 0.4280\n",
      "Epoch 5 Batch 4100 Loss 2.0887 Accuracy 0.4279\n",
      "Epoch 5 Batch 4150 Loss 2.0888 Accuracy 0.4279\n",
      "Epoch 5 Batch 4200 Loss 2.0887 Accuracy 0.4279\n",
      "Epoch 5 Batch 4250 Loss 2.0887 Accuracy 0.4279\n",
      "Epoch 5 Batch 4300 Loss 2.0886 Accuracy 0.4279\n",
      "Epoch 5 Batch 4350 Loss 2.0882 Accuracy 0.4279\n",
      "Epoch 5 Batch 4400 Loss 2.0878 Accuracy 0.4279\n",
      "Epoch 5 Batch 4450 Loss 2.0876 Accuracy 0.4280\n",
      "Epoch 5 Batch 4500 Loss 2.0878 Accuracy 0.4280\n",
      "Epoch 5 Batch 4550 Loss 2.0878 Accuracy 0.4279\n",
      "Epoch 5 Batch 4600 Loss 2.0877 Accuracy 0.4279\n",
      "Epoch 5 Batch 4650 Loss 2.0876 Accuracy 0.4279\n",
      "Epoch 5 Batch 4700 Loss 2.0872 Accuracy 0.4280\n",
      "Epoch 5 Batch 4750 Loss 2.0874 Accuracy 0.4279\n",
      "Epoch 5 Batch 4800 Loss 2.0873 Accuracy 0.4280\n",
      "Epoch 5 Batch 4850 Loss 2.0870 Accuracy 0.4280\n",
      "Epoch 5 Batch 4900 Loss 2.0868 Accuracy 0.4280\n",
      "Epoch 5 Batch 4950 Loss 2.0865 Accuracy 0.4280\n",
      "Epoch 5 Batch 5000 Loss 2.0864 Accuracy 0.4280\n",
      "Epoch 5 Batch 5050 Loss 2.0864 Accuracy 0.4280\n",
      "Epoch 5 Batch 5100 Loss 2.0867 Accuracy 0.4280\n",
      "Epoch 5 Batch 5150 Loss 2.0863 Accuracy 0.4280\n",
      "Epoch 5 Batch 5200 Loss 2.0862 Accuracy 0.4280\n",
      "Epoch 5 Batch 5250 Loss 2.0859 Accuracy 0.4280\n",
      "Epoch 5 Batch 5300 Loss 2.0857 Accuracy 0.4281\n",
      "Epoch 5 Batch 5350 Loss 2.0860 Accuracy 0.4280\n",
      "Epoch 5 Batch 5400 Loss 2.0855 Accuracy 0.4280\n",
      "Epoch 5 Batch 5450 Loss 2.0853 Accuracy 0.4280\n",
      "Epoch 5 Batch 5500 Loss 2.0850 Accuracy 0.4280\n",
      "Epoch 5 Batch 5550 Loss 2.0847 Accuracy 0.4280\n",
      "Epoch 5 Batch 5600 Loss 2.0843 Accuracy 0.4281\n",
      "Epoch 5 Batch 5650 Loss 2.0843 Accuracy 0.4281\n",
      "Epoch 5 Batch 5700 Loss 2.0840 Accuracy 0.4281\n",
      "Epoch 5 Batch 5750 Loss 2.0840 Accuracy 0.4282\n",
      "Epoch 5 Batch 5800 Loss 2.0839 Accuracy 0.4282\n",
      "Epoch 5 Batch 5850 Loss 2.0839 Accuracy 0.4282\n",
      "Epoch 5 Batch 5900 Loss 2.0836 Accuracy 0.4282\n",
      "Epoch 5 Batch 5950 Loss 2.0830 Accuracy 0.4283\n",
      "Epoch 5 Batch 6000 Loss 2.0827 Accuracy 0.4283\n",
      "Epoch 5 Batch 6050 Loss 2.0823 Accuracy 0.4283\n",
      "Epoch 5 Batch 6100 Loss 2.0822 Accuracy 0.4284\n",
      "Epoch 5 Batch 6150 Loss 2.0822 Accuracy 0.4284\n",
      "Epoch 5 Batch 6200 Loss 2.0822 Accuracy 0.4284\n",
      "Epoch 5 Batch 6250 Loss 2.0821 Accuracy 0.4283\n",
      "Epoch 5 Batch 6300 Loss 2.0819 Accuracy 0.4284\n",
      "Epoch 5 Batch 6350 Loss 2.0822 Accuracy 0.4284\n",
      "Epoch 5 Batch 6400 Loss 2.0821 Accuracy 0.4284\n",
      "Epoch 5 Batch 6450 Loss 2.0818 Accuracy 0.4284\n",
      "Epoch 5 Batch 6500 Loss 2.0813 Accuracy 0.4285\n",
      "Epoch 5 Batch 6550 Loss 2.0816 Accuracy 0.4285\n",
      "Epoch 5 Batch 6600 Loss 2.0814 Accuracy 0.4285\n",
      "Epoch 5 Batch 6650 Loss 2.0813 Accuracy 0.4285\n",
      "Epoch 5 Batch 6700 Loss 2.0809 Accuracy 0.4285\n",
      "Epoch 5 Batch 6750 Loss 2.0803 Accuracy 0.4286\n",
      "Epoch 5 Batch 6800 Loss 2.0800 Accuracy 0.4286\n",
      "Epoch 5 Batch 6850 Loss 2.0801 Accuracy 0.4286\n",
      "Epoch 5 Batch 6900 Loss 2.0799 Accuracy 0.4286\n",
      "Epoch 5 Batch 6950 Loss 2.0797 Accuracy 0.4286\n",
      "Epoch 5 Batch 7000 Loss 2.0793 Accuracy 0.4286\n",
      "Epoch 5 Batch 7050 Loss 2.0792 Accuracy 0.4286\n",
      "Epoch 5 Batch 7100 Loss 2.0788 Accuracy 0.4287\n",
      "Epoch 5 Batch 7150 Loss 2.0786 Accuracy 0.4287\n",
      "Epoch 5 Batch 7200 Loss 2.0784 Accuracy 0.4287\n",
      "Epoch 5 Batch 7250 Loss 2.0781 Accuracy 0.4287\n",
      "Epoch 5 Batch 7300 Loss 2.0778 Accuracy 0.4288\n",
      "Epoch 5 Batch 7350 Loss 2.0775 Accuracy 0.4288\n",
      "Epoch 5 Batch 7400 Loss 2.0776 Accuracy 0.4288\n",
      "Epoch 5 Batch 7450 Loss 2.0776 Accuracy 0.4287\n",
      "Epoch 5 Batch 7500 Loss 2.0775 Accuracy 0.4288\n",
      "Epoch 5 Batch 7550 Loss 2.0773 Accuracy 0.4288\n",
      "Epoch 5 Batch 7600 Loss 2.0772 Accuracy 0.4288\n",
      "Epoch 5 Batch 7650 Loss 2.0772 Accuracy 0.4288\n",
      "Epoch 5 Batch 7700 Loss 2.0771 Accuracy 0.4288\n",
      "Epoch 5 Batch 7750 Loss 2.0771 Accuracy 0.4288\n",
      "Epoch 5 Batch 7800 Loss 2.0769 Accuracy 0.4289\n",
      "Epoch 5 Batch 7850 Loss 2.0767 Accuracy 0.4289\n",
      "Epoch 5 Batch 7900 Loss 2.0765 Accuracy 0.4289\n",
      "Epoch 5 Batch 7950 Loss 2.0764 Accuracy 0.4289\n",
      "Epoch 5 Batch 8000 Loss 2.0758 Accuracy 0.4290\n",
      "Epoch 5 Batch 8050 Loss 2.0755 Accuracy 0.4290\n",
      "Epoch 5 Batch 8100 Loss 2.0751 Accuracy 0.4291\n",
      "Epoch 5 Batch 8150 Loss 2.0749 Accuracy 0.4291\n",
      "Epoch 5 Batch 8200 Loss 2.0750 Accuracy 0.4291\n",
      "Epoch 5 Batch 8250 Loss 2.0748 Accuracy 0.4291\n",
      "Epoch 5 Batch 8300 Loss 2.0746 Accuracy 0.4291\n",
      "Epoch 5 Batch 8350 Loss 2.0746 Accuracy 0.4291\n",
      "Epoch 5 Batch 8400 Loss 2.0747 Accuracy 0.4291\n",
      "Epoch 5 Batch 8450 Loss 2.0746 Accuracy 0.4291\n",
      "Epoch 5 Batch 8500 Loss 2.0745 Accuracy 0.4292\n",
      "Epoch 5 Batch 8550 Loss 2.0744 Accuracy 0.4291\n",
      "Epoch 5 Batch 8600 Loss 2.0741 Accuracy 0.4292\n",
      "Epoch 5 Batch 8650 Loss 2.0738 Accuracy 0.4292\n",
      "Epoch 5 Batch 8700 Loss 2.0737 Accuracy 0.4292\n",
      "Epoch 5 Batch 8750 Loss 2.0736 Accuracy 0.4292\n",
      "Epoch 5 Batch 8800 Loss 2.0735 Accuracy 0.4292\n",
      "Epoch 5 Batch 8850 Loss 2.0733 Accuracy 0.4292\n",
      "Epoch 5 Batch 8900 Loss 2.0733 Accuracy 0.4292\n",
      "Epoch 5 Batch 8950 Loss 2.0731 Accuracy 0.4292\n",
      "Epoch 5 Batch 9000 Loss 2.0730 Accuracy 0.4292\n",
      "Epoch 5 Batch 9050 Loss 2.0728 Accuracy 0.4292\n",
      "Epoch 5 Batch 9100 Loss 2.0726 Accuracy 0.4293\n",
      "Epoch 5 Batch 9150 Loss 2.0727 Accuracy 0.4293\n",
      "Epoch 5 Batch 9200 Loss 2.0726 Accuracy 0.4293\n",
      "Epoch 5 Batch 9250 Loss 2.0724 Accuracy 0.4293\n",
      "Epoch 5 Batch 9300 Loss 2.0723 Accuracy 0.4293\n",
      "Epoch 5 Batch 9350 Loss 2.0722 Accuracy 0.4293\n",
      "Epoch 5 Batch 9400 Loss 2.0720 Accuracy 0.4293\n",
      "Epoch 5 Batch 9450 Loss 2.0720 Accuracy 0.4293\n",
      "Epoch 5 Batch 9500 Loss 2.0718 Accuracy 0.4294\n",
      "Epoch 5 Batch 9550 Loss 2.0718 Accuracy 0.4294\n",
      "Epoch 5 Batch 9600 Loss 2.0716 Accuracy 0.4294\n",
      "Epoch 5 Batch 9650 Loss 2.0717 Accuracy 0.4293\n",
      "Epoch 5 Batch 9700 Loss 2.0714 Accuracy 0.4294\n",
      "Epoch 5 Batch 9750 Loss 2.0713 Accuracy 0.4294\n",
      "Epoch 5 Batch 9800 Loss 2.0712 Accuracy 0.4294\n",
      "Epoch 5 Batch 9850 Loss 2.0708 Accuracy 0.4294\n",
      "Epoch 5 Batch 9900 Loss 2.0708 Accuracy 0.4294\n",
      "Epoch 5 Batch 9950 Loss 2.0706 Accuracy 0.4294\n",
      "Epoch 5 Batch 10000 Loss 2.0706 Accuracy 0.4294\n",
      "Epoch 5 Batch 10050 Loss 2.0704 Accuracy 0.4295\n",
      "Epoch 5 Batch 10100 Loss 2.0702 Accuracy 0.4295\n",
      "Epoch 5 Batch 10150 Loss 2.0702 Accuracy 0.4295\n",
      "Epoch 5 Batch 10200 Loss 2.0701 Accuracy 0.4295\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 2.0700 Accuracy 0.4295\n",
      "Time taken for 1 epoch: 432.04125714302063 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.1147 Accuracy 0.3906\n",
      "Epoch 6 Batch 50 Loss 2.0675 Accuracy 0.4315\n",
      "Epoch 6 Batch 100 Loss 2.0509 Accuracy 0.4348\n",
      "Epoch 6 Batch 150 Loss 2.0405 Accuracy 0.4340\n",
      "Epoch 6 Batch 200 Loss 2.0232 Accuracy 0.4353\n",
      "Epoch 6 Batch 250 Loss 2.0242 Accuracy 0.4349\n",
      "Epoch 6 Batch 300 Loss 2.0309 Accuracy 0.4335\n",
      "Epoch 6 Batch 350 Loss 2.0294 Accuracy 0.4337\n",
      "Epoch 6 Batch 400 Loss 2.0302 Accuracy 0.4332\n",
      "Epoch 6 Batch 450 Loss 2.0306 Accuracy 0.4334\n",
      "Epoch 6 Batch 500 Loss 2.0322 Accuracy 0.4336\n",
      "Epoch 6 Batch 550 Loss 2.0332 Accuracy 0.4333\n",
      "Epoch 6 Batch 600 Loss 2.0347 Accuracy 0.4332\n",
      "Epoch 6 Batch 650 Loss 2.0347 Accuracy 0.4332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 700 Loss 2.0363 Accuracy 0.4331\n",
      "Epoch 6 Batch 750 Loss 2.0386 Accuracy 0.4328\n",
      "Epoch 6 Batch 800 Loss 2.0411 Accuracy 0.4327\n",
      "Epoch 6 Batch 850 Loss 2.0405 Accuracy 0.4326\n",
      "Epoch 6 Batch 900 Loss 2.0420 Accuracy 0.4325\n",
      "Epoch 6 Batch 950 Loss 2.0404 Accuracy 0.4327\n",
      "Epoch 6 Batch 1000 Loss 2.0400 Accuracy 0.4328\n",
      "Epoch 6 Batch 1050 Loss 2.0400 Accuracy 0.4328\n",
      "Epoch 6 Batch 1100 Loss 2.0389 Accuracy 0.4329\n",
      "Epoch 6 Batch 1150 Loss 2.0381 Accuracy 0.4329\n",
      "Epoch 6 Batch 1200 Loss 2.0393 Accuracy 0.4327\n",
      "Epoch 6 Batch 1250 Loss 2.0394 Accuracy 0.4327\n",
      "Epoch 6 Batch 1300 Loss 2.0388 Accuracy 0.4327\n",
      "Epoch 6 Batch 1350 Loss 2.0379 Accuracy 0.4328\n",
      "Epoch 6 Batch 1400 Loss 2.0364 Accuracy 0.4328\n",
      "Epoch 6 Batch 1450 Loss 2.0364 Accuracy 0.4328\n",
      "Epoch 6 Batch 1500 Loss 2.0366 Accuracy 0.4328\n",
      "Epoch 6 Batch 1550 Loss 2.0366 Accuracy 0.4329\n",
      "Epoch 6 Batch 1600 Loss 2.0357 Accuracy 0.4329\n",
      "Epoch 6 Batch 1650 Loss 2.0365 Accuracy 0.4329\n",
      "Epoch 6 Batch 1700 Loss 2.0361 Accuracy 0.4329\n",
      "Epoch 6 Batch 1750 Loss 2.0355 Accuracy 0.4330\n",
      "Epoch 6 Batch 1800 Loss 2.0361 Accuracy 0.4330\n",
      "Epoch 6 Batch 1850 Loss 2.0361 Accuracy 0.4330\n",
      "Epoch 6 Batch 1900 Loss 2.0357 Accuracy 0.4331\n",
      "Epoch 6 Batch 1950 Loss 2.0360 Accuracy 0.4330\n",
      "Epoch 6 Batch 2000 Loss 2.0358 Accuracy 0.4330\n",
      "Epoch 6 Batch 2050 Loss 2.0358 Accuracy 0.4331\n",
      "Epoch 6 Batch 2100 Loss 2.0356 Accuracy 0.4331\n",
      "Epoch 6 Batch 2150 Loss 2.0342 Accuracy 0.4333\n",
      "Epoch 6 Batch 2200 Loss 2.0336 Accuracy 0.4333\n",
      "Epoch 6 Batch 2250 Loss 2.0337 Accuracy 0.4333\n",
      "Epoch 6 Batch 2300 Loss 2.0336 Accuracy 0.4334\n",
      "Epoch 6 Batch 2350 Loss 2.0339 Accuracy 0.4334\n",
      "Epoch 6 Batch 2400 Loss 2.0339 Accuracy 0.4334\n",
      "Epoch 6 Batch 2450 Loss 2.0335 Accuracy 0.4335\n",
      "Epoch 6 Batch 2500 Loss 2.0336 Accuracy 0.4335\n",
      "Epoch 6 Batch 2550 Loss 2.0336 Accuracy 0.4336\n",
      "Epoch 6 Batch 2600 Loss 2.0337 Accuracy 0.4336\n",
      "Epoch 6 Batch 2650 Loss 2.0347 Accuracy 0.4336\n",
      "Epoch 6 Batch 2700 Loss 2.0345 Accuracy 0.4336\n",
      "Epoch 6 Batch 2750 Loss 2.0343 Accuracy 0.4337\n",
      "Epoch 6 Batch 2800 Loss 2.0352 Accuracy 0.4336\n",
      "Epoch 6 Batch 2850 Loss 2.0347 Accuracy 0.4336\n",
      "Epoch 6 Batch 2900 Loss 2.0345 Accuracy 0.4336\n",
      "Epoch 6 Batch 2950 Loss 2.0347 Accuracy 0.4336\n",
      "Epoch 6 Batch 3000 Loss 2.0351 Accuracy 0.4335\n",
      "Epoch 6 Batch 3050 Loss 2.0353 Accuracy 0.4335\n",
      "Epoch 6 Batch 3100 Loss 2.0342 Accuracy 0.4336\n",
      "Epoch 6 Batch 3150 Loss 2.0346 Accuracy 0.4336\n",
      "Epoch 6 Batch 3200 Loss 2.0346 Accuracy 0.4336\n",
      "Epoch 6 Batch 3250 Loss 2.0340 Accuracy 0.4337\n",
      "Epoch 6 Batch 3300 Loss 2.0342 Accuracy 0.4336\n",
      "Epoch 6 Batch 3350 Loss 2.0343 Accuracy 0.4336\n",
      "Epoch 6 Batch 3400 Loss 2.0340 Accuracy 0.4336\n",
      "Epoch 6 Batch 3450 Loss 2.0338 Accuracy 0.4336\n",
      "Epoch 6 Batch 3500 Loss 2.0339 Accuracy 0.4337\n",
      "Epoch 6 Batch 3550 Loss 2.0338 Accuracy 0.4336\n",
      "Epoch 6 Batch 3600 Loss 2.0339 Accuracy 0.4336\n",
      "Epoch 6 Batch 3650 Loss 2.0338 Accuracy 0.4336\n",
      "Epoch 6 Batch 3700 Loss 2.0330 Accuracy 0.4336\n",
      "Epoch 6 Batch 3750 Loss 2.0327 Accuracy 0.4337\n",
      "Epoch 6 Batch 3800 Loss 2.0327 Accuracy 0.4336\n",
      "Epoch 6 Batch 3850 Loss 2.0324 Accuracy 0.4336\n",
      "Epoch 6 Batch 3900 Loss 2.0326 Accuracy 0.4335\n",
      "Epoch 6 Batch 3950 Loss 2.0322 Accuracy 0.4336\n",
      "Epoch 6 Batch 4000 Loss 2.0324 Accuracy 0.4337\n",
      "Epoch 6 Batch 4050 Loss 2.0324 Accuracy 0.4337\n",
      "Epoch 6 Batch 4100 Loss 2.0323 Accuracy 0.4337\n",
      "Epoch 6 Batch 4150 Loss 2.0329 Accuracy 0.4336\n",
      "Epoch 6 Batch 4200 Loss 2.0331 Accuracy 0.4335\n",
      "Epoch 6 Batch 4250 Loss 2.0332 Accuracy 0.4335\n",
      "Epoch 6 Batch 4300 Loss 2.0331 Accuracy 0.4335\n",
      "Epoch 6 Batch 4350 Loss 2.0332 Accuracy 0.4334\n",
      "Epoch 6 Batch 4400 Loss 2.0328 Accuracy 0.4335\n",
      "Epoch 6 Batch 4450 Loss 2.0325 Accuracy 0.4335\n",
      "Epoch 6 Batch 4500 Loss 2.0324 Accuracy 0.4335\n",
      "Epoch 6 Batch 4550 Loss 2.0322 Accuracy 0.4336\n",
      "Epoch 6 Batch 4600 Loss 2.0320 Accuracy 0.4336\n",
      "Epoch 6 Batch 4650 Loss 2.0320 Accuracy 0.4336\n",
      "Epoch 6 Batch 4700 Loss 2.0320 Accuracy 0.4336\n",
      "Epoch 6 Batch 4750 Loss 2.0320 Accuracy 0.4335\n",
      "Epoch 6 Batch 4800 Loss 2.0315 Accuracy 0.4336\n",
      "Epoch 6 Batch 4850 Loss 2.0312 Accuracy 0.4336\n",
      "Epoch 6 Batch 4900 Loss 2.0313 Accuracy 0.4335\n",
      "Epoch 6 Batch 4950 Loss 2.0311 Accuracy 0.4336\n",
      "Epoch 6 Batch 5000 Loss 2.0309 Accuracy 0.4336\n",
      "Epoch 6 Batch 5050 Loss 2.0310 Accuracy 0.4336\n",
      "Epoch 6 Batch 5100 Loss 2.0307 Accuracy 0.4336\n",
      "Epoch 6 Batch 5150 Loss 2.0307 Accuracy 0.4336\n",
      "Epoch 6 Batch 5200 Loss 2.0306 Accuracy 0.4336\n",
      "Epoch 6 Batch 5250 Loss 2.0303 Accuracy 0.4336\n",
      "Epoch 6 Batch 5300 Loss 2.0303 Accuracy 0.4336\n",
      "Epoch 6 Batch 5350 Loss 2.0303 Accuracy 0.4336\n",
      "Epoch 6 Batch 5400 Loss 2.0299 Accuracy 0.4336\n",
      "Epoch 6 Batch 5450 Loss 2.0301 Accuracy 0.4336\n",
      "Epoch 6 Batch 5500 Loss 2.0299 Accuracy 0.4336\n",
      "Epoch 6 Batch 5550 Loss 2.0295 Accuracy 0.4336\n",
      "Epoch 6 Batch 5600 Loss 2.0298 Accuracy 0.4336\n",
      "Epoch 6 Batch 5650 Loss 2.0299 Accuracy 0.4336\n",
      "Epoch 6 Batch 5700 Loss 2.0296 Accuracy 0.4336\n",
      "Epoch 6 Batch 5750 Loss 2.0295 Accuracy 0.4336\n",
      "Epoch 6 Batch 5800 Loss 2.0292 Accuracy 0.4336\n",
      "Epoch 6 Batch 5850 Loss 2.0292 Accuracy 0.4337\n",
      "Epoch 6 Batch 5900 Loss 2.0288 Accuracy 0.4337\n",
      "Epoch 6 Batch 5950 Loss 2.0285 Accuracy 0.4337\n",
      "Epoch 6 Batch 6000 Loss 2.0284 Accuracy 0.4337\n",
      "Epoch 6 Batch 6050 Loss 2.0283 Accuracy 0.4338\n",
      "Epoch 6 Batch 6100 Loss 2.0282 Accuracy 0.4338\n",
      "Epoch 6 Batch 6150 Loss 2.0277 Accuracy 0.4338\n",
      "Epoch 6 Batch 6200 Loss 2.0276 Accuracy 0.4338\n",
      "Epoch 6 Batch 6250 Loss 2.0276 Accuracy 0.4338\n",
      "Epoch 6 Batch 6300 Loss 2.0276 Accuracy 0.4338\n",
      "Epoch 6 Batch 6350 Loss 2.0273 Accuracy 0.4339\n",
      "Epoch 6 Batch 6400 Loss 2.0275 Accuracy 0.4339\n",
      "Epoch 6 Batch 6450 Loss 2.0273 Accuracy 0.4339\n",
      "Epoch 6 Batch 6500 Loss 2.0273 Accuracy 0.4339\n",
      "Epoch 6 Batch 6550 Loss 2.0272 Accuracy 0.4339\n",
      "Epoch 6 Batch 6600 Loss 2.0272 Accuracy 0.4340\n",
      "Epoch 6 Batch 6650 Loss 2.0271 Accuracy 0.4340\n",
      "Epoch 6 Batch 6700 Loss 2.0269 Accuracy 0.4340\n",
      "Epoch 6 Batch 6750 Loss 2.0266 Accuracy 0.4340\n",
      "Epoch 6 Batch 6800 Loss 2.0263 Accuracy 0.4340\n",
      "Epoch 6 Batch 6850 Loss 2.0262 Accuracy 0.4340\n",
      "Epoch 6 Batch 6900 Loss 2.0262 Accuracy 0.4340\n",
      "Epoch 6 Batch 6950 Loss 2.0260 Accuracy 0.4340\n",
      "Epoch 6 Batch 7000 Loss 2.0256 Accuracy 0.4340\n",
      "Epoch 6 Batch 7050 Loss 2.0254 Accuracy 0.4340\n",
      "Epoch 6 Batch 7100 Loss 2.0252 Accuracy 0.4340\n",
      "Epoch 6 Batch 7150 Loss 2.0249 Accuracy 0.4341\n",
      "Epoch 6 Batch 7200 Loss 2.0250 Accuracy 0.4341\n",
      "Epoch 6 Batch 7250 Loss 2.0251 Accuracy 0.4341\n",
      "Epoch 6 Batch 7300 Loss 2.0251 Accuracy 0.4341\n",
      "Epoch 6 Batch 7350 Loss 2.0250 Accuracy 0.4341\n",
      "Epoch 6 Batch 7400 Loss 2.0250 Accuracy 0.4340\n",
      "Epoch 6 Batch 7450 Loss 2.0248 Accuracy 0.4341\n",
      "Epoch 6 Batch 7500 Loss 2.0247 Accuracy 0.4341\n",
      "Epoch 6 Batch 7550 Loss 2.0246 Accuracy 0.4341\n",
      "Epoch 6 Batch 7600 Loss 2.0246 Accuracy 0.4341\n",
      "Epoch 6 Batch 7650 Loss 2.0243 Accuracy 0.4341\n",
      "Epoch 6 Batch 7700 Loss 2.0242 Accuracy 0.4341\n",
      "Epoch 6 Batch 7750 Loss 2.0240 Accuracy 0.4341\n",
      "Epoch 6 Batch 7800 Loss 2.0238 Accuracy 0.4341\n",
      "Epoch 6 Batch 7850 Loss 2.0239 Accuracy 0.4341\n",
      "Epoch 6 Batch 7900 Loss 2.0237 Accuracy 0.4341\n",
      "Epoch 6 Batch 7950 Loss 2.0236 Accuracy 0.4342\n",
      "Epoch 6 Batch 8000 Loss 2.0234 Accuracy 0.4342\n",
      "Epoch 6 Batch 8050 Loss 2.0234 Accuracy 0.4342\n",
      "Epoch 6 Batch 8100 Loss 2.0233 Accuracy 0.4342\n",
      "Epoch 6 Batch 8150 Loss 2.0231 Accuracy 0.4342\n",
      "Epoch 6 Batch 8200 Loss 2.0229 Accuracy 0.4343\n",
      "Epoch 6 Batch 8250 Loss 2.0230 Accuracy 0.4343\n",
      "Epoch 6 Batch 8300 Loss 2.0229 Accuracy 0.4343\n",
      "Epoch 6 Batch 8350 Loss 2.0230 Accuracy 0.4343\n",
      "Epoch 6 Batch 8400 Loss 2.0227 Accuracy 0.4343\n",
      "Epoch 6 Batch 8450 Loss 2.0225 Accuracy 0.4343\n",
      "Epoch 6 Batch 8500 Loss 2.0223 Accuracy 0.4343\n",
      "Epoch 6 Batch 8550 Loss 2.0222 Accuracy 0.4343\n",
      "Epoch 6 Batch 8600 Loss 2.0221 Accuracy 0.4344\n",
      "Epoch 6 Batch 8650 Loss 2.0220 Accuracy 0.4344\n",
      "Epoch 6 Batch 8700 Loss 2.0219 Accuracy 0.4344\n",
      "Epoch 6 Batch 8750 Loss 2.0218 Accuracy 0.4344\n",
      "Epoch 6 Batch 8800 Loss 2.0216 Accuracy 0.4344\n",
      "Epoch 6 Batch 8850 Loss 2.0215 Accuracy 0.4344\n",
      "Epoch 6 Batch 8900 Loss 2.0215 Accuracy 0.4344\n",
      "Epoch 6 Batch 8950 Loss 2.0214 Accuracy 0.4344\n",
      "Epoch 6 Batch 9000 Loss 2.0216 Accuracy 0.4344\n",
      "Epoch 6 Batch 9050 Loss 2.0213 Accuracy 0.4344\n",
      "Epoch 6 Batch 9100 Loss 2.0213 Accuracy 0.4344\n",
      "Epoch 6 Batch 9150 Loss 2.0214 Accuracy 0.4344\n",
      "Epoch 6 Batch 9200 Loss 2.0212 Accuracy 0.4344\n",
      "Epoch 6 Batch 9250 Loss 2.0212 Accuracy 0.4344\n",
      "Epoch 6 Batch 9300 Loss 2.0211 Accuracy 0.4345\n",
      "Epoch 6 Batch 9350 Loss 2.0212 Accuracy 0.4344\n",
      "Epoch 6 Batch 9400 Loss 2.0212 Accuracy 0.4344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 9450 Loss 2.0212 Accuracy 0.4344\n",
      "Epoch 6 Batch 9500 Loss 2.0211 Accuracy 0.4345\n",
      "Epoch 6 Batch 9550 Loss 2.0208 Accuracy 0.4345\n",
      "Epoch 6 Batch 9600 Loss 2.0207 Accuracy 0.4345\n",
      "Epoch 6 Batch 9650 Loss 2.0206 Accuracy 0.4345\n",
      "Epoch 6 Batch 9700 Loss 2.0205 Accuracy 0.4345\n",
      "Epoch 6 Batch 9750 Loss 2.0203 Accuracy 0.4345\n",
      "Epoch 6 Batch 9800 Loss 2.0203 Accuracy 0.4345\n",
      "Epoch 6 Batch 9850 Loss 2.0202 Accuracy 0.4345\n",
      "Epoch 6 Batch 9900 Loss 2.0200 Accuracy 0.4345\n",
      "Epoch 6 Batch 9950 Loss 2.0199 Accuracy 0.4345\n",
      "Epoch 6 Batch 10000 Loss 2.0197 Accuracy 0.4346\n",
      "Epoch 6 Batch 10050 Loss 2.0195 Accuracy 0.4346\n",
      "Epoch 6 Batch 10100 Loss 2.0194 Accuracy 0.4346\n",
      "Epoch 6 Batch 10150 Loss 2.0194 Accuracy 0.4346\n",
      "Epoch 6 Batch 10200 Loss 2.0194 Accuracy 0.4346\n",
      "Epoch 6 Loss 2.0195 Accuracy 0.4346\n",
      "Time taken for 1 epoch: 431.68280029296875 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.0522 Accuracy 0.4211\n",
      "Epoch 7 Batch 50 Loss 1.9708 Accuracy 0.4400\n",
      "Epoch 7 Batch 100 Loss 1.9781 Accuracy 0.4399\n",
      "Epoch 7 Batch 150 Loss 1.9808 Accuracy 0.4398\n",
      "Epoch 7 Batch 200 Loss 1.9887 Accuracy 0.4391\n",
      "Epoch 7 Batch 250 Loss 1.9808 Accuracy 0.4399\n",
      "Epoch 7 Batch 300 Loss 1.9759 Accuracy 0.4405\n",
      "Epoch 7 Batch 350 Loss 1.9828 Accuracy 0.4400\n",
      "Epoch 7 Batch 400 Loss 1.9863 Accuracy 0.4392\n",
      "Epoch 7 Batch 450 Loss 1.9896 Accuracy 0.4390\n",
      "Epoch 7 Batch 500 Loss 1.9899 Accuracy 0.4385\n",
      "Epoch 7 Batch 550 Loss 1.9936 Accuracy 0.4380\n",
      "Epoch 7 Batch 600 Loss 1.9938 Accuracy 0.4379\n",
      "Epoch 7 Batch 650 Loss 1.9911 Accuracy 0.4382\n",
      "Epoch 7 Batch 700 Loss 1.9936 Accuracy 0.4376\n",
      "Epoch 7 Batch 750 Loss 1.9945 Accuracy 0.4373\n",
      "Epoch 7 Batch 800 Loss 1.9961 Accuracy 0.4371\n",
      "Epoch 7 Batch 850 Loss 1.9966 Accuracy 0.4371\n",
      "Epoch 7 Batch 900 Loss 1.9967 Accuracy 0.4370\n",
      "Epoch 7 Batch 950 Loss 1.9954 Accuracy 0.4370\n",
      "Epoch 7 Batch 1000 Loss 1.9962 Accuracy 0.4369\n",
      "Epoch 7 Batch 1050 Loss 1.9967 Accuracy 0.4366\n",
      "Epoch 7 Batch 1100 Loss 1.9973 Accuracy 0.4367\n",
      "Epoch 7 Batch 1150 Loss 1.9983 Accuracy 0.4366\n",
      "Epoch 7 Batch 1200 Loss 1.9993 Accuracy 0.4366\n",
      "Epoch 7 Batch 1250 Loss 1.9988 Accuracy 0.4368\n",
      "Epoch 7 Batch 1300 Loss 1.9987 Accuracy 0.4369\n",
      "Epoch 7 Batch 1350 Loss 1.9995 Accuracy 0.4368\n",
      "Epoch 7 Batch 1400 Loss 1.9983 Accuracy 0.4368\n",
      "Epoch 7 Batch 1450 Loss 1.9974 Accuracy 0.4369\n",
      "Epoch 7 Batch 1500 Loss 1.9961 Accuracy 0.4371\n",
      "Epoch 7 Batch 1550 Loss 1.9955 Accuracy 0.4372\n",
      "Epoch 7 Batch 1600 Loss 1.9948 Accuracy 0.4374\n",
      "Epoch 7 Batch 1650 Loss 1.9949 Accuracy 0.4375\n",
      "Epoch 7 Batch 1700 Loss 1.9956 Accuracy 0.4372\n",
      "Epoch 7 Batch 1750 Loss 1.9947 Accuracy 0.4373\n",
      "Epoch 7 Batch 1800 Loss 1.9943 Accuracy 0.4374\n",
      "Epoch 7 Batch 1850 Loss 1.9938 Accuracy 0.4375\n",
      "Epoch 7 Batch 1900 Loss 1.9930 Accuracy 0.4374\n",
      "Epoch 7 Batch 1950 Loss 1.9927 Accuracy 0.4373\n",
      "Epoch 7 Batch 2000 Loss 1.9934 Accuracy 0.4373\n",
      "Epoch 7 Batch 2050 Loss 1.9939 Accuracy 0.4372\n",
      "Epoch 7 Batch 2100 Loss 1.9934 Accuracy 0.4372\n",
      "Epoch 7 Batch 2150 Loss 1.9928 Accuracy 0.4372\n",
      "Epoch 7 Batch 2200 Loss 1.9932 Accuracy 0.4371\n",
      "Epoch 7 Batch 2250 Loss 1.9930 Accuracy 0.4372\n",
      "Epoch 7 Batch 2300 Loss 1.9930 Accuracy 0.4372\n",
      "Epoch 7 Batch 2350 Loss 1.9925 Accuracy 0.4374\n",
      "Epoch 7 Batch 2400 Loss 1.9922 Accuracy 0.4374\n",
      "Epoch 7 Batch 2450 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 2500 Loss 1.9930 Accuracy 0.4374\n",
      "Epoch 7 Batch 2550 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 2600 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 2650 Loss 1.9931 Accuracy 0.4374\n",
      "Epoch 7 Batch 2700 Loss 1.9934 Accuracy 0.4374\n",
      "Epoch 7 Batch 2750 Loss 1.9945 Accuracy 0.4373\n",
      "Epoch 7 Batch 2800 Loss 1.9941 Accuracy 0.4374\n",
      "Epoch 7 Batch 2850 Loss 1.9941 Accuracy 0.4374\n",
      "Epoch 7 Batch 2900 Loss 1.9947 Accuracy 0.4374\n",
      "Epoch 7 Batch 2950 Loss 1.9942 Accuracy 0.4374\n",
      "Epoch 7 Batch 3000 Loss 1.9940 Accuracy 0.4373\n",
      "Epoch 7 Batch 3050 Loss 1.9941 Accuracy 0.4373\n",
      "Epoch 7 Batch 3100 Loss 1.9937 Accuracy 0.4375\n",
      "Epoch 7 Batch 3150 Loss 1.9938 Accuracy 0.4375\n",
      "Epoch 7 Batch 3200 Loss 1.9937 Accuracy 0.4375\n",
      "Epoch 7 Batch 3250 Loss 1.9934 Accuracy 0.4376\n",
      "Epoch 7 Batch 3300 Loss 1.9932 Accuracy 0.4375\n",
      "Epoch 7 Batch 3350 Loss 1.9934 Accuracy 0.4375\n",
      "Epoch 7 Batch 3400 Loss 1.9933 Accuracy 0.4375\n",
      "Epoch 7 Batch 3450 Loss 1.9933 Accuracy 0.4375\n",
      "Epoch 7 Batch 3500 Loss 1.9933 Accuracy 0.4375\n",
      "Epoch 7 Batch 3550 Loss 1.9929 Accuracy 0.4374\n",
      "Epoch 7 Batch 3600 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 3650 Loss 1.9924 Accuracy 0.4374\n",
      "Epoch 7 Batch 3700 Loss 1.9925 Accuracy 0.4374\n",
      "Epoch 7 Batch 3750 Loss 1.9924 Accuracy 0.4374\n",
      "Epoch 7 Batch 3800 Loss 1.9923 Accuracy 0.4375\n",
      "Epoch 7 Batch 3850 Loss 1.9928 Accuracy 0.4374\n",
      "Epoch 7 Batch 3900 Loss 1.9933 Accuracy 0.4373\n",
      "Epoch 7 Batch 3950 Loss 1.9928 Accuracy 0.4374\n",
      "Epoch 7 Batch 4000 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 4050 Loss 1.9924 Accuracy 0.4375\n",
      "Epoch 7 Batch 4100 Loss 1.9925 Accuracy 0.4374\n",
      "Epoch 7 Batch 4150 Loss 1.9925 Accuracy 0.4375\n",
      "Epoch 7 Batch 4200 Loss 1.9927 Accuracy 0.4375\n",
      "Epoch 7 Batch 4250 Loss 1.9929 Accuracy 0.4375\n",
      "Epoch 7 Batch 4300 Loss 1.9928 Accuracy 0.4374\n",
      "Epoch 7 Batch 4350 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 4400 Loss 1.9924 Accuracy 0.4374\n",
      "Epoch 7 Batch 4450 Loss 1.9927 Accuracy 0.4374\n",
      "Epoch 7 Batch 4500 Loss 1.9922 Accuracy 0.4374\n",
      "Epoch 7 Batch 4550 Loss 1.9925 Accuracy 0.4374\n",
      "Epoch 7 Batch 4600 Loss 1.9924 Accuracy 0.4373\n",
      "Epoch 7 Batch 4650 Loss 1.9922 Accuracy 0.4374\n",
      "Epoch 7 Batch 4700 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 4750 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 4800 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 4850 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 4900 Loss 1.9924 Accuracy 0.4374\n",
      "Epoch 7 Batch 4950 Loss 1.9921 Accuracy 0.4374\n",
      "Epoch 7 Batch 5000 Loss 1.9922 Accuracy 0.4374\n",
      "Epoch 7 Batch 5050 Loss 1.9921 Accuracy 0.4374\n",
      "Epoch 7 Batch 5100 Loss 1.9920 Accuracy 0.4374\n",
      "Epoch 7 Batch 5150 Loss 1.9916 Accuracy 0.4374\n",
      "Epoch 7 Batch 5200 Loss 1.9916 Accuracy 0.4374\n",
      "Epoch 7 Batch 5250 Loss 1.9914 Accuracy 0.4375\n",
      "Epoch 7 Batch 5300 Loss 1.9911 Accuracy 0.4375\n",
      "Epoch 7 Batch 5350 Loss 1.9908 Accuracy 0.4375\n",
      "Epoch 7 Batch 5400 Loss 1.9905 Accuracy 0.4375\n",
      "Epoch 7 Batch 5450 Loss 1.9903 Accuracy 0.4375\n",
      "Epoch 7 Batch 5500 Loss 1.9903 Accuracy 0.4375\n",
      "Epoch 7 Batch 5550 Loss 1.9901 Accuracy 0.4375\n",
      "Epoch 7 Batch 5600 Loss 1.9902 Accuracy 0.4375\n",
      "Epoch 7 Batch 5650 Loss 1.9902 Accuracy 0.4375\n",
      "Epoch 7 Batch 5700 Loss 1.9900 Accuracy 0.4375\n",
      "Epoch 7 Batch 5750 Loss 1.9898 Accuracy 0.4376\n",
      "Epoch 7 Batch 5800 Loss 1.9897 Accuracy 0.4376\n",
      "Epoch 7 Batch 5850 Loss 1.9899 Accuracy 0.4376\n",
      "Epoch 7 Batch 5900 Loss 1.9899 Accuracy 0.4375\n",
      "Epoch 7 Batch 5950 Loss 1.9899 Accuracy 0.4376\n",
      "Epoch 7 Batch 6000 Loss 1.9898 Accuracy 0.4376\n",
      "Epoch 7 Batch 6050 Loss 1.9897 Accuracy 0.4375\n",
      "Epoch 7 Batch 6100 Loss 1.9895 Accuracy 0.4376\n",
      "Epoch 7 Batch 6150 Loss 1.9893 Accuracy 0.4376\n",
      "Epoch 7 Batch 6200 Loss 1.9893 Accuracy 0.4376\n",
      "Epoch 7 Batch 6250 Loss 1.9894 Accuracy 0.4376\n",
      "Epoch 7 Batch 6300 Loss 1.9891 Accuracy 0.4377\n",
      "Epoch 7 Batch 6350 Loss 1.9889 Accuracy 0.4377\n",
      "Epoch 7 Batch 6400 Loss 1.9885 Accuracy 0.4377\n",
      "Epoch 7 Batch 6450 Loss 1.9885 Accuracy 0.4378\n",
      "Epoch 7 Batch 6500 Loss 1.9885 Accuracy 0.4377\n",
      "Epoch 7 Batch 6550 Loss 1.9884 Accuracy 0.4378\n",
      "Epoch 7 Batch 6600 Loss 1.9880 Accuracy 0.4378\n",
      "Epoch 7 Batch 6650 Loss 1.9877 Accuracy 0.4378\n",
      "Epoch 7 Batch 6700 Loss 1.9878 Accuracy 0.4378\n",
      "Epoch 7 Batch 6750 Loss 1.9875 Accuracy 0.4378\n",
      "Epoch 7 Batch 6800 Loss 1.9874 Accuracy 0.4378\n",
      "Epoch 7 Batch 6850 Loss 1.9872 Accuracy 0.4379\n",
      "Epoch 7 Batch 6900 Loss 1.9871 Accuracy 0.4378\n",
      "Epoch 7 Batch 6950 Loss 1.9871 Accuracy 0.4378\n",
      "Epoch 7 Batch 7000 Loss 1.9872 Accuracy 0.4379\n",
      "Epoch 7 Batch 7050 Loss 1.9871 Accuracy 0.4379\n",
      "Epoch 7 Batch 7100 Loss 1.9872 Accuracy 0.4379\n",
      "Epoch 7 Batch 7150 Loss 1.9871 Accuracy 0.4379\n",
      "Epoch 7 Batch 7200 Loss 1.9871 Accuracy 0.4379\n",
      "Epoch 7 Batch 7250 Loss 1.9871 Accuracy 0.4379\n",
      "Epoch 7 Batch 7300 Loss 1.9870 Accuracy 0.4379\n",
      "Epoch 7 Batch 7350 Loss 1.9869 Accuracy 0.4379\n",
      "Epoch 7 Batch 7400 Loss 1.9866 Accuracy 0.4379\n",
      "Epoch 7 Batch 7450 Loss 1.9862 Accuracy 0.4380\n",
      "Epoch 7 Batch 7500 Loss 1.9862 Accuracy 0.4380\n",
      "Epoch 7 Batch 7550 Loss 1.9863 Accuracy 0.4380\n",
      "Epoch 7 Batch 7600 Loss 1.9860 Accuracy 0.4380\n",
      "Epoch 7 Batch 7650 Loss 1.9858 Accuracy 0.4380\n",
      "Epoch 7 Batch 7700 Loss 1.9858 Accuracy 0.4380\n",
      "Epoch 7 Batch 7750 Loss 1.9859 Accuracy 0.4380\n",
      "Epoch 7 Batch 7800 Loss 1.9858 Accuracy 0.4381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 7850 Loss 1.9859 Accuracy 0.4380\n",
      "Epoch 7 Batch 7900 Loss 1.9857 Accuracy 0.4380\n",
      "Epoch 7 Batch 7950 Loss 1.9856 Accuracy 0.4380\n",
      "Epoch 7 Batch 8000 Loss 1.9851 Accuracy 0.4381\n",
      "Epoch 7 Batch 8050 Loss 1.9850 Accuracy 0.4381\n",
      "Epoch 7 Batch 8100 Loss 1.9852 Accuracy 0.4381\n",
      "Epoch 7 Batch 8150 Loss 1.9851 Accuracy 0.4381\n",
      "Epoch 7 Batch 8200 Loss 1.9849 Accuracy 0.4381\n",
      "Epoch 7 Batch 8250 Loss 1.9849 Accuracy 0.4381\n",
      "Epoch 7 Batch 8300 Loss 1.9849 Accuracy 0.4381\n",
      "Epoch 7 Batch 8350 Loss 1.9847 Accuracy 0.4381\n",
      "Epoch 7 Batch 8400 Loss 1.9847 Accuracy 0.4381\n",
      "Epoch 7 Batch 8450 Loss 1.9844 Accuracy 0.4382\n",
      "Epoch 7 Batch 8500 Loss 1.9845 Accuracy 0.4382\n",
      "Epoch 7 Batch 8550 Loss 1.9842 Accuracy 0.4382\n",
      "Epoch 7 Batch 8600 Loss 1.9841 Accuracy 0.4382\n",
      "Epoch 7 Batch 8650 Loss 1.9840 Accuracy 0.4382\n",
      "Epoch 7 Batch 8700 Loss 1.9838 Accuracy 0.4382\n",
      "Epoch 7 Batch 8750 Loss 1.9837 Accuracy 0.4382\n",
      "Epoch 7 Batch 8800 Loss 1.9835 Accuracy 0.4382\n",
      "Epoch 7 Batch 8850 Loss 1.9836 Accuracy 0.4382\n",
      "Epoch 7 Batch 8900 Loss 1.9836 Accuracy 0.4382\n",
      "Epoch 7 Batch 8950 Loss 1.9834 Accuracy 0.4383\n",
      "Epoch 7 Batch 9000 Loss 1.9834 Accuracy 0.4383\n",
      "Epoch 7 Batch 9050 Loss 1.9835 Accuracy 0.4382\n",
      "Epoch 7 Batch 9100 Loss 1.9834 Accuracy 0.4382\n",
      "Epoch 7 Batch 9150 Loss 1.9833 Accuracy 0.4382\n",
      "Epoch 7 Batch 9200 Loss 1.9831 Accuracy 0.4382\n",
      "Epoch 7 Batch 9250 Loss 1.9831 Accuracy 0.4382\n",
      "Epoch 7 Batch 9300 Loss 1.9831 Accuracy 0.4382\n",
      "Epoch 7 Batch 9350 Loss 1.9832 Accuracy 0.4382\n",
      "Epoch 7 Batch 9400 Loss 1.9830 Accuracy 0.4383\n",
      "Epoch 7 Batch 9450 Loss 1.9829 Accuracy 0.4383\n",
      "Epoch 7 Batch 9500 Loss 1.9826 Accuracy 0.4383\n",
      "Epoch 7 Batch 9550 Loss 1.9826 Accuracy 0.4383\n",
      "Epoch 7 Batch 9600 Loss 1.9825 Accuracy 0.4383\n",
      "Epoch 7 Batch 9650 Loss 1.9823 Accuracy 0.4383\n",
      "Epoch 7 Batch 9700 Loss 1.9822 Accuracy 0.4383\n",
      "Epoch 7 Batch 9750 Loss 1.9821 Accuracy 0.4383\n",
      "Epoch 7 Batch 9800 Loss 1.9823 Accuracy 0.4383\n",
      "Epoch 7 Batch 9850 Loss 1.9822 Accuracy 0.4383\n",
      "Epoch 7 Batch 9900 Loss 1.9824 Accuracy 0.4383\n",
      "Epoch 7 Batch 9950 Loss 1.9824 Accuracy 0.4383\n",
      "Epoch 7 Batch 10000 Loss 1.9823 Accuracy 0.4383\n",
      "Epoch 7 Batch 10050 Loss 1.9824 Accuracy 0.4383\n",
      "Epoch 7 Batch 10100 Loss 1.9823 Accuracy 0.4383\n",
      "Epoch 7 Batch 10150 Loss 1.9822 Accuracy 0.4383\n",
      "Epoch 7 Batch 10200 Loss 1.9821 Accuracy 0.4383\n",
      "Epoch 7 Loss 1.9820 Accuracy 0.4384\n",
      "Time taken for 1 epoch: 431.68370938301086 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.8741 Accuracy 0.4646\n",
      "Epoch 8 Batch 50 Loss 1.9846 Accuracy 0.4410\n",
      "Epoch 8 Batch 100 Loss 1.9557 Accuracy 0.4425\n",
      "Epoch 8 Batch 150 Loss 1.9475 Accuracy 0.4437\n",
      "Epoch 8 Batch 200 Loss 1.9585 Accuracy 0.4419\n",
      "Epoch 8 Batch 250 Loss 1.9578 Accuracy 0.4421\n",
      "Epoch 8 Batch 300 Loss 1.9597 Accuracy 0.4422\n",
      "Epoch 8 Batch 350 Loss 1.9630 Accuracy 0.4420\n",
      "Epoch 8 Batch 400 Loss 1.9604 Accuracy 0.4419\n",
      "Epoch 8 Batch 450 Loss 1.9593 Accuracy 0.4420\n",
      "Epoch 8 Batch 500 Loss 1.9581 Accuracy 0.4427\n",
      "Epoch 8 Batch 550 Loss 1.9588 Accuracy 0.4423\n",
      "Epoch 8 Batch 600 Loss 1.9609 Accuracy 0.4425\n",
      "Epoch 8 Batch 650 Loss 1.9616 Accuracy 0.4421\n",
      "Epoch 8 Batch 700 Loss 1.9653 Accuracy 0.4416\n",
      "Epoch 8 Batch 750 Loss 1.9658 Accuracy 0.4413\n",
      "Epoch 8 Batch 800 Loss 1.9687 Accuracy 0.4411\n",
      "Epoch 8 Batch 850 Loss 1.9679 Accuracy 0.4407\n",
      "Epoch 8 Batch 900 Loss 1.9687 Accuracy 0.4406\n",
      "Epoch 8 Batch 950 Loss 1.9696 Accuracy 0.4405\n",
      "Epoch 8 Batch 1000 Loss 1.9673 Accuracy 0.4404\n",
      "Epoch 8 Batch 1050 Loss 1.9695 Accuracy 0.4402\n",
      "Epoch 8 Batch 1100 Loss 1.9691 Accuracy 0.4402\n",
      "Epoch 8 Batch 1150 Loss 1.9668 Accuracy 0.4404\n",
      "Epoch 8 Batch 1200 Loss 1.9668 Accuracy 0.4405\n",
      "Epoch 8 Batch 1250 Loss 1.9667 Accuracy 0.4403\n",
      "Epoch 8 Batch 1300 Loss 1.9666 Accuracy 0.4403\n",
      "Epoch 8 Batch 1350 Loss 1.9664 Accuracy 0.4402\n",
      "Epoch 8 Batch 1400 Loss 1.9667 Accuracy 0.4402\n",
      "Epoch 8 Batch 1450 Loss 1.9663 Accuracy 0.4401\n",
      "Epoch 8 Batch 1500 Loss 1.9663 Accuracy 0.4400\n",
      "Epoch 8 Batch 1550 Loss 1.9653 Accuracy 0.4400\n",
      "Epoch 8 Batch 1600 Loss 1.9638 Accuracy 0.4401\n",
      "Epoch 8 Batch 1650 Loss 1.9633 Accuracy 0.4402\n",
      "Epoch 8 Batch 1700 Loss 1.9624 Accuracy 0.4403\n",
      "Epoch 8 Batch 1750 Loss 1.9623 Accuracy 0.4403\n",
      "Epoch 8 Batch 1800 Loss 1.9618 Accuracy 0.4404\n",
      "Epoch 8 Batch 1850 Loss 1.9610 Accuracy 0.4405\n",
      "Epoch 8 Batch 1900 Loss 1.9610 Accuracy 0.4405\n",
      "Epoch 8 Batch 1950 Loss 1.9618 Accuracy 0.4405\n",
      "Epoch 8 Batch 2000 Loss 1.9621 Accuracy 0.4405\n",
      "Epoch 8 Batch 2050 Loss 1.9613 Accuracy 0.4406\n",
      "Epoch 8 Batch 2100 Loss 1.9612 Accuracy 0.4406\n",
      "Epoch 8 Batch 2150 Loss 1.9611 Accuracy 0.4407\n",
      "Epoch 8 Batch 2200 Loss 1.9611 Accuracy 0.4407\n",
      "Epoch 8 Batch 2250 Loss 1.9610 Accuracy 0.4408\n",
      "Epoch 8 Batch 2300 Loss 1.9602 Accuracy 0.4408\n",
      "Epoch 8 Batch 2350 Loss 1.9603 Accuracy 0.4408\n",
      "Epoch 8 Batch 2400 Loss 1.9609 Accuracy 0.4407\n",
      "Epoch 8 Batch 2450 Loss 1.9607 Accuracy 0.4408\n",
      "Epoch 8 Batch 2500 Loss 1.9610 Accuracy 0.4407\n",
      "Epoch 8 Batch 2550 Loss 1.9613 Accuracy 0.4406\n",
      "Epoch 8 Batch 2600 Loss 1.9607 Accuracy 0.4407\n",
      "Epoch 8 Batch 2650 Loss 1.9612 Accuracy 0.4406\n",
      "Epoch 8 Batch 2700 Loss 1.9616 Accuracy 0.4405\n",
      "Epoch 8 Batch 2750 Loss 1.9620 Accuracy 0.4405\n",
      "Epoch 8 Batch 2800 Loss 1.9615 Accuracy 0.4405\n",
      "Epoch 8 Batch 2850 Loss 1.9616 Accuracy 0.4405\n",
      "Epoch 8 Batch 2900 Loss 1.9616 Accuracy 0.4406\n",
      "Epoch 8 Batch 2950 Loss 1.9619 Accuracy 0.4407\n",
      "Epoch 8 Batch 3000 Loss 1.9615 Accuracy 0.4407\n",
      "Epoch 8 Batch 3050 Loss 1.9617 Accuracy 0.4407\n",
      "Epoch 8 Batch 3100 Loss 1.9618 Accuracy 0.4408\n",
      "Epoch 8 Batch 3150 Loss 1.9622 Accuracy 0.4408\n",
      "Epoch 8 Batch 3200 Loss 1.9617 Accuracy 0.4409\n",
      "Epoch 8 Batch 3250 Loss 1.9618 Accuracy 0.4408\n",
      "Epoch 8 Batch 3300 Loss 1.9617 Accuracy 0.4408\n",
      "Epoch 8 Batch 3350 Loss 1.9614 Accuracy 0.4408\n",
      "Epoch 8 Batch 3400 Loss 1.9613 Accuracy 0.4408\n",
      "Epoch 8 Batch 3450 Loss 1.9610 Accuracy 0.4408\n",
      "Epoch 8 Batch 3500 Loss 1.9608 Accuracy 0.4409\n",
      "Epoch 8 Batch 3550 Loss 1.9606 Accuracy 0.4409\n",
      "Epoch 8 Batch 3600 Loss 1.9609 Accuracy 0.4407\n",
      "Epoch 8 Batch 3650 Loss 1.9611 Accuracy 0.4407\n",
      "Epoch 8 Batch 3700 Loss 1.9610 Accuracy 0.4407\n",
      "Epoch 8 Batch 3750 Loss 1.9611 Accuracy 0.4407\n",
      "Epoch 8 Batch 3800 Loss 1.9608 Accuracy 0.4406\n",
      "Epoch 8 Batch 3850 Loss 1.9606 Accuracy 0.4407\n",
      "Epoch 8 Batch 3900 Loss 1.9602 Accuracy 0.4406\n",
      "Epoch 8 Batch 3950 Loss 1.9600 Accuracy 0.4407\n",
      "Epoch 8 Batch 4000 Loss 1.9607 Accuracy 0.4407\n",
      "Epoch 8 Batch 4050 Loss 1.9610 Accuracy 0.4406\n",
      "Epoch 8 Batch 4100 Loss 1.9615 Accuracy 0.4406\n",
      "Epoch 8 Batch 4150 Loss 1.9613 Accuracy 0.4406\n",
      "Epoch 8 Batch 4200 Loss 1.9616 Accuracy 0.4405\n",
      "Epoch 8 Batch 4250 Loss 1.9617 Accuracy 0.4405\n",
      "Epoch 8 Batch 4300 Loss 1.9614 Accuracy 0.4406\n",
      "Epoch 8 Batch 4350 Loss 1.9613 Accuracy 0.4406\n",
      "Epoch 8 Batch 4400 Loss 1.9610 Accuracy 0.4406\n",
      "Epoch 8 Batch 4450 Loss 1.9607 Accuracy 0.4406\n",
      "Epoch 8 Batch 4500 Loss 1.9606 Accuracy 0.4406\n",
      "Epoch 8 Batch 4550 Loss 1.9611 Accuracy 0.4406\n",
      "Epoch 8 Batch 4600 Loss 1.9609 Accuracy 0.4406\n",
      "Epoch 8 Batch 4650 Loss 1.9607 Accuracy 0.4406\n",
      "Epoch 8 Batch 4700 Loss 1.9606 Accuracy 0.4407\n",
      "Epoch 8 Batch 4750 Loss 1.9602 Accuracy 0.4407\n",
      "Epoch 8 Batch 4800 Loss 1.9599 Accuracy 0.4408\n",
      "Epoch 8 Batch 4850 Loss 1.9601 Accuracy 0.4407\n",
      "Epoch 8 Batch 4900 Loss 1.9605 Accuracy 0.4407\n",
      "Epoch 8 Batch 4950 Loss 1.9602 Accuracy 0.4407\n",
      "Epoch 8 Batch 5000 Loss 1.9605 Accuracy 0.4406\n",
      "Epoch 8 Batch 5050 Loss 1.9604 Accuracy 0.4407\n",
      "Epoch 8 Batch 5100 Loss 1.9603 Accuracy 0.4407\n",
      "Epoch 8 Batch 5150 Loss 1.9602 Accuracy 0.4407\n",
      "Epoch 8 Batch 5200 Loss 1.9599 Accuracy 0.4407\n",
      "Epoch 8 Batch 5250 Loss 1.9597 Accuracy 0.4407\n",
      "Epoch 8 Batch 5300 Loss 1.9597 Accuracy 0.4407\n",
      "Epoch 8 Batch 5350 Loss 1.9599 Accuracy 0.4407\n",
      "Epoch 8 Batch 5400 Loss 1.9599 Accuracy 0.4406\n",
      "Epoch 8 Batch 5450 Loss 1.9600 Accuracy 0.4406\n",
      "Epoch 8 Batch 5500 Loss 1.9598 Accuracy 0.4406\n",
      "Epoch 8 Batch 5550 Loss 1.9596 Accuracy 0.4406\n",
      "Epoch 8 Batch 5600 Loss 1.9598 Accuracy 0.4406\n",
      "Epoch 8 Batch 5650 Loss 1.9594 Accuracy 0.4407\n",
      "Epoch 8 Batch 5700 Loss 1.9593 Accuracy 0.4407\n",
      "Epoch 8 Batch 5750 Loss 1.9593 Accuracy 0.4407\n",
      "Epoch 8 Batch 5800 Loss 1.9592 Accuracy 0.4407\n",
      "Epoch 8 Batch 5850 Loss 1.9588 Accuracy 0.4407\n",
      "Epoch 8 Batch 5900 Loss 1.9588 Accuracy 0.4408\n",
      "Epoch 8 Batch 5950 Loss 1.9585 Accuracy 0.4408\n",
      "Epoch 8 Batch 6000 Loss 1.9583 Accuracy 0.4408\n",
      "Epoch 8 Batch 6050 Loss 1.9584 Accuracy 0.4408\n",
      "Epoch 8 Batch 6100 Loss 1.9585 Accuracy 0.4408\n",
      "Epoch 8 Batch 6150 Loss 1.9586 Accuracy 0.4408\n",
      "Epoch 8 Batch 6200 Loss 1.9589 Accuracy 0.4408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 6250 Loss 1.9586 Accuracy 0.4408\n",
      "Epoch 8 Batch 6300 Loss 1.9586 Accuracy 0.4408\n",
      "Epoch 8 Batch 6350 Loss 1.9582 Accuracy 0.4409\n",
      "Epoch 8 Batch 6400 Loss 1.9581 Accuracy 0.4409\n",
      "Epoch 8 Batch 6450 Loss 1.9576 Accuracy 0.4409\n",
      "Epoch 8 Batch 6500 Loss 1.9575 Accuracy 0.4410\n",
      "Epoch 8 Batch 6550 Loss 1.9571 Accuracy 0.4410\n",
      "Epoch 8 Batch 6600 Loss 1.9572 Accuracy 0.4410\n",
      "Epoch 8 Batch 6650 Loss 1.9568 Accuracy 0.4410\n",
      "Epoch 8 Batch 6700 Loss 1.9567 Accuracy 0.4410\n",
      "Epoch 8 Batch 6750 Loss 1.9569 Accuracy 0.4410\n",
      "Epoch 8 Batch 6800 Loss 1.9569 Accuracy 0.4410\n",
      "Epoch 8 Batch 6850 Loss 1.9569 Accuracy 0.4410\n",
      "Epoch 8 Batch 6900 Loss 1.9565 Accuracy 0.4411\n",
      "Epoch 8 Batch 6950 Loss 1.9564 Accuracy 0.4410\n",
      "Epoch 8 Batch 7000 Loss 1.9562 Accuracy 0.4411\n",
      "Epoch 8 Batch 7050 Loss 1.9562 Accuracy 0.4411\n",
      "Epoch 8 Batch 7100 Loss 1.9561 Accuracy 0.4411\n",
      "Epoch 8 Batch 7150 Loss 1.9560 Accuracy 0.4411\n",
      "Epoch 8 Batch 7200 Loss 1.9560 Accuracy 0.4411\n",
      "Epoch 8 Batch 7250 Loss 1.9558 Accuracy 0.4411\n",
      "Epoch 8 Batch 7300 Loss 1.9558 Accuracy 0.4411\n",
      "Epoch 8 Batch 7350 Loss 1.9557 Accuracy 0.4411\n",
      "Epoch 8 Batch 7400 Loss 1.9559 Accuracy 0.4411\n",
      "Epoch 8 Batch 7450 Loss 1.9557 Accuracy 0.4411\n",
      "Epoch 8 Batch 7500 Loss 1.9555 Accuracy 0.4412\n",
      "Epoch 8 Batch 7550 Loss 1.9556 Accuracy 0.4412\n",
      "Epoch 8 Batch 7600 Loss 1.9554 Accuracy 0.4412\n",
      "Epoch 8 Batch 7650 Loss 1.9555 Accuracy 0.4412\n",
      "Epoch 8 Batch 7700 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 7750 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 7800 Loss 1.9555 Accuracy 0.4412\n",
      "Epoch 8 Batch 7850 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 7900 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 7950 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 8000 Loss 1.9553 Accuracy 0.4412\n",
      "Epoch 8 Batch 8050 Loss 1.9550 Accuracy 0.4412\n",
      "Epoch 8 Batch 8100 Loss 1.9547 Accuracy 0.4413\n",
      "Epoch 8 Batch 8150 Loss 1.9545 Accuracy 0.4413\n",
      "Epoch 8 Batch 8200 Loss 1.9544 Accuracy 0.4413\n",
      "Epoch 8 Batch 8250 Loss 1.9543 Accuracy 0.4413\n",
      "Epoch 8 Batch 8300 Loss 1.9542 Accuracy 0.4413\n",
      "Epoch 8 Batch 8350 Loss 1.9540 Accuracy 0.4413\n",
      "Epoch 8 Batch 8400 Loss 1.9540 Accuracy 0.4414\n",
      "Epoch 8 Batch 8450 Loss 1.9540 Accuracy 0.4414\n",
      "Epoch 8 Batch 8500 Loss 1.9538 Accuracy 0.4414\n",
      "Epoch 8 Batch 8550 Loss 1.9537 Accuracy 0.4414\n",
      "Epoch 8 Batch 8600 Loss 1.9538 Accuracy 0.4414\n",
      "Epoch 8 Batch 8650 Loss 1.9539 Accuracy 0.4414\n",
      "Epoch 8 Batch 8700 Loss 1.9539 Accuracy 0.4414\n",
      "Epoch 8 Batch 8750 Loss 1.9537 Accuracy 0.4414\n",
      "Epoch 8 Batch 8800 Loss 1.9537 Accuracy 0.4414\n",
      "Epoch 8 Batch 8850 Loss 1.9535 Accuracy 0.4414\n",
      "Epoch 8 Batch 8900 Loss 1.9535 Accuracy 0.4414\n",
      "Epoch 8 Batch 8950 Loss 1.9534 Accuracy 0.4415\n",
      "Epoch 8 Batch 9000 Loss 1.9535 Accuracy 0.4415\n",
      "Epoch 8 Batch 9050 Loss 1.9536 Accuracy 0.4414\n",
      "Epoch 8 Batch 9100 Loss 1.9537 Accuracy 0.4414\n",
      "Epoch 8 Batch 9150 Loss 1.9536 Accuracy 0.4414\n",
      "Epoch 8 Batch 9200 Loss 1.9537 Accuracy 0.4414\n",
      "Epoch 8 Batch 9250 Loss 1.9533 Accuracy 0.4414\n",
      "Epoch 8 Batch 9300 Loss 1.9532 Accuracy 0.4415\n",
      "Epoch 8 Batch 9350 Loss 1.9531 Accuracy 0.4415\n",
      "Epoch 8 Batch 9400 Loss 1.9530 Accuracy 0.4415\n",
      "Epoch 8 Batch 9450 Loss 1.9527 Accuracy 0.4415\n",
      "Epoch 8 Batch 9500 Loss 1.9528 Accuracy 0.4415\n",
      "Epoch 8 Batch 9550 Loss 1.9528 Accuracy 0.4415\n",
      "Epoch 8 Batch 9600 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 9650 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 9700 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 9750 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 9800 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 9850 Loss 1.9531 Accuracy 0.4415\n",
      "Epoch 8 Batch 9900 Loss 1.9528 Accuracy 0.4415\n",
      "Epoch 8 Batch 9950 Loss 1.9529 Accuracy 0.4415\n",
      "Epoch 8 Batch 10000 Loss 1.9527 Accuracy 0.4415\n",
      "Epoch 8 Batch 10050 Loss 1.9526 Accuracy 0.4415\n",
      "Epoch 8 Batch 10100 Loss 1.9525 Accuracy 0.4415\n",
      "Epoch 8 Batch 10150 Loss 1.9524 Accuracy 0.4415\n",
      "Epoch 8 Batch 10200 Loss 1.9524 Accuracy 0.4415\n",
      "Epoch 8 Loss 1.9524 Accuracy 0.4415\n",
      "Time taken for 1 epoch: 431.6773793697357 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.7798 Accuracy 0.4334\n",
      "Epoch 9 Batch 50 Loss 1.9419 Accuracy 0.4427\n",
      "Epoch 9 Batch 100 Loss 1.9314 Accuracy 0.4465\n",
      "Epoch 9 Batch 150 Loss 1.9285 Accuracy 0.4459\n",
      "Epoch 9 Batch 200 Loss 1.9200 Accuracy 0.4454\n",
      "Epoch 9 Batch 250 Loss 1.9264 Accuracy 0.4443\n",
      "Epoch 9 Batch 300 Loss 1.9299 Accuracy 0.4438\n",
      "Epoch 9 Batch 350 Loss 1.9307 Accuracy 0.4443\n",
      "Epoch 9 Batch 400 Loss 1.9309 Accuracy 0.4443\n",
      "Epoch 9 Batch 450 Loss 1.9333 Accuracy 0.4442\n",
      "Epoch 9 Batch 500 Loss 1.9364 Accuracy 0.4440\n",
      "Epoch 9 Batch 550 Loss 1.9401 Accuracy 0.4436\n",
      "Epoch 9 Batch 600 Loss 1.9415 Accuracy 0.4434\n",
      "Epoch 9 Batch 650 Loss 1.9383 Accuracy 0.4436\n",
      "Epoch 9 Batch 700 Loss 1.9366 Accuracy 0.4436\n",
      "Epoch 9 Batch 750 Loss 1.9394 Accuracy 0.4432\n",
      "Epoch 9 Batch 800 Loss 1.9396 Accuracy 0.4432\n",
      "Epoch 9 Batch 850 Loss 1.9408 Accuracy 0.4431\n",
      "Epoch 9 Batch 900 Loss 1.9404 Accuracy 0.4431\n",
      "Epoch 9 Batch 950 Loss 1.9406 Accuracy 0.4431\n",
      "Epoch 9 Batch 1000 Loss 1.9423 Accuracy 0.4430\n",
      "Epoch 9 Batch 1050 Loss 1.9425 Accuracy 0.4430\n",
      "Epoch 9 Batch 1100 Loss 1.9413 Accuracy 0.4432\n",
      "Epoch 9 Batch 1150 Loss 1.9401 Accuracy 0.4433\n",
      "Epoch 9 Batch 1200 Loss 1.9386 Accuracy 0.4435\n",
      "Epoch 9 Batch 1250 Loss 1.9396 Accuracy 0.4435\n",
      "Epoch 9 Batch 1300 Loss 1.9400 Accuracy 0.4433\n",
      "Epoch 9 Batch 1350 Loss 1.9399 Accuracy 0.4433\n",
      "Epoch 9 Batch 1400 Loss 1.9397 Accuracy 0.4433\n",
      "Epoch 9 Batch 1450 Loss 1.9380 Accuracy 0.4435\n",
      "Epoch 9 Batch 1500 Loss 1.9376 Accuracy 0.4435\n",
      "Epoch 9 Batch 1550 Loss 1.9361 Accuracy 0.4436\n",
      "Epoch 9 Batch 1600 Loss 1.9347 Accuracy 0.4436\n",
      "Epoch 9 Batch 1650 Loss 1.9339 Accuracy 0.4436\n",
      "Epoch 9 Batch 1700 Loss 1.9326 Accuracy 0.4438\n",
      "Epoch 9 Batch 1750 Loss 1.9331 Accuracy 0.4438\n",
      "Epoch 9 Batch 1800 Loss 1.9332 Accuracy 0.4437\n",
      "Epoch 9 Batch 1850 Loss 1.9329 Accuracy 0.4438\n",
      "Epoch 9 Batch 1900 Loss 1.9330 Accuracy 0.4439\n",
      "Epoch 9 Batch 1950 Loss 1.9332 Accuracy 0.4438\n",
      "Epoch 9 Batch 2000 Loss 1.9324 Accuracy 0.4439\n",
      "Epoch 9 Batch 2050 Loss 1.9325 Accuracy 0.4438\n",
      "Epoch 9 Batch 2100 Loss 1.9320 Accuracy 0.4439\n",
      "Epoch 9 Batch 2150 Loss 1.9318 Accuracy 0.4439\n",
      "Epoch 9 Batch 2200 Loss 1.9315 Accuracy 0.4440\n",
      "Epoch 9 Batch 2250 Loss 1.9316 Accuracy 0.4439\n",
      "Epoch 9 Batch 2300 Loss 1.9321 Accuracy 0.4440\n",
      "Epoch 9 Batch 2350 Loss 1.9328 Accuracy 0.4440\n",
      "Epoch 9 Batch 2400 Loss 1.9326 Accuracy 0.4440\n",
      "Epoch 9 Batch 2450 Loss 1.9322 Accuracy 0.4440\n",
      "Epoch 9 Batch 2500 Loss 1.9329 Accuracy 0.4440\n",
      "Epoch 9 Batch 2550 Loss 1.9326 Accuracy 0.4440\n",
      "Epoch 9 Batch 2600 Loss 1.9328 Accuracy 0.4440\n",
      "Epoch 9 Batch 2650 Loss 1.9321 Accuracy 0.4440\n",
      "Epoch 9 Batch 2700 Loss 1.9325 Accuracy 0.4440\n",
      "Epoch 9 Batch 2750 Loss 1.9333 Accuracy 0.4440\n",
      "Epoch 9 Batch 2800 Loss 1.9345 Accuracy 0.4440\n",
      "Epoch 9 Batch 2850 Loss 1.9346 Accuracy 0.4439\n",
      "Epoch 9 Batch 2900 Loss 1.9350 Accuracy 0.4439\n",
      "Epoch 9 Batch 2950 Loss 1.9347 Accuracy 0.4439\n",
      "Epoch 9 Batch 3000 Loss 1.9350 Accuracy 0.4439\n",
      "Epoch 9 Batch 3050 Loss 1.9349 Accuracy 0.4439\n",
      "Epoch 9 Batch 3100 Loss 1.9351 Accuracy 0.4439\n",
      "Epoch 9 Batch 3150 Loss 1.9353 Accuracy 0.4438\n",
      "Epoch 9 Batch 3200 Loss 1.9358 Accuracy 0.4438\n",
      "Epoch 9 Batch 3250 Loss 1.9357 Accuracy 0.4438\n",
      "Epoch 9 Batch 3300 Loss 1.9355 Accuracy 0.4438\n",
      "Epoch 9 Batch 3350 Loss 1.9350 Accuracy 0.4439\n",
      "Epoch 9 Batch 3400 Loss 1.9351 Accuracy 0.4439\n",
      "Epoch 9 Batch 3450 Loss 1.9347 Accuracy 0.4439\n",
      "Epoch 9 Batch 3500 Loss 1.9348 Accuracy 0.4439\n",
      "Epoch 9 Batch 3550 Loss 1.9348 Accuracy 0.4438\n",
      "Epoch 9 Batch 3600 Loss 1.9350 Accuracy 0.4438\n",
      "Epoch 9 Batch 3650 Loss 1.9350 Accuracy 0.4438\n",
      "Epoch 9 Batch 3700 Loss 1.9346 Accuracy 0.4438\n",
      "Epoch 9 Batch 3750 Loss 1.9343 Accuracy 0.4438\n",
      "Epoch 9 Batch 3800 Loss 1.9344 Accuracy 0.4438\n",
      "Epoch 9 Batch 3850 Loss 1.9345 Accuracy 0.4438\n",
      "Epoch 9 Batch 3900 Loss 1.9343 Accuracy 0.4438\n",
      "Epoch 9 Batch 3950 Loss 1.9345 Accuracy 0.4438\n",
      "Epoch 9 Batch 4000 Loss 1.9345 Accuracy 0.4437\n",
      "Epoch 9 Batch 4050 Loss 1.9350 Accuracy 0.4437\n",
      "Epoch 9 Batch 4100 Loss 1.9352 Accuracy 0.4437\n",
      "Epoch 9 Batch 4150 Loss 1.9349 Accuracy 0.4437\n",
      "Epoch 9 Batch 4200 Loss 1.9346 Accuracy 0.4437\n",
      "Epoch 9 Batch 4250 Loss 1.9346 Accuracy 0.4437\n",
      "Epoch 9 Batch 4300 Loss 1.9344 Accuracy 0.4437\n",
      "Epoch 9 Batch 4350 Loss 1.9343 Accuracy 0.4437\n",
      "Epoch 9 Batch 4400 Loss 1.9338 Accuracy 0.4437\n",
      "Epoch 9 Batch 4450 Loss 1.9337 Accuracy 0.4438\n",
      "Epoch 9 Batch 4500 Loss 1.9336 Accuracy 0.4437\n",
      "Epoch 9 Batch 4550 Loss 1.9338 Accuracy 0.4437\n",
      "Epoch 9 Batch 4600 Loss 1.9339 Accuracy 0.4437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 4650 Loss 1.9342 Accuracy 0.4438\n",
      "Epoch 9 Batch 4700 Loss 1.9336 Accuracy 0.4438\n",
      "Epoch 9 Batch 4750 Loss 1.9335 Accuracy 0.4438\n",
      "Epoch 9 Batch 4800 Loss 1.9338 Accuracy 0.4438\n",
      "Epoch 9 Batch 4850 Loss 1.9332 Accuracy 0.4438\n",
      "Epoch 9 Batch 4900 Loss 1.9332 Accuracy 0.4438\n",
      "Epoch 9 Batch 4950 Loss 1.9329 Accuracy 0.4438\n",
      "Epoch 9 Batch 5000 Loss 1.9328 Accuracy 0.4438\n",
      "Epoch 9 Batch 5050 Loss 1.9328 Accuracy 0.4437\n",
      "Epoch 9 Batch 5100 Loss 1.9323 Accuracy 0.4438\n",
      "Epoch 9 Batch 5150 Loss 1.9327 Accuracy 0.4437\n",
      "Epoch 9 Batch 5200 Loss 1.9327 Accuracy 0.4437\n",
      "Epoch 9 Batch 5250 Loss 1.9321 Accuracy 0.4438\n",
      "Epoch 9 Batch 5300 Loss 1.9321 Accuracy 0.4437\n",
      "Epoch 9 Batch 5350 Loss 1.9320 Accuracy 0.4437\n",
      "Epoch 9 Batch 5400 Loss 1.9319 Accuracy 0.4437\n",
      "Epoch 9 Batch 5450 Loss 1.9322 Accuracy 0.4437\n",
      "Epoch 9 Batch 5500 Loss 1.9321 Accuracy 0.4437\n",
      "Epoch 9 Batch 5550 Loss 1.9320 Accuracy 0.4438\n",
      "Epoch 9 Batch 5600 Loss 1.9322 Accuracy 0.4437\n",
      "Epoch 9 Batch 5650 Loss 1.9325 Accuracy 0.4437\n",
      "Epoch 9 Batch 5700 Loss 1.9326 Accuracy 0.4437\n",
      "Epoch 9 Batch 5750 Loss 1.9331 Accuracy 0.4437\n",
      "Epoch 9 Batch 5800 Loss 1.9330 Accuracy 0.4437\n",
      "Epoch 9 Batch 5850 Loss 1.9329 Accuracy 0.4436\n",
      "Epoch 9 Batch 5900 Loss 1.9327 Accuracy 0.4436\n",
      "Epoch 9 Batch 5950 Loss 1.9323 Accuracy 0.4436\n",
      "Epoch 9 Batch 6000 Loss 1.9325 Accuracy 0.4436\n",
      "Epoch 9 Batch 6050 Loss 1.9326 Accuracy 0.4436\n",
      "Epoch 9 Batch 6100 Loss 1.9325 Accuracy 0.4436\n",
      "Epoch 9 Batch 6150 Loss 1.9325 Accuracy 0.4436\n",
      "Epoch 9 Batch 6200 Loss 1.9321 Accuracy 0.4436\n",
      "Epoch 9 Batch 6250 Loss 1.9321 Accuracy 0.4437\n",
      "Epoch 9 Batch 6300 Loss 1.9322 Accuracy 0.4437\n",
      "Epoch 9 Batch 6350 Loss 1.9322 Accuracy 0.4437\n",
      "Epoch 9 Batch 6400 Loss 1.9321 Accuracy 0.4437\n",
      "Epoch 9 Batch 6450 Loss 1.9321 Accuracy 0.4437\n",
      "Epoch 9 Batch 6500 Loss 1.9319 Accuracy 0.4437\n",
      "Epoch 9 Batch 6550 Loss 1.9319 Accuracy 0.4438\n",
      "Epoch 9 Batch 6600 Loss 1.9319 Accuracy 0.4438\n",
      "Epoch 9 Batch 6650 Loss 1.9319 Accuracy 0.4437\n",
      "Epoch 9 Batch 6700 Loss 1.9318 Accuracy 0.4437\n",
      "Epoch 9 Batch 6750 Loss 1.9319 Accuracy 0.4437\n",
      "Epoch 9 Batch 6800 Loss 1.9320 Accuracy 0.4437\n",
      "Epoch 9 Batch 6850 Loss 1.9320 Accuracy 0.4437\n",
      "Epoch 9 Batch 6900 Loss 1.9319 Accuracy 0.4437\n",
      "Epoch 9 Batch 6950 Loss 1.9317 Accuracy 0.4437\n",
      "Epoch 9 Batch 7000 Loss 1.9315 Accuracy 0.4438\n",
      "Epoch 9 Batch 7050 Loss 1.9314 Accuracy 0.4438\n",
      "Epoch 9 Batch 7100 Loss 1.9313 Accuracy 0.4438\n",
      "Epoch 9 Batch 7150 Loss 1.9310 Accuracy 0.4438\n",
      "Epoch 9 Batch 7200 Loss 1.9309 Accuracy 0.4438\n",
      "Epoch 9 Batch 7250 Loss 1.9308 Accuracy 0.4438\n",
      "Epoch 9 Batch 7300 Loss 1.9306 Accuracy 0.4438\n",
      "Epoch 9 Batch 7350 Loss 1.9306 Accuracy 0.4438\n",
      "Epoch 9 Batch 7400 Loss 1.9304 Accuracy 0.4438\n",
      "Epoch 9 Batch 7450 Loss 1.9305 Accuracy 0.4438\n",
      "Epoch 9 Batch 7500 Loss 1.9306 Accuracy 0.4438\n",
      "Epoch 9 Batch 7550 Loss 1.9308 Accuracy 0.4438\n",
      "Epoch 9 Batch 7600 Loss 1.9304 Accuracy 0.4438\n",
      "Epoch 9 Batch 7650 Loss 1.9304 Accuracy 0.4438\n",
      "Epoch 9 Batch 7700 Loss 1.9303 Accuracy 0.4439\n",
      "Epoch 9 Batch 7750 Loss 1.9303 Accuracy 0.4439\n",
      "Epoch 9 Batch 7800 Loss 1.9303 Accuracy 0.4439\n",
      "Epoch 9 Batch 7850 Loss 1.9301 Accuracy 0.4439\n",
      "Epoch 9 Batch 7900 Loss 1.9301 Accuracy 0.4439\n",
      "Epoch 9 Batch 7950 Loss 1.9303 Accuracy 0.4439\n",
      "Epoch 9 Batch 8000 Loss 1.9299 Accuracy 0.4439\n",
      "Epoch 9 Batch 8050 Loss 1.9298 Accuracy 0.4439\n",
      "Epoch 9 Batch 8100 Loss 1.9297 Accuracy 0.4439\n",
      "Epoch 9 Batch 8150 Loss 1.9293 Accuracy 0.4440\n",
      "Epoch 9 Batch 8200 Loss 1.9293 Accuracy 0.4440\n",
      "Epoch 9 Batch 8250 Loss 1.9294 Accuracy 0.4439\n",
      "Epoch 9 Batch 8300 Loss 1.9293 Accuracy 0.4439\n",
      "Epoch 9 Batch 8350 Loss 1.9291 Accuracy 0.4440\n",
      "Epoch 9 Batch 8400 Loss 1.9287 Accuracy 0.4440\n",
      "Epoch 9 Batch 8450 Loss 1.9286 Accuracy 0.4440\n",
      "Epoch 9 Batch 8500 Loss 1.9285 Accuracy 0.4440\n",
      "Epoch 9 Batch 8550 Loss 1.9285 Accuracy 0.4440\n",
      "Epoch 9 Batch 8600 Loss 1.9285 Accuracy 0.4440\n",
      "Epoch 9 Batch 8650 Loss 1.9284 Accuracy 0.4440\n",
      "Epoch 9 Batch 8700 Loss 1.9283 Accuracy 0.4441\n",
      "Epoch 9 Batch 8750 Loss 1.9285 Accuracy 0.4441\n",
      "Epoch 9 Batch 8800 Loss 1.9286 Accuracy 0.4441\n",
      "Epoch 9 Batch 8850 Loss 1.9285 Accuracy 0.4441\n",
      "Epoch 9 Batch 8900 Loss 1.9286 Accuracy 0.4441\n",
      "Epoch 9 Batch 8950 Loss 1.9286 Accuracy 0.4441\n",
      "Epoch 9 Batch 9000 Loss 1.9285 Accuracy 0.4441\n",
      "Epoch 9 Batch 9050 Loss 1.9283 Accuracy 0.4441\n",
      "Epoch 9 Batch 9100 Loss 1.9283 Accuracy 0.4441\n",
      "Epoch 9 Batch 9150 Loss 1.9283 Accuracy 0.4441\n",
      "Epoch 9 Batch 9200 Loss 1.9282 Accuracy 0.4441\n",
      "Epoch 9 Batch 9250 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9300 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9350 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9400 Loss 1.9278 Accuracy 0.4441\n",
      "Epoch 9 Batch 9450 Loss 1.9278 Accuracy 0.4442\n",
      "Epoch 9 Batch 9500 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9550 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9600 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9650 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9700 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9750 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 9800 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9850 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9900 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 9950 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 10000 Loss 1.9280 Accuracy 0.4441\n",
      "Epoch 9 Batch 10050 Loss 1.9279 Accuracy 0.4441\n",
      "Epoch 9 Batch 10100 Loss 1.9276 Accuracy 0.4441\n",
      "Epoch 9 Batch 10150 Loss 1.9275 Accuracy 0.4441\n",
      "Epoch 9 Batch 10200 Loss 1.9276 Accuracy 0.4441\n",
      "Epoch 9 Loss 1.9275 Accuracy 0.4441\n",
      "Time taken for 1 epoch: 431.64261388778687 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.7337 Accuracy 0.4638\n",
      "Epoch 10 Batch 50 Loss 1.8945 Accuracy 0.4478\n",
      "Epoch 10 Batch 100 Loss 1.8995 Accuracy 0.4479\n",
      "Epoch 10 Batch 150 Loss 1.8869 Accuracy 0.4488\n",
      "Epoch 10 Batch 200 Loss 1.8871 Accuracy 0.4492\n",
      "Epoch 10 Batch 250 Loss 1.8981 Accuracy 0.4474\n",
      "Epoch 10 Batch 300 Loss 1.9067 Accuracy 0.4465\n",
      "Epoch 10 Batch 350 Loss 1.9076 Accuracy 0.4465\n",
      "Epoch 10 Batch 400 Loss 1.9067 Accuracy 0.4467\n",
      "Epoch 10 Batch 450 Loss 1.9127 Accuracy 0.4461\n",
      "Epoch 10 Batch 500 Loss 1.9112 Accuracy 0.4465\n",
      "Epoch 10 Batch 550 Loss 1.9117 Accuracy 0.4465\n",
      "Epoch 10 Batch 600 Loss 1.9173 Accuracy 0.4460\n",
      "Epoch 10 Batch 650 Loss 1.9149 Accuracy 0.4459\n",
      "Epoch 10 Batch 700 Loss 1.9149 Accuracy 0.4460\n",
      "Epoch 10 Batch 750 Loss 1.9139 Accuracy 0.4458\n",
      "Epoch 10 Batch 800 Loss 1.9158 Accuracy 0.4457\n",
      "Epoch 10 Batch 850 Loss 1.9163 Accuracy 0.4456\n",
      "Epoch 10 Batch 900 Loss 1.9185 Accuracy 0.4454\n",
      "Epoch 10 Batch 950 Loss 1.9166 Accuracy 0.4454\n",
      "Epoch 10 Batch 1000 Loss 1.9169 Accuracy 0.4452\n",
      "Epoch 10 Batch 1050 Loss 1.9160 Accuracy 0.4454\n",
      "Epoch 10 Batch 1100 Loss 1.9162 Accuracy 0.4453\n",
      "Epoch 10 Batch 1150 Loss 1.9164 Accuracy 0.4451\n",
      "Epoch 10 Batch 1200 Loss 1.9175 Accuracy 0.4451\n",
      "Epoch 10 Batch 1250 Loss 1.9186 Accuracy 0.4449\n",
      "Epoch 10 Batch 1300 Loss 1.9182 Accuracy 0.4449\n",
      "Epoch 10 Batch 1350 Loss 1.9175 Accuracy 0.4450\n",
      "Epoch 10 Batch 1400 Loss 1.9171 Accuracy 0.4450\n",
      "Epoch 10 Batch 1450 Loss 1.9172 Accuracy 0.4450\n",
      "Epoch 10 Batch 1500 Loss 1.9179 Accuracy 0.4450\n",
      "Epoch 10 Batch 1550 Loss 1.9165 Accuracy 0.4450\n",
      "Epoch 10 Batch 1600 Loss 1.9162 Accuracy 0.4450\n",
      "Epoch 10 Batch 1650 Loss 1.9157 Accuracy 0.4451\n",
      "Epoch 10 Batch 1700 Loss 1.9149 Accuracy 0.4452\n",
      "Epoch 10 Batch 1750 Loss 1.9150 Accuracy 0.4452\n",
      "Epoch 10 Batch 1800 Loss 1.9147 Accuracy 0.4453\n",
      "Epoch 10 Batch 1850 Loss 1.9138 Accuracy 0.4455\n",
      "Epoch 10 Batch 1900 Loss 1.9137 Accuracy 0.4454\n",
      "Epoch 10 Batch 1950 Loss 1.9143 Accuracy 0.4454\n",
      "Epoch 10 Batch 2000 Loss 1.9136 Accuracy 0.4455\n",
      "Epoch 10 Batch 2050 Loss 1.9128 Accuracy 0.4455\n",
      "Epoch 10 Batch 2100 Loss 1.9120 Accuracy 0.4456\n",
      "Epoch 10 Batch 2150 Loss 1.9118 Accuracy 0.4457\n",
      "Epoch 10 Batch 2200 Loss 1.9118 Accuracy 0.4457\n",
      "Epoch 10 Batch 2250 Loss 1.9118 Accuracy 0.4458\n",
      "Epoch 10 Batch 2300 Loss 1.9111 Accuracy 0.4459\n",
      "Epoch 10 Batch 2350 Loss 1.9114 Accuracy 0.4459\n",
      "Epoch 10 Batch 2400 Loss 1.9113 Accuracy 0.4458\n",
      "Epoch 10 Batch 2450 Loss 1.9119 Accuracy 0.4458\n",
      "Epoch 10 Batch 2500 Loss 1.9122 Accuracy 0.4457\n",
      "Epoch 10 Batch 2550 Loss 1.9122 Accuracy 0.4458\n",
      "Epoch 10 Batch 2600 Loss 1.9123 Accuracy 0.4458\n",
      "Epoch 10 Batch 2650 Loss 1.9124 Accuracy 0.4458\n",
      "Epoch 10 Batch 2700 Loss 1.9131 Accuracy 0.4457\n",
      "Epoch 10 Batch 2750 Loss 1.9133 Accuracy 0.4457\n",
      "Epoch 10 Batch 2800 Loss 1.9131 Accuracy 0.4457\n",
      "Epoch 10 Batch 2850 Loss 1.9131 Accuracy 0.4456\n",
      "Epoch 10 Batch 2900 Loss 1.9126 Accuracy 0.4457\n",
      "Epoch 10 Batch 2950 Loss 1.9129 Accuracy 0.4457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 3000 Loss 1.9133 Accuracy 0.4456\n",
      "Epoch 10 Batch 3050 Loss 1.9134 Accuracy 0.4456\n",
      "Epoch 10 Batch 3100 Loss 1.9127 Accuracy 0.4456\n",
      "Epoch 10 Batch 3150 Loss 1.9126 Accuracy 0.4457\n",
      "Epoch 10 Batch 3200 Loss 1.9133 Accuracy 0.4456\n",
      "Epoch 10 Batch 3250 Loss 1.9138 Accuracy 0.4456\n",
      "Epoch 10 Batch 3300 Loss 1.9137 Accuracy 0.4457\n",
      "Epoch 10 Batch 3350 Loss 1.9133 Accuracy 0.4457\n",
      "Epoch 10 Batch 3400 Loss 1.9135 Accuracy 0.4457\n",
      "Epoch 10 Batch 3450 Loss 1.9135 Accuracy 0.4457\n",
      "Epoch 10 Batch 3500 Loss 1.9134 Accuracy 0.4457\n",
      "Epoch 10 Batch 3550 Loss 1.9130 Accuracy 0.4457\n",
      "Epoch 10 Batch 3600 Loss 1.9122 Accuracy 0.4458\n",
      "Epoch 10 Batch 3650 Loss 1.9119 Accuracy 0.4458\n",
      "Epoch 10 Batch 3700 Loss 1.9118 Accuracy 0.4458\n",
      "Epoch 10 Batch 3750 Loss 1.9117 Accuracy 0.4458\n",
      "Epoch 10 Batch 3800 Loss 1.9126 Accuracy 0.4457\n",
      "Epoch 10 Batch 3850 Loss 1.9130 Accuracy 0.4457\n",
      "Epoch 10 Batch 3900 Loss 1.9129 Accuracy 0.4457\n",
      "Epoch 10 Batch 3950 Loss 1.9128 Accuracy 0.4457\n",
      "Epoch 10 Batch 4000 Loss 1.9124 Accuracy 0.4457\n",
      "Epoch 10 Batch 4050 Loss 1.9126 Accuracy 0.4457\n",
      "Epoch 10 Batch 4100 Loss 1.9127 Accuracy 0.4457\n",
      "Epoch 10 Batch 4150 Loss 1.9123 Accuracy 0.4457\n",
      "Epoch 10 Batch 4200 Loss 1.9124 Accuracy 0.4457\n",
      "Epoch 10 Batch 4250 Loss 1.9127 Accuracy 0.4456\n",
      "Epoch 10 Batch 4300 Loss 1.9127 Accuracy 0.4456\n",
      "Epoch 10 Batch 4350 Loss 1.9129 Accuracy 0.4457\n",
      "Epoch 10 Batch 4400 Loss 1.9129 Accuracy 0.4457\n",
      "Epoch 10 Batch 4450 Loss 1.9132 Accuracy 0.4456\n",
      "Epoch 10 Batch 4500 Loss 1.9133 Accuracy 0.4456\n",
      "Epoch 10 Batch 4550 Loss 1.9133 Accuracy 0.4456\n",
      "Epoch 10 Batch 4600 Loss 1.9136 Accuracy 0.4456\n",
      "Epoch 10 Batch 4650 Loss 1.9132 Accuracy 0.4456\n",
      "Epoch 10 Batch 4700 Loss 1.9132 Accuracy 0.4456\n",
      "Epoch 10 Batch 4750 Loss 1.9127 Accuracy 0.4457\n",
      "Epoch 10 Batch 4800 Loss 1.9122 Accuracy 0.4457\n",
      "Epoch 10 Batch 4850 Loss 1.9121 Accuracy 0.4457\n",
      "Epoch 10 Batch 4900 Loss 1.9121 Accuracy 0.4457\n",
      "Epoch 10 Batch 4950 Loss 1.9122 Accuracy 0.4457\n",
      "Epoch 10 Batch 5000 Loss 1.9123 Accuracy 0.4457\n",
      "Epoch 10 Batch 5050 Loss 1.9124 Accuracy 0.4457\n",
      "Epoch 10 Batch 5100 Loss 1.9124 Accuracy 0.4456\n",
      "Epoch 10 Batch 5150 Loss 1.9122 Accuracy 0.4457\n",
      "Epoch 10 Batch 5200 Loss 1.9118 Accuracy 0.4457\n",
      "Epoch 10 Batch 5250 Loss 1.9116 Accuracy 0.4457\n",
      "Epoch 10 Batch 5300 Loss 1.9119 Accuracy 0.4457\n",
      "Epoch 10 Batch 5350 Loss 1.9117 Accuracy 0.4457\n",
      "Epoch 10 Batch 5400 Loss 1.9116 Accuracy 0.4457\n",
      "Epoch 10 Batch 5450 Loss 1.9115 Accuracy 0.4456\n",
      "Epoch 10 Batch 5500 Loss 1.9117 Accuracy 0.4456\n",
      "Epoch 10 Batch 5550 Loss 1.9117 Accuracy 0.4456\n",
      "Epoch 10 Batch 5600 Loss 1.9114 Accuracy 0.4456\n",
      "Epoch 10 Batch 5650 Loss 1.9117 Accuracy 0.4456\n",
      "Epoch 10 Batch 5700 Loss 1.9116 Accuracy 0.4456\n",
      "Epoch 10 Batch 5750 Loss 1.9113 Accuracy 0.4456\n",
      "Epoch 10 Batch 5800 Loss 1.9110 Accuracy 0.4456\n",
      "Epoch 10 Batch 5850 Loss 1.9112 Accuracy 0.4456\n",
      "Epoch 10 Batch 5900 Loss 1.9111 Accuracy 0.4456\n",
      "Epoch 10 Batch 5950 Loss 1.9112 Accuracy 0.4456\n",
      "Epoch 10 Batch 6000 Loss 1.9109 Accuracy 0.4456\n",
      "Epoch 10 Batch 6050 Loss 1.9109 Accuracy 0.4456\n",
      "Epoch 10 Batch 6100 Loss 1.9108 Accuracy 0.4457\n",
      "Epoch 10 Batch 6150 Loss 1.9108 Accuracy 0.4456\n",
      "Epoch 10 Batch 6200 Loss 1.9108 Accuracy 0.4457\n",
      "Epoch 10 Batch 6250 Loss 1.9106 Accuracy 0.4457\n",
      "Epoch 10 Batch 6300 Loss 1.9106 Accuracy 0.4457\n",
      "Epoch 10 Batch 6350 Loss 1.9106 Accuracy 0.4457\n",
      "Epoch 10 Batch 6400 Loss 1.9105 Accuracy 0.4457\n",
      "Epoch 10 Batch 6450 Loss 1.9104 Accuracy 0.4457\n",
      "Epoch 10 Batch 6500 Loss 1.9103 Accuracy 0.4458\n",
      "Epoch 10 Batch 6550 Loss 1.9102 Accuracy 0.4458\n",
      "Epoch 10 Batch 6600 Loss 1.9101 Accuracy 0.4457\n",
      "Epoch 10 Batch 6650 Loss 1.9100 Accuracy 0.4457\n",
      "Epoch 10 Batch 6700 Loss 1.9099 Accuracy 0.4458\n",
      "Epoch 10 Batch 6750 Loss 1.9100 Accuracy 0.4458\n",
      "Epoch 10 Batch 6800 Loss 1.9100 Accuracy 0.4458\n",
      "Epoch 10 Batch 6850 Loss 1.9099 Accuracy 0.4458\n",
      "Epoch 10 Batch 6900 Loss 1.9100 Accuracy 0.4458\n",
      "Epoch 10 Batch 6950 Loss 1.9099 Accuracy 0.4458\n",
      "Epoch 10 Batch 7000 Loss 1.9098 Accuracy 0.4458\n",
      "Epoch 10 Batch 7050 Loss 1.9097 Accuracy 0.4458\n",
      "Epoch 10 Batch 7100 Loss 1.9095 Accuracy 0.4458\n",
      "Epoch 10 Batch 7150 Loss 1.9095 Accuracy 0.4458\n",
      "Epoch 10 Batch 7200 Loss 1.9096 Accuracy 0.4458\n",
      "Epoch 10 Batch 7250 Loss 1.9094 Accuracy 0.4458\n",
      "Epoch 10 Batch 7300 Loss 1.9095 Accuracy 0.4458\n",
      "Epoch 10 Batch 7350 Loss 1.9096 Accuracy 0.4458\n",
      "Epoch 10 Batch 7400 Loss 1.9096 Accuracy 0.4458\n",
      "Epoch 10 Batch 7450 Loss 1.9093 Accuracy 0.4458\n",
      "Epoch 10 Batch 7500 Loss 1.9091 Accuracy 0.4458\n",
      "Epoch 10 Batch 7550 Loss 1.9089 Accuracy 0.4459\n",
      "Epoch 10 Batch 7600 Loss 1.9088 Accuracy 0.4459\n",
      "Epoch 10 Batch 7650 Loss 1.9089 Accuracy 0.4459\n",
      "Epoch 10 Batch 7700 Loss 1.9090 Accuracy 0.4459\n",
      "Epoch 10 Batch 7750 Loss 1.9089 Accuracy 0.4459\n",
      "Epoch 10 Batch 7800 Loss 1.9088 Accuracy 0.4459\n",
      "Epoch 10 Batch 7850 Loss 1.9085 Accuracy 0.4459\n",
      "Epoch 10 Batch 7900 Loss 1.9084 Accuracy 0.4459\n",
      "Epoch 10 Batch 7950 Loss 1.9085 Accuracy 0.4459\n",
      "Epoch 10 Batch 8000 Loss 1.9085 Accuracy 0.4459\n",
      "Epoch 10 Batch 8050 Loss 1.9083 Accuracy 0.4459\n",
      "Epoch 10 Batch 8100 Loss 1.9085 Accuracy 0.4459\n",
      "Epoch 10 Batch 8150 Loss 1.9082 Accuracy 0.4460\n",
      "Epoch 10 Batch 8200 Loss 1.9080 Accuracy 0.4460\n",
      "Epoch 10 Batch 8250 Loss 1.9078 Accuracy 0.4460\n",
      "Epoch 10 Batch 8300 Loss 1.9079 Accuracy 0.4460\n",
      "Epoch 10 Batch 8350 Loss 1.9083 Accuracy 0.4460\n",
      "Epoch 10 Batch 8400 Loss 1.9081 Accuracy 0.4460\n",
      "Epoch 10 Batch 8450 Loss 1.9079 Accuracy 0.4460\n",
      "Epoch 10 Batch 8500 Loss 1.9080 Accuracy 0.4460\n",
      "Epoch 10 Batch 8550 Loss 1.9079 Accuracy 0.4460\n",
      "Epoch 10 Batch 8600 Loss 1.9080 Accuracy 0.4460\n",
      "Epoch 10 Batch 8650 Loss 1.9080 Accuracy 0.4460\n",
      "Epoch 10 Batch 8700 Loss 1.9078 Accuracy 0.4460\n",
      "Epoch 10 Batch 8750 Loss 1.9079 Accuracy 0.4460\n",
      "Epoch 10 Batch 8800 Loss 1.9078 Accuracy 0.4460\n",
      "Epoch 10 Batch 8850 Loss 1.9077 Accuracy 0.4460\n",
      "Epoch 10 Batch 8900 Loss 1.9077 Accuracy 0.4460\n",
      "Epoch 10 Batch 8950 Loss 1.9078 Accuracy 0.4460\n",
      "Epoch 10 Batch 9000 Loss 1.9075 Accuracy 0.4460\n",
      "Epoch 10 Batch 9050 Loss 1.9074 Accuracy 0.4461\n",
      "Epoch 10 Batch 9100 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9150 Loss 1.9075 Accuracy 0.4460\n",
      "Epoch 10 Batch 9200 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9250 Loss 1.9077 Accuracy 0.4460\n",
      "Epoch 10 Batch 9300 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9350 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9400 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9450 Loss 1.9076 Accuracy 0.4460\n",
      "Epoch 10 Batch 9500 Loss 1.9074 Accuracy 0.4460\n",
      "Epoch 10 Batch 9550 Loss 1.9074 Accuracy 0.4460\n",
      "Epoch 10 Batch 9600 Loss 1.9074 Accuracy 0.4460\n",
      "Epoch 10 Batch 9650 Loss 1.9071 Accuracy 0.4461\n",
      "Epoch 10 Batch 9700 Loss 1.9072 Accuracy 0.4461\n",
      "Epoch 10 Batch 9750 Loss 1.9073 Accuracy 0.4460\n",
      "Epoch 10 Batch 9800 Loss 1.9071 Accuracy 0.4461\n",
      "Epoch 10 Batch 9850 Loss 1.9070 Accuracy 0.4461\n",
      "Epoch 10 Batch 9900 Loss 1.9070 Accuracy 0.4461\n",
      "Epoch 10 Batch 9950 Loss 1.9070 Accuracy 0.4461\n",
      "Epoch 10 Batch 10000 Loss 1.9069 Accuracy 0.4461\n",
      "Epoch 10 Batch 10050 Loss 1.9069 Accuracy 0.4461\n",
      "Epoch 10 Batch 10100 Loss 1.9069 Accuracy 0.4461\n",
      "Epoch 10 Batch 10150 Loss 1.9067 Accuracy 0.4461\n",
      "Epoch 10 Batch 10200 Loss 1.9069 Accuracy 0.4461\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Epoch 10 Loss 1.9069 Accuracy 0.4461\n",
      "Time taken for 1 epoch: 431.7898733615875 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.8191 Accuracy 0.4597\n",
      "Epoch 11 Batch 50 Loss 1.8876 Accuracy 0.4454\n",
      "Epoch 11 Batch 100 Loss 1.8930 Accuracy 0.4481\n",
      "Epoch 11 Batch 150 Loss 1.8865 Accuracy 0.4480\n",
      "Epoch 11 Batch 200 Loss 1.8836 Accuracy 0.4482\n",
      "Epoch 11 Batch 250 Loss 1.8851 Accuracy 0.4486\n",
      "Epoch 11 Batch 300 Loss 1.8882 Accuracy 0.4484\n",
      "Epoch 11 Batch 350 Loss 1.8902 Accuracy 0.4480\n",
      "Epoch 11 Batch 400 Loss 1.8901 Accuracy 0.4479\n",
      "Epoch 11 Batch 450 Loss 1.8937 Accuracy 0.4476\n",
      "Epoch 11 Batch 500 Loss 1.8919 Accuracy 0.4474\n",
      "Epoch 11 Batch 550 Loss 1.8928 Accuracy 0.4476\n",
      "Epoch 11 Batch 600 Loss 1.8932 Accuracy 0.4479\n",
      "Epoch 11 Batch 650 Loss 1.8932 Accuracy 0.4480\n",
      "Epoch 11 Batch 700 Loss 1.8937 Accuracy 0.4480\n",
      "Epoch 11 Batch 750 Loss 1.8975 Accuracy 0.4474\n",
      "Epoch 11 Batch 800 Loss 1.8966 Accuracy 0.4474\n",
      "Epoch 11 Batch 850 Loss 1.8972 Accuracy 0.4472\n",
      "Epoch 11 Batch 900 Loss 1.8984 Accuracy 0.4471\n",
      "Epoch 11 Batch 950 Loss 1.8973 Accuracy 0.4475\n",
      "Epoch 11 Batch 1000 Loss 1.8981 Accuracy 0.4474\n",
      "Epoch 11 Batch 1050 Loss 1.8983 Accuracy 0.4472\n",
      "Epoch 11 Batch 1100 Loss 1.8986 Accuracy 0.4472\n",
      "Epoch 11 Batch 1150 Loss 1.8991 Accuracy 0.4472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 1200 Loss 1.8989 Accuracy 0.4472\n",
      "Epoch 11 Batch 1250 Loss 1.8974 Accuracy 0.4474\n",
      "Epoch 11 Batch 1300 Loss 1.8979 Accuracy 0.4473\n",
      "Epoch 11 Batch 1350 Loss 1.8978 Accuracy 0.4473\n",
      "Epoch 11 Batch 1400 Loss 1.8985 Accuracy 0.4472\n",
      "Epoch 11 Batch 1450 Loss 1.8988 Accuracy 0.4472\n",
      "Epoch 11 Batch 1500 Loss 1.8977 Accuracy 0.4472\n",
      "Epoch 11 Batch 1550 Loss 1.8957 Accuracy 0.4473\n",
      "Epoch 11 Batch 1600 Loss 1.8956 Accuracy 0.4474\n",
      "Epoch 11 Batch 1650 Loss 1.8947 Accuracy 0.4475\n",
      "Epoch 11 Batch 1700 Loss 1.8945 Accuracy 0.4476\n",
      "Epoch 11 Batch 1750 Loss 1.8939 Accuracy 0.4477\n",
      "Epoch 11 Batch 1800 Loss 1.8935 Accuracy 0.4478\n",
      "Epoch 11 Batch 1850 Loss 1.8941 Accuracy 0.4477\n",
      "Epoch 11 Batch 1900 Loss 1.8931 Accuracy 0.4477\n",
      "Epoch 11 Batch 1950 Loss 1.8920 Accuracy 0.4478\n",
      "Epoch 11 Batch 2000 Loss 1.8918 Accuracy 0.4478\n",
      "Epoch 11 Batch 2050 Loss 1.8918 Accuracy 0.4478\n",
      "Epoch 11 Batch 2100 Loss 1.8921 Accuracy 0.4478\n",
      "Epoch 11 Batch 2150 Loss 1.8924 Accuracy 0.4478\n",
      "Epoch 11 Batch 2200 Loss 1.8928 Accuracy 0.4478\n",
      "Epoch 11 Batch 2250 Loss 1.8925 Accuracy 0.4479\n",
      "Epoch 11 Batch 2300 Loss 1.8925 Accuracy 0.4479\n",
      "Epoch 11 Batch 2350 Loss 1.8934 Accuracy 0.4477\n",
      "Epoch 11 Batch 2400 Loss 1.8932 Accuracy 0.4478\n",
      "Epoch 11 Batch 2450 Loss 1.8932 Accuracy 0.4478\n",
      "Epoch 11 Batch 2500 Loss 1.8933 Accuracy 0.4479\n",
      "Epoch 11 Batch 2550 Loss 1.8938 Accuracy 0.4478\n",
      "Epoch 11 Batch 2600 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 2650 Loss 1.8937 Accuracy 0.4479\n",
      "Epoch 11 Batch 2700 Loss 1.8938 Accuracy 0.4479\n",
      "Epoch 11 Batch 2750 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 2800 Loss 1.8940 Accuracy 0.4479\n",
      "Epoch 11 Batch 2850 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 2900 Loss 1.8940 Accuracy 0.4479\n",
      "Epoch 11 Batch 2950 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 3000 Loss 1.8936 Accuracy 0.4479\n",
      "Epoch 11 Batch 3050 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 3100 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 3150 Loss 1.8929 Accuracy 0.4479\n",
      "Epoch 11 Batch 3200 Loss 1.8933 Accuracy 0.4478\n",
      "Epoch 11 Batch 3250 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 3300 Loss 1.8937 Accuracy 0.4479\n",
      "Epoch 11 Batch 3350 Loss 1.8938 Accuracy 0.4478\n",
      "Epoch 11 Batch 3400 Loss 1.8942 Accuracy 0.4478\n",
      "Epoch 11 Batch 3450 Loss 1.8938 Accuracy 0.4479\n",
      "Epoch 11 Batch 3500 Loss 1.8940 Accuracy 0.4479\n",
      "Epoch 11 Batch 3550 Loss 1.8935 Accuracy 0.4478\n",
      "Epoch 11 Batch 3600 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 3650 Loss 1.8931 Accuracy 0.4479\n",
      "Epoch 11 Batch 3700 Loss 1.8924 Accuracy 0.4479\n",
      "Epoch 11 Batch 3750 Loss 1.8925 Accuracy 0.4479\n",
      "Epoch 11 Batch 3800 Loss 1.8930 Accuracy 0.4478\n",
      "Epoch 11 Batch 3850 Loss 1.8930 Accuracy 0.4478\n",
      "Epoch 11 Batch 3900 Loss 1.8931 Accuracy 0.4478\n",
      "Epoch 11 Batch 3950 Loss 1.8932 Accuracy 0.4478\n",
      "Epoch 11 Batch 4000 Loss 1.8935 Accuracy 0.4478\n",
      "Epoch 11 Batch 4050 Loss 1.8937 Accuracy 0.4478\n",
      "Epoch 11 Batch 4100 Loss 1.8940 Accuracy 0.4478\n",
      "Epoch 11 Batch 4150 Loss 1.8942 Accuracy 0.4478\n",
      "Epoch 11 Batch 4200 Loss 1.8942 Accuracy 0.4478\n",
      "Epoch 11 Batch 4250 Loss 1.8940 Accuracy 0.4478\n",
      "Epoch 11 Batch 4300 Loss 1.8942 Accuracy 0.4478\n",
      "Epoch 11 Batch 4350 Loss 1.8941 Accuracy 0.4478\n",
      "Epoch 11 Batch 4400 Loss 1.8935 Accuracy 0.4478\n",
      "Epoch 11 Batch 4450 Loss 1.8935 Accuracy 0.4479\n",
      "Epoch 11 Batch 4500 Loss 1.8937 Accuracy 0.4478\n",
      "Epoch 11 Batch 4550 Loss 1.8938 Accuracy 0.4478\n",
      "Epoch 11 Batch 4600 Loss 1.8935 Accuracy 0.4478\n",
      "Epoch 11 Batch 4650 Loss 1.8936 Accuracy 0.4478\n",
      "Epoch 11 Batch 4700 Loss 1.8934 Accuracy 0.4479\n",
      "Epoch 11 Batch 4750 Loss 1.8929 Accuracy 0.4479\n",
      "Epoch 11 Batch 4800 Loss 1.8930 Accuracy 0.4479\n",
      "Epoch 11 Batch 4850 Loss 1.8932 Accuracy 0.4478\n",
      "Epoch 11 Batch 4900 Loss 1.8930 Accuracy 0.4478\n",
      "Epoch 11 Batch 4950 Loss 1.8931 Accuracy 0.4478\n",
      "Epoch 11 Batch 5000 Loss 1.8930 Accuracy 0.4478\n",
      "Epoch 11 Batch 5050 Loss 1.8928 Accuracy 0.4478\n",
      "Epoch 11 Batch 5100 Loss 1.8930 Accuracy 0.4478\n",
      "Epoch 11 Batch 5150 Loss 1.8931 Accuracy 0.4478\n",
      "Epoch 11 Batch 5200 Loss 1.8932 Accuracy 0.4477\n",
      "Epoch 11 Batch 5250 Loss 1.8932 Accuracy 0.4477\n",
      "Epoch 11 Batch 5300 Loss 1.8929 Accuracy 0.4477\n",
      "Epoch 11 Batch 5350 Loss 1.8927 Accuracy 0.4477\n",
      "Epoch 11 Batch 5400 Loss 1.8928 Accuracy 0.4477\n",
      "Epoch 11 Batch 5450 Loss 1.8927 Accuracy 0.4477\n",
      "Epoch 11 Batch 5500 Loss 1.8928 Accuracy 0.4477\n",
      "Epoch 11 Batch 5550 Loss 1.8928 Accuracy 0.4476\n",
      "Epoch 11 Batch 5600 Loss 1.8928 Accuracy 0.4477\n",
      "Epoch 11 Batch 5650 Loss 1.8927 Accuracy 0.4477\n",
      "Epoch 11 Batch 5700 Loss 1.8923 Accuracy 0.4477\n",
      "Epoch 11 Batch 5750 Loss 1.8923 Accuracy 0.4477\n",
      "Epoch 11 Batch 5800 Loss 1.8921 Accuracy 0.4477\n",
      "Epoch 11 Batch 5850 Loss 1.8918 Accuracy 0.4477\n",
      "Epoch 11 Batch 5900 Loss 1.8917 Accuracy 0.4477\n",
      "Epoch 11 Batch 5950 Loss 1.8917 Accuracy 0.4477\n",
      "Epoch 11 Batch 6000 Loss 1.8917 Accuracy 0.4477\n",
      "Epoch 11 Batch 6050 Loss 1.8916 Accuracy 0.4477\n",
      "Epoch 11 Batch 6100 Loss 1.8917 Accuracy 0.4477\n",
      "Epoch 11 Batch 6150 Loss 1.8917 Accuracy 0.4478\n",
      "Epoch 11 Batch 6200 Loss 1.8919 Accuracy 0.4478\n",
      "Epoch 11 Batch 6250 Loss 1.8922 Accuracy 0.4477\n",
      "Epoch 11 Batch 6300 Loss 1.8919 Accuracy 0.4477\n",
      "Epoch 11 Batch 6350 Loss 1.8920 Accuracy 0.4477\n",
      "Epoch 11 Batch 6400 Loss 1.8919 Accuracy 0.4477\n",
      "Epoch 11 Batch 6450 Loss 1.8919 Accuracy 0.4477\n",
      "Epoch 11 Batch 6500 Loss 1.8919 Accuracy 0.4477\n",
      "Epoch 11 Batch 6550 Loss 1.8916 Accuracy 0.4477\n",
      "Epoch 11 Batch 6600 Loss 1.8917 Accuracy 0.4477\n",
      "Epoch 11 Batch 6650 Loss 1.8912 Accuracy 0.4477\n",
      "Epoch 11 Batch 6700 Loss 1.8909 Accuracy 0.4478\n",
      "Epoch 11 Batch 6750 Loss 1.8910 Accuracy 0.4478\n",
      "Epoch 11 Batch 6800 Loss 1.8909 Accuracy 0.4478\n",
      "Epoch 11 Batch 6850 Loss 1.8907 Accuracy 0.4478\n",
      "Epoch 11 Batch 6900 Loss 1.8907 Accuracy 0.4478\n",
      "Epoch 11 Batch 6950 Loss 1.8904 Accuracy 0.4478\n",
      "Epoch 11 Batch 7000 Loss 1.8905 Accuracy 0.4478\n",
      "Epoch 11 Batch 7050 Loss 1.8906 Accuracy 0.4478\n",
      "Epoch 11 Batch 7100 Loss 1.8906 Accuracy 0.4478\n",
      "Epoch 11 Batch 7150 Loss 1.8906 Accuracy 0.4478\n",
      "Epoch 11 Batch 7200 Loss 1.8905 Accuracy 0.4478\n",
      "Epoch 11 Batch 7250 Loss 1.8902 Accuracy 0.4478\n",
      "Epoch 11 Batch 7300 Loss 1.8904 Accuracy 0.4478\n",
      "Epoch 11 Batch 7350 Loss 1.8903 Accuracy 0.4478\n",
      "Epoch 11 Batch 7400 Loss 1.8905 Accuracy 0.4478\n",
      "Epoch 11 Batch 7450 Loss 1.8902 Accuracy 0.4478\n",
      "Epoch 11 Batch 7500 Loss 1.8902 Accuracy 0.4478\n",
      "Epoch 11 Batch 7550 Loss 1.8905 Accuracy 0.4478\n",
      "Epoch 11 Batch 7600 Loss 1.8903 Accuracy 0.4478\n",
      "Epoch 11 Batch 7650 Loss 1.8899 Accuracy 0.4479\n",
      "Epoch 11 Batch 7700 Loss 1.8901 Accuracy 0.4478\n",
      "Epoch 11 Batch 7750 Loss 1.8901 Accuracy 0.4478\n",
      "Epoch 11 Batch 7800 Loss 1.8900 Accuracy 0.4479\n",
      "Epoch 11 Batch 7850 Loss 1.8898 Accuracy 0.4479\n",
      "Epoch 11 Batch 7900 Loss 1.8896 Accuracy 0.4479\n",
      "Epoch 11 Batch 7950 Loss 1.8893 Accuracy 0.4480\n",
      "Epoch 11 Batch 8000 Loss 1.8890 Accuracy 0.4480\n",
      "Epoch 11 Batch 8050 Loss 1.8890 Accuracy 0.4480\n",
      "Epoch 11 Batch 8100 Loss 1.8889 Accuracy 0.4480\n",
      "Epoch 11 Batch 8150 Loss 1.8889 Accuracy 0.4480\n",
      "Epoch 11 Batch 8200 Loss 1.8888 Accuracy 0.4480\n",
      "Epoch 11 Batch 8250 Loss 1.8887 Accuracy 0.4481\n",
      "Epoch 11 Batch 8300 Loss 1.8885 Accuracy 0.4481\n",
      "Epoch 11 Batch 8350 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 8400 Loss 1.8889 Accuracy 0.4481\n",
      "Epoch 11 Batch 8450 Loss 1.8890 Accuracy 0.4481\n",
      "Epoch 11 Batch 8500 Loss 1.8889 Accuracy 0.4481\n",
      "Epoch 11 Batch 8550 Loss 1.8888 Accuracy 0.4481\n",
      "Epoch 11 Batch 8600 Loss 1.8888 Accuracy 0.4481\n",
      "Epoch 11 Batch 8650 Loss 1.8888 Accuracy 0.4481\n",
      "Epoch 11 Batch 8700 Loss 1.8887 Accuracy 0.4481\n",
      "Epoch 11 Batch 8750 Loss 1.8886 Accuracy 0.4480\n",
      "Epoch 11 Batch 8800 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 8850 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 8900 Loss 1.8885 Accuracy 0.4481\n",
      "Epoch 11 Batch 8950 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 9000 Loss 1.8886 Accuracy 0.4480\n",
      "Epoch 11 Batch 9050 Loss 1.8884 Accuracy 0.4480\n",
      "Epoch 11 Batch 9100 Loss 1.8885 Accuracy 0.4481\n",
      "Epoch 11 Batch 9150 Loss 1.8886 Accuracy 0.4480\n",
      "Epoch 11 Batch 9200 Loss 1.8887 Accuracy 0.4480\n",
      "Epoch 11 Batch 9250 Loss 1.8887 Accuracy 0.4480\n",
      "Epoch 11 Batch 9300 Loss 1.8887 Accuracy 0.4480\n",
      "Epoch 11 Batch 9350 Loss 1.8889 Accuracy 0.4480\n",
      "Epoch 11 Batch 9400 Loss 1.8887 Accuracy 0.4480\n",
      "Epoch 11 Batch 9450 Loss 1.8885 Accuracy 0.4481\n",
      "Epoch 11 Batch 9500 Loss 1.8886 Accuracy 0.4480\n",
      "Epoch 11 Batch 9550 Loss 1.8889 Accuracy 0.4480\n",
      "Epoch 11 Batch 9600 Loss 1.8889 Accuracy 0.4480\n",
      "Epoch 11 Batch 9650 Loss 1.8888 Accuracy 0.4480\n",
      "Epoch 11 Batch 9700 Loss 1.8886 Accuracy 0.4480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 9750 Loss 1.8887 Accuracy 0.4480\n",
      "Epoch 11 Batch 9800 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 9850 Loss 1.8886 Accuracy 0.4481\n",
      "Epoch 11 Batch 9900 Loss 1.8885 Accuracy 0.4480\n",
      "Epoch 11 Batch 9950 Loss 1.8885 Accuracy 0.4481\n",
      "Epoch 11 Batch 10000 Loss 1.8884 Accuracy 0.4481\n",
      "Epoch 11 Batch 10050 Loss 1.8884 Accuracy 0.4481\n",
      "Epoch 11 Batch 10100 Loss 1.8883 Accuracy 0.4481\n",
      "Epoch 11 Batch 10150 Loss 1.8881 Accuracy 0.4481\n",
      "Epoch 11 Batch 10200 Loss 1.8880 Accuracy 0.4481\n",
      "Epoch 11 Loss 1.8880 Accuracy 0.4481\n",
      "Time taken for 1 epoch: 431.68221402168274 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.8530 Accuracy 0.4342\n",
      "Epoch 12 Batch 50 Loss 1.8693 Accuracy 0.4491\n",
      "Epoch 12 Batch 100 Loss 1.8629 Accuracy 0.4504\n",
      "Epoch 12 Batch 150 Loss 1.8600 Accuracy 0.4523\n",
      "Epoch 12 Batch 200 Loss 1.8641 Accuracy 0.4517\n",
      "Epoch 12 Batch 250 Loss 1.8666 Accuracy 0.4510\n",
      "Epoch 12 Batch 300 Loss 1.8632 Accuracy 0.4508\n",
      "Epoch 12 Batch 350 Loss 1.8689 Accuracy 0.4503\n",
      "Epoch 12 Batch 400 Loss 1.8716 Accuracy 0.4496\n",
      "Epoch 12 Batch 450 Loss 1.8733 Accuracy 0.4493\n",
      "Epoch 12 Batch 500 Loss 1.8713 Accuracy 0.4497\n",
      "Epoch 12 Batch 550 Loss 1.8757 Accuracy 0.4490\n",
      "Epoch 12 Batch 600 Loss 1.8767 Accuracy 0.4487\n",
      "Epoch 12 Batch 650 Loss 1.8766 Accuracy 0.4491\n",
      "Epoch 12 Batch 700 Loss 1.8758 Accuracy 0.4494\n",
      "Epoch 12 Batch 750 Loss 1.8773 Accuracy 0.4492\n",
      "Epoch 12 Batch 800 Loss 1.8783 Accuracy 0.4488\n",
      "Epoch 12 Batch 850 Loss 1.8776 Accuracy 0.4487\n",
      "Epoch 12 Batch 900 Loss 1.8778 Accuracy 0.4487\n",
      "Epoch 12 Batch 950 Loss 1.8789 Accuracy 0.4489\n",
      "Epoch 12 Batch 1000 Loss 1.8798 Accuracy 0.4487\n",
      "Epoch 12 Batch 1050 Loss 1.8791 Accuracy 0.4488\n",
      "Epoch 12 Batch 1100 Loss 1.8792 Accuracy 0.4489\n",
      "Epoch 12 Batch 1150 Loss 1.8783 Accuracy 0.4489\n",
      "Epoch 12 Batch 1200 Loss 1.8798 Accuracy 0.4487\n",
      "Epoch 12 Batch 1250 Loss 1.8801 Accuracy 0.4486\n",
      "Epoch 12 Batch 1300 Loss 1.8793 Accuracy 0.4488\n",
      "Epoch 12 Batch 1350 Loss 1.8789 Accuracy 0.4489\n",
      "Epoch 12 Batch 1400 Loss 1.8786 Accuracy 0.4489\n",
      "Epoch 12 Batch 1450 Loss 1.8767 Accuracy 0.4491\n",
      "Epoch 12 Batch 1500 Loss 1.8763 Accuracy 0.4491\n",
      "Epoch 12 Batch 1550 Loss 1.8757 Accuracy 0.4490\n",
      "Epoch 12 Batch 1600 Loss 1.8758 Accuracy 0.4491\n",
      "Epoch 12 Batch 1650 Loss 1.8749 Accuracy 0.4492\n",
      "Epoch 12 Batch 1700 Loss 1.8749 Accuracy 0.4492\n",
      "Epoch 12 Batch 1750 Loss 1.8750 Accuracy 0.4491\n",
      "Epoch 12 Batch 1800 Loss 1.8744 Accuracy 0.4491\n",
      "Epoch 12 Batch 1850 Loss 1.8743 Accuracy 0.4492\n",
      "Epoch 12 Batch 1900 Loss 1.8743 Accuracy 0.4492\n",
      "Epoch 12 Batch 1950 Loss 1.8752 Accuracy 0.4491\n",
      "Epoch 12 Batch 2000 Loss 1.8756 Accuracy 0.4492\n",
      "Epoch 12 Batch 2050 Loss 1.8755 Accuracy 0.4492\n",
      "Epoch 12 Batch 2100 Loss 1.8756 Accuracy 0.4493\n",
      "Epoch 12 Batch 2150 Loss 1.8753 Accuracy 0.4494\n",
      "Epoch 12 Batch 2200 Loss 1.8757 Accuracy 0.4493\n",
      "Epoch 12 Batch 2250 Loss 1.8753 Accuracy 0.4494\n",
      "Epoch 12 Batch 2300 Loss 1.8755 Accuracy 0.4494\n",
      "Epoch 12 Batch 2350 Loss 1.8757 Accuracy 0.4493\n",
      "Epoch 12 Batch 2400 Loss 1.8763 Accuracy 0.4493\n",
      "Epoch 12 Batch 2450 Loss 1.8761 Accuracy 0.4494\n",
      "Epoch 12 Batch 2500 Loss 1.8764 Accuracy 0.4494\n",
      "Epoch 12 Batch 2550 Loss 1.8761 Accuracy 0.4494\n",
      "Epoch 12 Batch 2600 Loss 1.8759 Accuracy 0.4493\n",
      "Epoch 12 Batch 2650 Loss 1.8760 Accuracy 0.4493\n",
      "Epoch 12 Batch 2700 Loss 1.8757 Accuracy 0.4494\n",
      "Epoch 12 Batch 2750 Loss 1.8762 Accuracy 0.4493\n",
      "Epoch 12 Batch 2800 Loss 1.8761 Accuracy 0.4493\n",
      "Epoch 12 Batch 2850 Loss 1.8756 Accuracy 0.4494\n",
      "Epoch 12 Batch 2900 Loss 1.8763 Accuracy 0.4494\n",
      "Epoch 12 Batch 2950 Loss 1.8763 Accuracy 0.4494\n",
      "Epoch 12 Batch 3000 Loss 1.8763 Accuracy 0.4493\n",
      "Epoch 12 Batch 3050 Loss 1.8770 Accuracy 0.4493\n",
      "Epoch 12 Batch 3100 Loss 1.8767 Accuracy 0.4493\n",
      "Epoch 12 Batch 3150 Loss 1.8764 Accuracy 0.4493\n",
      "Epoch 12 Batch 3200 Loss 1.8764 Accuracy 0.4493\n",
      "Epoch 12 Batch 3250 Loss 1.8766 Accuracy 0.4492\n",
      "Epoch 12 Batch 3300 Loss 1.8766 Accuracy 0.4492\n",
      "Epoch 12 Batch 3350 Loss 1.8766 Accuracy 0.4492\n",
      "Epoch 12 Batch 3400 Loss 1.8762 Accuracy 0.4493\n",
      "Epoch 12 Batch 3450 Loss 1.8758 Accuracy 0.4493\n",
      "Epoch 12 Batch 3500 Loss 1.8759 Accuracy 0.4493\n",
      "Epoch 12 Batch 3550 Loss 1.8758 Accuracy 0.4493\n",
      "Epoch 12 Batch 3600 Loss 1.8758 Accuracy 0.4493\n",
      "Epoch 12 Batch 3650 Loss 1.8758 Accuracy 0.4494\n",
      "Epoch 12 Batch 3700 Loss 1.8758 Accuracy 0.4493\n",
      "Epoch 12 Batch 3750 Loss 1.8757 Accuracy 0.4493\n",
      "Epoch 12 Batch 3800 Loss 1.8756 Accuracy 0.4493\n",
      "Epoch 12 Batch 3850 Loss 1.8758 Accuracy 0.4492\n",
      "Epoch 12 Batch 3900 Loss 1.8762 Accuracy 0.4492\n",
      "Epoch 12 Batch 3950 Loss 1.8761 Accuracy 0.4492\n",
      "Epoch 12 Batch 4000 Loss 1.8766 Accuracy 0.4492\n",
      "Epoch 12 Batch 4050 Loss 1.8767 Accuracy 0.4492\n",
      "Epoch 12 Batch 4100 Loss 1.8766 Accuracy 0.4492\n",
      "Epoch 12 Batch 4150 Loss 1.8768 Accuracy 0.4492\n",
      "Epoch 12 Batch 4200 Loss 1.8767 Accuracy 0.4492\n",
      "Epoch 12 Batch 4250 Loss 1.8764 Accuracy 0.4492\n",
      "Epoch 12 Batch 4300 Loss 1.8765 Accuracy 0.4492\n",
      "Epoch 12 Batch 4350 Loss 1.8770 Accuracy 0.4491\n",
      "Epoch 12 Batch 4400 Loss 1.8772 Accuracy 0.4491\n",
      "Epoch 12 Batch 4450 Loss 1.8772 Accuracy 0.4491\n",
      "Epoch 12 Batch 4500 Loss 1.8767 Accuracy 0.4491\n",
      "Epoch 12 Batch 4550 Loss 1.8770 Accuracy 0.4491\n",
      "Epoch 12 Batch 4600 Loss 1.8768 Accuracy 0.4491\n",
      "Epoch 12 Batch 4650 Loss 1.8764 Accuracy 0.4491\n",
      "Epoch 12 Batch 4700 Loss 1.8764 Accuracy 0.4491\n",
      "Epoch 12 Batch 4750 Loss 1.8765 Accuracy 0.4491\n",
      "Epoch 12 Batch 4800 Loss 1.8765 Accuracy 0.4491\n",
      "Epoch 12 Batch 4850 Loss 1.8762 Accuracy 0.4492\n",
      "Epoch 12 Batch 4900 Loss 1.8765 Accuracy 0.4491\n",
      "Epoch 12 Batch 4950 Loss 1.8763 Accuracy 0.4492\n",
      "Epoch 12 Batch 5000 Loss 1.8761 Accuracy 0.4492\n",
      "Epoch 12 Batch 5050 Loss 1.8758 Accuracy 0.4492\n",
      "Epoch 12 Batch 5100 Loss 1.8760 Accuracy 0.4492\n",
      "Epoch 12 Batch 5150 Loss 1.8760 Accuracy 0.4492\n",
      "Epoch 12 Batch 5200 Loss 1.8756 Accuracy 0.4492\n",
      "Epoch 12 Batch 5250 Loss 1.8753 Accuracy 0.4492\n",
      "Epoch 12 Batch 5300 Loss 1.8752 Accuracy 0.4492\n",
      "Epoch 12 Batch 5350 Loss 1.8755 Accuracy 0.4492\n",
      "Epoch 12 Batch 5400 Loss 1.8755 Accuracy 0.4492\n",
      "Epoch 12 Batch 5450 Loss 1.8754 Accuracy 0.4491\n",
      "Epoch 12 Batch 5500 Loss 1.8756 Accuracy 0.4491\n",
      "Epoch 12 Batch 5550 Loss 1.8752 Accuracy 0.4491\n",
      "Epoch 12 Batch 5600 Loss 1.8752 Accuracy 0.4491\n",
      "Epoch 12 Batch 5650 Loss 1.8750 Accuracy 0.4491\n",
      "Epoch 12 Batch 5700 Loss 1.8750 Accuracy 0.4491\n",
      "Epoch 12 Batch 5750 Loss 1.8750 Accuracy 0.4492\n",
      "Epoch 12 Batch 5800 Loss 1.8750 Accuracy 0.4492\n",
      "Epoch 12 Batch 5850 Loss 1.8750 Accuracy 0.4492\n",
      "Epoch 12 Batch 5900 Loss 1.8749 Accuracy 0.4492\n",
      "Epoch 12 Batch 5950 Loss 1.8749 Accuracy 0.4492\n",
      "Epoch 12 Batch 6000 Loss 1.8751 Accuracy 0.4492\n",
      "Epoch 12 Batch 6050 Loss 1.8749 Accuracy 0.4492\n",
      "Epoch 12 Batch 6100 Loss 1.8746 Accuracy 0.4492\n",
      "Epoch 12 Batch 6150 Loss 1.8747 Accuracy 0.4492\n",
      "Epoch 12 Batch 6200 Loss 1.8747 Accuracy 0.4492\n",
      "Epoch 12 Batch 6250 Loss 1.8744 Accuracy 0.4492\n",
      "Epoch 12 Batch 6300 Loss 1.8746 Accuracy 0.4492\n",
      "Epoch 12 Batch 6350 Loss 1.8744 Accuracy 0.4492\n",
      "Epoch 12 Batch 6400 Loss 1.8742 Accuracy 0.4492\n",
      "Epoch 12 Batch 6450 Loss 1.8740 Accuracy 0.4493\n",
      "Epoch 12 Batch 6500 Loss 1.8738 Accuracy 0.4493\n",
      "Epoch 12 Batch 6550 Loss 1.8737 Accuracy 0.4493\n",
      "Epoch 12 Batch 6600 Loss 1.8738 Accuracy 0.4493\n",
      "Epoch 12 Batch 6650 Loss 1.8739 Accuracy 0.4493\n",
      "Epoch 12 Batch 6700 Loss 1.8740 Accuracy 0.4493\n",
      "Epoch 12 Batch 6750 Loss 1.8739 Accuracy 0.4493\n",
      "Epoch 12 Batch 6800 Loss 1.8741 Accuracy 0.4492\n",
      "Epoch 12 Batch 6850 Loss 1.8740 Accuracy 0.4493\n",
      "Epoch 12 Batch 6900 Loss 1.8739 Accuracy 0.4492\n",
      "Epoch 12 Batch 6950 Loss 1.8739 Accuracy 0.4492\n",
      "Epoch 12 Batch 7000 Loss 1.8740 Accuracy 0.4492\n",
      "Epoch 12 Batch 7050 Loss 1.8738 Accuracy 0.4493\n",
      "Epoch 12 Batch 7100 Loss 1.8739 Accuracy 0.4492\n",
      "Epoch 12 Batch 7150 Loss 1.8737 Accuracy 0.4493\n",
      "Epoch 12 Batch 7200 Loss 1.8735 Accuracy 0.4493\n",
      "Epoch 12 Batch 7250 Loss 1.8734 Accuracy 0.4493\n",
      "Epoch 12 Batch 7300 Loss 1.8734 Accuracy 0.4493\n",
      "Epoch 12 Batch 7350 Loss 1.8733 Accuracy 0.4493\n",
      "Epoch 12 Batch 7400 Loss 1.8733 Accuracy 0.4494\n",
      "Epoch 12 Batch 7450 Loss 1.8734 Accuracy 0.4493\n",
      "Epoch 12 Batch 7500 Loss 1.8730 Accuracy 0.4494\n",
      "Epoch 12 Batch 7550 Loss 1.8732 Accuracy 0.4494\n",
      "Epoch 12 Batch 7600 Loss 1.8731 Accuracy 0.4494\n",
      "Epoch 12 Batch 7650 Loss 1.8731 Accuracy 0.4494\n",
      "Epoch 12 Batch 7700 Loss 1.8732 Accuracy 0.4494\n",
      "Epoch 12 Batch 7750 Loss 1.8732 Accuracy 0.4494\n",
      "Epoch 12 Batch 7800 Loss 1.8732 Accuracy 0.4494\n",
      "Epoch 12 Batch 7850 Loss 1.8732 Accuracy 0.4494\n",
      "Epoch 12 Batch 7900 Loss 1.8730 Accuracy 0.4494\n",
      "Epoch 12 Batch 7950 Loss 1.8730 Accuracy 0.4494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 8000 Loss 1.8728 Accuracy 0.4494\n",
      "Epoch 12 Batch 8050 Loss 1.8729 Accuracy 0.4494\n",
      "Epoch 12 Batch 8100 Loss 1.8728 Accuracy 0.4494\n",
      "Epoch 12 Batch 8150 Loss 1.8728 Accuracy 0.4494\n",
      "Epoch 12 Batch 8200 Loss 1.8726 Accuracy 0.4494\n",
      "Epoch 12 Batch 8250 Loss 1.8722 Accuracy 0.4494\n",
      "Epoch 12 Batch 8300 Loss 1.8722 Accuracy 0.4494\n",
      "Epoch 12 Batch 8350 Loss 1.8723 Accuracy 0.4494\n",
      "Epoch 12 Batch 8400 Loss 1.8724 Accuracy 0.4495\n",
      "Epoch 12 Batch 8450 Loss 1.8724 Accuracy 0.4495\n",
      "Epoch 12 Batch 8500 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 8550 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 8600 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 8650 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 8700 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 8750 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 8800 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 8850 Loss 1.8719 Accuracy 0.4495\n",
      "Epoch 12 Batch 8900 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 8950 Loss 1.8719 Accuracy 0.4496\n",
      "Epoch 12 Batch 9000 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 9050 Loss 1.8724 Accuracy 0.4495\n",
      "Epoch 12 Batch 9100 Loss 1.8725 Accuracy 0.4495\n",
      "Epoch 12 Batch 9150 Loss 1.8725 Accuracy 0.4495\n",
      "Epoch 12 Batch 9200 Loss 1.8724 Accuracy 0.4495\n",
      "Epoch 12 Batch 9250 Loss 1.8726 Accuracy 0.4495\n",
      "Epoch 12 Batch 9300 Loss 1.8725 Accuracy 0.4495\n",
      "Epoch 12 Batch 9350 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 9400 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 9450 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 9500 Loss 1.8722 Accuracy 0.4495\n",
      "Epoch 12 Batch 9550 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 9600 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 9650 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 9700 Loss 1.8720 Accuracy 0.4496\n",
      "Epoch 12 Batch 9750 Loss 1.8719 Accuracy 0.4496\n",
      "Epoch 12 Batch 9800 Loss 1.8720 Accuracy 0.4496\n",
      "Epoch 12 Batch 9850 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 9900 Loss 1.8722 Accuracy 0.4495\n",
      "Epoch 12 Batch 9950 Loss 1.8723 Accuracy 0.4495\n",
      "Epoch 12 Batch 10000 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 10050 Loss 1.8721 Accuracy 0.4495\n",
      "Epoch 12 Batch 10100 Loss 1.8720 Accuracy 0.4495\n",
      "Epoch 12 Batch 10150 Loss 1.8718 Accuracy 0.4496\n",
      "Epoch 12 Batch 10200 Loss 1.8718 Accuracy 0.4496\n",
      "Epoch 12 Loss 1.8718 Accuracy 0.4496\n",
      "Time taken for 1 epoch: 431.758811712265 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.9935 Accuracy 0.4556\n",
      "Epoch 13 Batch 50 Loss 1.8847 Accuracy 0.4486\n",
      "Epoch 13 Batch 100 Loss 1.8682 Accuracy 0.4508\n",
      "Epoch 13 Batch 150 Loss 1.8619 Accuracy 0.4501\n",
      "Epoch 13 Batch 200 Loss 1.8603 Accuracy 0.4510\n",
      "Epoch 13 Batch 250 Loss 1.8668 Accuracy 0.4507\n",
      "Epoch 13 Batch 300 Loss 1.8648 Accuracy 0.4506\n",
      "Epoch 13 Batch 350 Loss 1.8595 Accuracy 0.4510\n",
      "Epoch 13 Batch 400 Loss 1.8622 Accuracy 0.4508\n",
      "Epoch 13 Batch 450 Loss 1.8604 Accuracy 0.4509\n",
      "Epoch 13 Batch 500 Loss 1.8636 Accuracy 0.4508\n",
      "Epoch 13 Batch 550 Loss 1.8632 Accuracy 0.4509\n",
      "Epoch 13 Batch 600 Loss 1.8596 Accuracy 0.4511\n",
      "Epoch 13 Batch 650 Loss 1.8610 Accuracy 0.4509\n",
      "Epoch 13 Batch 700 Loss 1.8617 Accuracy 0.4511\n",
      "Epoch 13 Batch 750 Loss 1.8610 Accuracy 0.4509\n",
      "Epoch 13 Batch 800 Loss 1.8622 Accuracy 0.4506\n",
      "Epoch 13 Batch 850 Loss 1.8636 Accuracy 0.4506\n",
      "Epoch 13 Batch 900 Loss 1.8635 Accuracy 0.4504\n",
      "Epoch 13 Batch 950 Loss 1.8642 Accuracy 0.4503\n",
      "Epoch 13 Batch 1000 Loss 1.8629 Accuracy 0.4504\n",
      "Epoch 13 Batch 1050 Loss 1.8636 Accuracy 0.4503\n",
      "Epoch 13 Batch 1100 Loss 1.8639 Accuracy 0.4504\n",
      "Epoch 13 Batch 1150 Loss 1.8639 Accuracy 0.4504\n",
      "Epoch 13 Batch 1200 Loss 1.8644 Accuracy 0.4504\n",
      "Epoch 13 Batch 1250 Loss 1.8637 Accuracy 0.4505\n",
      "Epoch 13 Batch 1300 Loss 1.8641 Accuracy 0.4505\n",
      "Epoch 13 Batch 1350 Loss 1.8641 Accuracy 0.4503\n",
      "Epoch 13 Batch 1400 Loss 1.8636 Accuracy 0.4506\n",
      "Epoch 13 Batch 1450 Loss 1.8631 Accuracy 0.4506\n",
      "Epoch 13 Batch 1500 Loss 1.8624 Accuracy 0.4508\n",
      "Epoch 13 Batch 1550 Loss 1.8615 Accuracy 0.4508\n",
      "Epoch 13 Batch 1600 Loss 1.8620 Accuracy 0.4508\n",
      "Epoch 13 Batch 1650 Loss 1.8626 Accuracy 0.4508\n",
      "Epoch 13 Batch 1700 Loss 1.8624 Accuracy 0.4507\n",
      "Epoch 13 Batch 1750 Loss 1.8625 Accuracy 0.4506\n",
      "Epoch 13 Batch 1800 Loss 1.8627 Accuracy 0.4506\n",
      "Epoch 13 Batch 1850 Loss 1.8621 Accuracy 0.4508\n",
      "Epoch 13 Batch 1900 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 1950 Loss 1.8616 Accuracy 0.4507\n",
      "Epoch 13 Batch 2000 Loss 1.8621 Accuracy 0.4507\n",
      "Epoch 13 Batch 2050 Loss 1.8615 Accuracy 0.4506\n",
      "Epoch 13 Batch 2100 Loss 1.8614 Accuracy 0.4505\n",
      "Epoch 13 Batch 2150 Loss 1.8612 Accuracy 0.4506\n",
      "Epoch 13 Batch 2200 Loss 1.8608 Accuracy 0.4508\n",
      "Epoch 13 Batch 2250 Loss 1.8607 Accuracy 0.4509\n",
      "Epoch 13 Batch 2300 Loss 1.8607 Accuracy 0.4509\n",
      "Epoch 13 Batch 2350 Loss 1.8608 Accuracy 0.4509\n",
      "Epoch 13 Batch 2400 Loss 1.8617 Accuracy 0.4509\n",
      "Epoch 13 Batch 2450 Loss 1.8613 Accuracy 0.4509\n",
      "Epoch 13 Batch 2500 Loss 1.8616 Accuracy 0.4509\n",
      "Epoch 13 Batch 2550 Loss 1.8614 Accuracy 0.4510\n",
      "Epoch 13 Batch 2600 Loss 1.8619 Accuracy 0.4510\n",
      "Epoch 13 Batch 2650 Loss 1.8617 Accuracy 0.4510\n",
      "Epoch 13 Batch 2700 Loss 1.8613 Accuracy 0.4510\n",
      "Epoch 13 Batch 2750 Loss 1.8614 Accuracy 0.4510\n",
      "Epoch 13 Batch 2800 Loss 1.8610 Accuracy 0.4509\n",
      "Epoch 13 Batch 2850 Loss 1.8606 Accuracy 0.4510\n",
      "Epoch 13 Batch 2900 Loss 1.8608 Accuracy 0.4510\n",
      "Epoch 13 Batch 2950 Loss 1.8609 Accuracy 0.4511\n",
      "Epoch 13 Batch 3000 Loss 1.8616 Accuracy 0.4510\n",
      "Epoch 13 Batch 3050 Loss 1.8616 Accuracy 0.4511\n",
      "Epoch 13 Batch 3100 Loss 1.8615 Accuracy 0.4511\n",
      "Epoch 13 Batch 3150 Loss 1.8617 Accuracy 0.4511\n",
      "Epoch 13 Batch 3200 Loss 1.8621 Accuracy 0.4511\n",
      "Epoch 13 Batch 3250 Loss 1.8622 Accuracy 0.4510\n",
      "Epoch 13 Batch 3300 Loss 1.8621 Accuracy 0.4510\n",
      "Epoch 13 Batch 3350 Loss 1.8620 Accuracy 0.4509\n",
      "Epoch 13 Batch 3400 Loss 1.8617 Accuracy 0.4510\n",
      "Epoch 13 Batch 3450 Loss 1.8613 Accuracy 0.4510\n",
      "Epoch 13 Batch 3500 Loss 1.8608 Accuracy 0.4510\n",
      "Epoch 13 Batch 3550 Loss 1.8610 Accuracy 0.4510\n",
      "Epoch 13 Batch 3600 Loss 1.8611 Accuracy 0.4511\n",
      "Epoch 13 Batch 3650 Loss 1.8610 Accuracy 0.4510\n",
      "Epoch 13 Batch 3700 Loss 1.8611 Accuracy 0.4510\n",
      "Epoch 13 Batch 3750 Loss 1.8610 Accuracy 0.4510\n",
      "Epoch 13 Batch 3800 Loss 1.8612 Accuracy 0.4509\n",
      "Epoch 13 Batch 3850 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 3900 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 3950 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 4000 Loss 1.8612 Accuracy 0.4509\n",
      "Epoch 13 Batch 4050 Loss 1.8610 Accuracy 0.4509\n",
      "Epoch 13 Batch 4100 Loss 1.8615 Accuracy 0.4508\n",
      "Epoch 13 Batch 4150 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 4200 Loss 1.8616 Accuracy 0.4508\n",
      "Epoch 13 Batch 4250 Loss 1.8617 Accuracy 0.4508\n",
      "Epoch 13 Batch 4300 Loss 1.8619 Accuracy 0.4508\n",
      "Epoch 13 Batch 4350 Loss 1.8621 Accuracy 0.4508\n",
      "Epoch 13 Batch 4400 Loss 1.8621 Accuracy 0.4508\n",
      "Epoch 13 Batch 4450 Loss 1.8621 Accuracy 0.4508\n",
      "Epoch 13 Batch 4500 Loss 1.8620 Accuracy 0.4508\n",
      "Epoch 13 Batch 4550 Loss 1.8617 Accuracy 0.4509\n",
      "Epoch 13 Batch 4600 Loss 1.8618 Accuracy 0.4508\n",
      "Epoch 13 Batch 4650 Loss 1.8618 Accuracy 0.4508\n",
      "Epoch 13 Batch 4700 Loss 1.8615 Accuracy 0.4508\n",
      "Epoch 13 Batch 4750 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 4800 Loss 1.8616 Accuracy 0.4508\n",
      "Epoch 13 Batch 4850 Loss 1.8618 Accuracy 0.4508\n",
      "Epoch 13 Batch 4900 Loss 1.8620 Accuracy 0.4508\n",
      "Epoch 13 Batch 4950 Loss 1.8619 Accuracy 0.4508\n",
      "Epoch 13 Batch 5000 Loss 1.8621 Accuracy 0.4508\n",
      "Epoch 13 Batch 5050 Loss 1.8619 Accuracy 0.4508\n",
      "Epoch 13 Batch 5100 Loss 1.8615 Accuracy 0.4508\n",
      "Epoch 13 Batch 5150 Loss 1.8614 Accuracy 0.4508\n",
      "Epoch 13 Batch 5200 Loss 1.8616 Accuracy 0.4508\n",
      "Epoch 13 Batch 5250 Loss 1.8615 Accuracy 0.4507\n",
      "Epoch 13 Batch 5300 Loss 1.8613 Accuracy 0.4507\n",
      "Epoch 13 Batch 5350 Loss 1.8614 Accuracy 0.4507\n",
      "Epoch 13 Batch 5400 Loss 1.8613 Accuracy 0.4507\n",
      "Epoch 13 Batch 5450 Loss 1.8612 Accuracy 0.4507\n",
      "Epoch 13 Batch 5500 Loss 1.8613 Accuracy 0.4507\n",
      "Epoch 13 Batch 5550 Loss 1.8609 Accuracy 0.4507\n",
      "Epoch 13 Batch 5600 Loss 1.8613 Accuracy 0.4507\n",
      "Epoch 13 Batch 5650 Loss 1.8611 Accuracy 0.4508\n",
      "Epoch 13 Batch 5700 Loss 1.8609 Accuracy 0.4508\n",
      "Epoch 13 Batch 5750 Loss 1.8606 Accuracy 0.4508\n",
      "Epoch 13 Batch 5800 Loss 1.8605 Accuracy 0.4508\n",
      "Epoch 13 Batch 5850 Loss 1.8604 Accuracy 0.4508\n",
      "Epoch 13 Batch 5900 Loss 1.8604 Accuracy 0.4509\n",
      "Epoch 13 Batch 5950 Loss 1.8605 Accuracy 0.4508\n",
      "Epoch 13 Batch 6000 Loss 1.8605 Accuracy 0.4508\n",
      "Epoch 13 Batch 6050 Loss 1.8606 Accuracy 0.4508\n",
      "Epoch 13 Batch 6100 Loss 1.8604 Accuracy 0.4508\n",
      "Epoch 13 Batch 6150 Loss 1.8604 Accuracy 0.4508\n",
      "Epoch 13 Batch 6200 Loss 1.8606 Accuracy 0.4508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 6250 Loss 1.8606 Accuracy 0.4508\n",
      "Epoch 13 Batch 6300 Loss 1.8606 Accuracy 0.4508\n",
      "Epoch 13 Batch 6350 Loss 1.8607 Accuracy 0.4508\n",
      "Epoch 13 Batch 6400 Loss 1.8604 Accuracy 0.4508\n",
      "Epoch 13 Batch 6450 Loss 1.8603 Accuracy 0.4509\n",
      "Epoch 13 Batch 6500 Loss 1.8601 Accuracy 0.4509\n",
      "Epoch 13 Batch 6550 Loss 1.8601 Accuracy 0.4510\n",
      "Epoch 13 Batch 6600 Loss 1.8599 Accuracy 0.4510\n",
      "Epoch 13 Batch 6650 Loss 1.8596 Accuracy 0.4510\n",
      "Epoch 13 Batch 6700 Loss 1.8594 Accuracy 0.4510\n",
      "Epoch 13 Batch 6750 Loss 1.8593 Accuracy 0.4510\n",
      "Epoch 13 Batch 6800 Loss 1.8593 Accuracy 0.4510\n",
      "Epoch 13 Batch 6850 Loss 1.8592 Accuracy 0.4510\n",
      "Epoch 13 Batch 6900 Loss 1.8593 Accuracy 0.4510\n",
      "Epoch 13 Batch 6950 Loss 1.8595 Accuracy 0.4510\n",
      "Epoch 13 Batch 7000 Loss 1.8593 Accuracy 0.4510\n",
      "Epoch 13 Batch 7050 Loss 1.8595 Accuracy 0.4509\n",
      "Epoch 13 Batch 7100 Loss 1.8594 Accuracy 0.4509\n",
      "Epoch 13 Batch 7150 Loss 1.8595 Accuracy 0.4509\n",
      "Epoch 13 Batch 7200 Loss 1.8596 Accuracy 0.4509\n",
      "Epoch 13 Batch 7250 Loss 1.8596 Accuracy 0.4509\n",
      "Epoch 13 Batch 7300 Loss 1.8597 Accuracy 0.4509\n",
      "Epoch 13 Batch 7350 Loss 1.8594 Accuracy 0.4509\n",
      "Epoch 13 Batch 7400 Loss 1.8597 Accuracy 0.4509\n",
      "Epoch 13 Batch 7450 Loss 1.8597 Accuracy 0.4509\n",
      "Epoch 13 Batch 7500 Loss 1.8596 Accuracy 0.4509\n",
      "Epoch 13 Batch 7550 Loss 1.8594 Accuracy 0.4509\n",
      "Epoch 13 Batch 7600 Loss 1.8594 Accuracy 0.4509\n",
      "Epoch 13 Batch 7650 Loss 1.8592 Accuracy 0.4509\n",
      "Epoch 13 Batch 7700 Loss 1.8589 Accuracy 0.4510\n",
      "Epoch 13 Batch 7750 Loss 1.8586 Accuracy 0.4510\n",
      "Epoch 13 Batch 7800 Loss 1.8585 Accuracy 0.4510\n",
      "Epoch 13 Batch 7850 Loss 1.8585 Accuracy 0.4510\n",
      "Epoch 13 Batch 7900 Loss 1.8587 Accuracy 0.4510\n",
      "Epoch 13 Batch 7950 Loss 1.8586 Accuracy 0.4510\n",
      "Epoch 13 Batch 8000 Loss 1.8586 Accuracy 0.4510\n",
      "Epoch 13 Batch 8050 Loss 1.8585 Accuracy 0.4510\n",
      "Epoch 13 Batch 8100 Loss 1.8584 Accuracy 0.4510\n",
      "Epoch 13 Batch 8150 Loss 1.8582 Accuracy 0.4510\n",
      "Epoch 13 Batch 8200 Loss 1.8582 Accuracy 0.4510\n",
      "Epoch 13 Batch 8250 Loss 1.8582 Accuracy 0.4510\n",
      "Epoch 13 Batch 8300 Loss 1.8584 Accuracy 0.4510\n",
      "Epoch 13 Batch 8350 Loss 1.8585 Accuracy 0.4510\n",
      "Epoch 13 Batch 8400 Loss 1.8584 Accuracy 0.4510\n",
      "Epoch 13 Batch 8450 Loss 1.8582 Accuracy 0.4510\n",
      "Epoch 13 Batch 8500 Loss 1.8582 Accuracy 0.4510\n",
      "Epoch 13 Batch 8550 Loss 1.8581 Accuracy 0.4510\n",
      "Epoch 13 Batch 8600 Loss 1.8583 Accuracy 0.4510\n",
      "Epoch 13 Batch 8650 Loss 1.8583 Accuracy 0.4510\n",
      "Epoch 13 Batch 8700 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 8750 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 8800 Loss 1.8582 Accuracy 0.4511\n",
      "Epoch 13 Batch 8850 Loss 1.8583 Accuracy 0.4510\n",
      "Epoch 13 Batch 8900 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 8950 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 9000 Loss 1.8582 Accuracy 0.4511\n",
      "Epoch 13 Batch 9050 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 9100 Loss 1.8582 Accuracy 0.4511\n",
      "Epoch 13 Batch 9150 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 9200 Loss 1.8581 Accuracy 0.4511\n",
      "Epoch 13 Batch 9250 Loss 1.8580 Accuracy 0.4511\n",
      "Epoch 13 Batch 9300 Loss 1.8580 Accuracy 0.4511\n",
      "Epoch 13 Batch 9350 Loss 1.8580 Accuracy 0.4511\n",
      "Epoch 13 Batch 9400 Loss 1.8580 Accuracy 0.4511\n",
      "Epoch 13 Batch 9450 Loss 1.8580 Accuracy 0.4511\n",
      "Epoch 13 Batch 9500 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 9550 Loss 1.8576 Accuracy 0.4511\n",
      "Epoch 13 Batch 9600 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 9650 Loss 1.8576 Accuracy 0.4511\n",
      "Epoch 13 Batch 9700 Loss 1.8578 Accuracy 0.4511\n",
      "Epoch 13 Batch 9750 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 9800 Loss 1.8578 Accuracy 0.4511\n",
      "Epoch 13 Batch 9850 Loss 1.8578 Accuracy 0.4511\n",
      "Epoch 13 Batch 9900 Loss 1.8579 Accuracy 0.4511\n",
      "Epoch 13 Batch 9950 Loss 1.8576 Accuracy 0.4511\n",
      "Epoch 13 Batch 10000 Loss 1.8576 Accuracy 0.4511\n",
      "Epoch 13 Batch 10050 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 10100 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 10150 Loss 1.8577 Accuracy 0.4511\n",
      "Epoch 13 Batch 10200 Loss 1.8578 Accuracy 0.4511\n",
      "Epoch 13 Loss 1.8578 Accuracy 0.4511\n",
      "Time taken for 1 epoch: 432.32368564605713 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.7285 Accuracy 0.4474\n",
      "Epoch 14 Batch 50 Loss 1.8613 Accuracy 0.4517\n",
      "Epoch 14 Batch 100 Loss 1.8484 Accuracy 0.4511\n",
      "Epoch 14 Batch 150 Loss 1.8470 Accuracy 0.4506\n",
      "Epoch 14 Batch 200 Loss 1.8477 Accuracy 0.4513\n",
      "Epoch 14 Batch 250 Loss 1.8486 Accuracy 0.4518\n",
      "Epoch 14 Batch 300 Loss 1.8538 Accuracy 0.4515\n",
      "Epoch 14 Batch 350 Loss 1.8511 Accuracy 0.4517\n",
      "Epoch 14 Batch 400 Loss 1.8515 Accuracy 0.4517\n",
      "Epoch 14 Batch 450 Loss 1.8550 Accuracy 0.4510\n",
      "Epoch 14 Batch 500 Loss 1.8532 Accuracy 0.4514\n",
      "Epoch 14 Batch 550 Loss 1.8518 Accuracy 0.4515\n",
      "Epoch 14 Batch 600 Loss 1.8518 Accuracy 0.4517\n",
      "Epoch 14 Batch 650 Loss 1.8519 Accuracy 0.4517\n",
      "Epoch 14 Batch 700 Loss 1.8532 Accuracy 0.4516\n",
      "Epoch 14 Batch 750 Loss 1.8519 Accuracy 0.4518\n",
      "Epoch 14 Batch 800 Loss 1.8517 Accuracy 0.4520\n",
      "Epoch 14 Batch 850 Loss 1.8510 Accuracy 0.4521\n",
      "Epoch 14 Batch 900 Loss 1.8512 Accuracy 0.4522\n",
      "Epoch 14 Batch 950 Loss 1.8505 Accuracy 0.4523\n",
      "Epoch 14 Batch 1000 Loss 1.8518 Accuracy 0.4523\n",
      "Epoch 14 Batch 1050 Loss 1.8520 Accuracy 0.4523\n",
      "Epoch 14 Batch 1100 Loss 1.8523 Accuracy 0.4522\n",
      "Epoch 14 Batch 1150 Loss 1.8540 Accuracy 0.4520\n",
      "Epoch 14 Batch 1200 Loss 1.8513 Accuracy 0.4522\n",
      "Epoch 14 Batch 1250 Loss 1.8534 Accuracy 0.4520\n",
      "Epoch 14 Batch 1300 Loss 1.8522 Accuracy 0.4520\n",
      "Epoch 14 Batch 1350 Loss 1.8535 Accuracy 0.4519\n",
      "Epoch 14 Batch 1400 Loss 1.8525 Accuracy 0.4518\n",
      "Epoch 14 Batch 1450 Loss 1.8508 Accuracy 0.4520\n",
      "Epoch 14 Batch 1500 Loss 1.8501 Accuracy 0.4520\n",
      "Epoch 14 Batch 1550 Loss 1.8491 Accuracy 0.4522\n",
      "Epoch 14 Batch 1600 Loss 1.8492 Accuracy 0.4521\n",
      "Epoch 14 Batch 1650 Loss 1.8488 Accuracy 0.4521\n",
      "Epoch 14 Batch 1700 Loss 1.8481 Accuracy 0.4522\n",
      "Epoch 14 Batch 1750 Loss 1.8484 Accuracy 0.4522\n",
      "Epoch 14 Batch 1800 Loss 1.8486 Accuracy 0.4522\n",
      "Epoch 14 Batch 1850 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 1900 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 1950 Loss 1.8470 Accuracy 0.4521\n",
      "Epoch 14 Batch 2000 Loss 1.8480 Accuracy 0.4522\n",
      "Epoch 14 Batch 2050 Loss 1.8475 Accuracy 0.4523\n",
      "Epoch 14 Batch 2100 Loss 1.8476 Accuracy 0.4523\n",
      "Epoch 14 Batch 2150 Loss 1.8474 Accuracy 0.4523\n",
      "Epoch 14 Batch 2200 Loss 1.8474 Accuracy 0.4522\n",
      "Epoch 14 Batch 2250 Loss 1.8475 Accuracy 0.4521\n",
      "Epoch 14 Batch 2300 Loss 1.8473 Accuracy 0.4523\n",
      "Epoch 14 Batch 2350 Loss 1.8476 Accuracy 0.4523\n",
      "Epoch 14 Batch 2400 Loss 1.8472 Accuracy 0.4523\n",
      "Epoch 14 Batch 2450 Loss 1.8477 Accuracy 0.4522\n",
      "Epoch 14 Batch 2500 Loss 1.8481 Accuracy 0.4522\n",
      "Epoch 14 Batch 2550 Loss 1.8489 Accuracy 0.4521\n",
      "Epoch 14 Batch 2600 Loss 1.8491 Accuracy 0.4522\n",
      "Epoch 14 Batch 2650 Loss 1.8490 Accuracy 0.4522\n",
      "Epoch 14 Batch 2700 Loss 1.8493 Accuracy 0.4522\n",
      "Epoch 14 Batch 2750 Loss 1.8493 Accuracy 0.4522\n",
      "Epoch 14 Batch 2800 Loss 1.8497 Accuracy 0.4522\n",
      "Epoch 14 Batch 2850 Loss 1.8495 Accuracy 0.4522\n",
      "Epoch 14 Batch 2900 Loss 1.8495 Accuracy 0.4522\n",
      "Epoch 14 Batch 2950 Loss 1.8493 Accuracy 0.4521\n",
      "Epoch 14 Batch 3000 Loss 1.8485 Accuracy 0.4522\n",
      "Epoch 14 Batch 3050 Loss 1.8490 Accuracy 0.4522\n",
      "Epoch 14 Batch 3100 Loss 1.8496 Accuracy 0.4521\n",
      "Epoch 14 Batch 3150 Loss 1.8498 Accuracy 0.4522\n",
      "Epoch 14 Batch 3200 Loss 1.8501 Accuracy 0.4522\n",
      "Epoch 14 Batch 3250 Loss 1.8497 Accuracy 0.4522\n",
      "Epoch 14 Batch 3300 Loss 1.8501 Accuracy 0.4522\n",
      "Epoch 14 Batch 3350 Loss 1.8498 Accuracy 0.4522\n",
      "Epoch 14 Batch 3400 Loss 1.8492 Accuracy 0.4523\n",
      "Epoch 14 Batch 3450 Loss 1.8490 Accuracy 0.4523\n",
      "Epoch 14 Batch 3500 Loss 1.8492 Accuracy 0.4523\n",
      "Epoch 14 Batch 3550 Loss 1.8494 Accuracy 0.4522\n",
      "Epoch 14 Batch 3600 Loss 1.8492 Accuracy 0.4522\n",
      "Epoch 14 Batch 3650 Loss 1.8490 Accuracy 0.4523\n",
      "Epoch 14 Batch 3700 Loss 1.8491 Accuracy 0.4522\n",
      "Epoch 14 Batch 3750 Loss 1.8488 Accuracy 0.4523\n",
      "Epoch 14 Batch 3800 Loss 1.8487 Accuracy 0.4522\n",
      "Epoch 14 Batch 3850 Loss 1.8487 Accuracy 0.4523\n",
      "Epoch 14 Batch 3900 Loss 1.8490 Accuracy 0.4522\n",
      "Epoch 14 Batch 3950 Loss 1.8493 Accuracy 0.4522\n",
      "Epoch 14 Batch 4000 Loss 1.8491 Accuracy 0.4522\n",
      "Epoch 14 Batch 4050 Loss 1.8492 Accuracy 0.4523\n",
      "Epoch 14 Batch 4100 Loss 1.8488 Accuracy 0.4523\n",
      "Epoch 14 Batch 4150 Loss 1.8491 Accuracy 0.4523\n",
      "Epoch 14 Batch 4200 Loss 1.8493 Accuracy 0.4522\n",
      "Epoch 14 Batch 4250 Loss 1.8494 Accuracy 0.4522\n",
      "Epoch 14 Batch 4300 Loss 1.8492 Accuracy 0.4522\n",
      "Epoch 14 Batch 4350 Loss 1.8494 Accuracy 0.4522\n",
      "Epoch 14 Batch 4400 Loss 1.8494 Accuracy 0.4521\n",
      "Epoch 14 Batch 4450 Loss 1.8498 Accuracy 0.4521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 4500 Loss 1.8496 Accuracy 0.4521\n",
      "Epoch 14 Batch 4550 Loss 1.8495 Accuracy 0.4522\n",
      "Epoch 14 Batch 4600 Loss 1.8493 Accuracy 0.4522\n",
      "Epoch 14 Batch 4650 Loss 1.8495 Accuracy 0.4522\n",
      "Epoch 14 Batch 4700 Loss 1.8491 Accuracy 0.4522\n",
      "Epoch 14 Batch 4750 Loss 1.8487 Accuracy 0.4522\n",
      "Epoch 14 Batch 4800 Loss 1.8486 Accuracy 0.4522\n",
      "Epoch 14 Batch 4850 Loss 1.8485 Accuracy 0.4522\n",
      "Epoch 14 Batch 4900 Loss 1.8485 Accuracy 0.4522\n",
      "Epoch 14 Batch 4950 Loss 1.8485 Accuracy 0.4522\n",
      "Epoch 14 Batch 5000 Loss 1.8482 Accuracy 0.4522\n",
      "Epoch 14 Batch 5050 Loss 1.8482 Accuracy 0.4522\n",
      "Epoch 14 Batch 5100 Loss 1.8481 Accuracy 0.4522\n",
      "Epoch 14 Batch 5150 Loss 1.8480 Accuracy 0.4522\n",
      "Epoch 14 Batch 5200 Loss 1.8479 Accuracy 0.4522\n",
      "Epoch 14 Batch 5250 Loss 1.8476 Accuracy 0.4522\n",
      "Epoch 14 Batch 5300 Loss 1.8476 Accuracy 0.4522\n",
      "Epoch 14 Batch 5350 Loss 1.8476 Accuracy 0.4522\n",
      "Epoch 14 Batch 5400 Loss 1.8478 Accuracy 0.4521\n",
      "Epoch 14 Batch 5450 Loss 1.8481 Accuracy 0.4521\n",
      "Epoch 14 Batch 5500 Loss 1.8481 Accuracy 0.4521\n",
      "Epoch 14 Batch 5550 Loss 1.8480 Accuracy 0.4521\n",
      "Epoch 14 Batch 5600 Loss 1.8482 Accuracy 0.4521\n",
      "Epoch 14 Batch 5650 Loss 1.8480 Accuracy 0.4521\n",
      "Epoch 14 Batch 5700 Loss 1.8480 Accuracy 0.4521\n",
      "Epoch 14 Batch 5750 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 5800 Loss 1.8474 Accuracy 0.4522\n",
      "Epoch 14 Batch 5850 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 5900 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 5950 Loss 1.8481 Accuracy 0.4522\n",
      "Epoch 14 Batch 6000 Loss 1.8480 Accuracy 0.4522\n",
      "Epoch 14 Batch 6050 Loss 1.8481 Accuracy 0.4522\n",
      "Epoch 14 Batch 6100 Loss 1.8477 Accuracy 0.4522\n",
      "Epoch 14 Batch 6150 Loss 1.8479 Accuracy 0.4522\n",
      "Epoch 14 Batch 6200 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 6250 Loss 1.8475 Accuracy 0.4523\n",
      "Epoch 14 Batch 6300 Loss 1.8477 Accuracy 0.4523\n",
      "Epoch 14 Batch 6350 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 6400 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 6450 Loss 1.8479 Accuracy 0.4522\n",
      "Epoch 14 Batch 6500 Loss 1.8479 Accuracy 0.4522\n",
      "Epoch 14 Batch 6550 Loss 1.8478 Accuracy 0.4522\n",
      "Epoch 14 Batch 6600 Loss 1.8479 Accuracy 0.4522\n",
      "Epoch 14 Batch 6650 Loss 1.8477 Accuracy 0.4522\n",
      "Epoch 14 Batch 6700 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 6750 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 6800 Loss 1.8474 Accuracy 0.4523\n",
      "Epoch 14 Batch 6850 Loss 1.8475 Accuracy 0.4522\n",
      "Epoch 14 Batch 6900 Loss 1.8472 Accuracy 0.4523\n",
      "Epoch 14 Batch 6950 Loss 1.8469 Accuracy 0.4523\n",
      "Epoch 14 Batch 7000 Loss 1.8470 Accuracy 0.4523\n",
      "Epoch 14 Batch 7050 Loss 1.8469 Accuracy 0.4523\n",
      "Epoch 14 Batch 7100 Loss 1.8465 Accuracy 0.4523\n",
      "Epoch 14 Batch 7150 Loss 1.8465 Accuracy 0.4523\n",
      "Epoch 14 Batch 7200 Loss 1.8465 Accuracy 0.4523\n",
      "Epoch 14 Batch 7250 Loss 1.8463 Accuracy 0.4523\n",
      "Epoch 14 Batch 7300 Loss 1.8466 Accuracy 0.4523\n",
      "Epoch 14 Batch 7350 Loss 1.8465 Accuracy 0.4523\n",
      "Epoch 14 Batch 7400 Loss 1.8467 Accuracy 0.4523\n",
      "Epoch 14 Batch 7450 Loss 1.8472 Accuracy 0.4522\n",
      "Epoch 14 Batch 7500 Loss 1.8471 Accuracy 0.4523\n",
      "Epoch 14 Batch 7550 Loss 1.8470 Accuracy 0.4522\n",
      "Epoch 14 Batch 7600 Loss 1.8470 Accuracy 0.4522\n",
      "Epoch 14 Batch 7650 Loss 1.8470 Accuracy 0.4522\n",
      "Epoch 14 Batch 7700 Loss 1.8470 Accuracy 0.4522\n",
      "Epoch 14 Batch 7750 Loss 1.8467 Accuracy 0.4523\n",
      "Epoch 14 Batch 7800 Loss 1.8467 Accuracy 0.4523\n",
      "Epoch 14 Batch 7850 Loss 1.8466 Accuracy 0.4523\n",
      "Epoch 14 Batch 7900 Loss 1.8465 Accuracy 0.4523\n",
      "Epoch 14 Batch 7950 Loss 1.8464 Accuracy 0.4523\n",
      "Epoch 14 Batch 8000 Loss 1.8461 Accuracy 0.4523\n",
      "Epoch 14 Batch 8050 Loss 1.8458 Accuracy 0.4523\n",
      "Epoch 14 Batch 8100 Loss 1.8459 Accuracy 0.4523\n",
      "Epoch 14 Batch 8150 Loss 1.8460 Accuracy 0.4523\n",
      "Epoch 14 Batch 8200 Loss 1.8461 Accuracy 0.4523\n",
      "Epoch 14 Batch 8250 Loss 1.8459 Accuracy 0.4523\n",
      "Epoch 14 Batch 8300 Loss 1.8459 Accuracy 0.4523\n",
      "Epoch 14 Batch 8350 Loss 1.8459 Accuracy 0.4524\n",
      "Epoch 14 Batch 8400 Loss 1.8458 Accuracy 0.4524\n",
      "Epoch 14 Batch 8450 Loss 1.8458 Accuracy 0.4524\n",
      "Epoch 14 Batch 8500 Loss 1.8456 Accuracy 0.4524\n",
      "Epoch 14 Batch 8550 Loss 1.8455 Accuracy 0.4524\n",
      "Epoch 14 Batch 8600 Loss 1.8452 Accuracy 0.4525\n",
      "Epoch 14 Batch 8650 Loss 1.8452 Accuracy 0.4525\n",
      "Epoch 14 Batch 8700 Loss 1.8451 Accuracy 0.4525\n",
      "Epoch 14 Batch 8750 Loss 1.8451 Accuracy 0.4525\n",
      "Epoch 14 Batch 8800 Loss 1.8449 Accuracy 0.4525\n",
      "Epoch 14 Batch 8850 Loss 1.8451 Accuracy 0.4525\n",
      "Epoch 14 Batch 8900 Loss 1.8453 Accuracy 0.4525\n",
      "Epoch 14 Batch 8950 Loss 1.8453 Accuracy 0.4525\n",
      "Epoch 14 Batch 9000 Loss 1.8455 Accuracy 0.4525\n",
      "Epoch 14 Batch 9050 Loss 1.8457 Accuracy 0.4524\n",
      "Epoch 14 Batch 9100 Loss 1.8457 Accuracy 0.4524\n",
      "Epoch 14 Batch 9150 Loss 1.8456 Accuracy 0.4524\n",
      "Epoch 14 Batch 9200 Loss 1.8455 Accuracy 0.4524\n",
      "Epoch 14 Batch 9250 Loss 1.8455 Accuracy 0.4525\n",
      "Epoch 14 Batch 9300 Loss 1.8453 Accuracy 0.4525\n",
      "Epoch 14 Batch 9350 Loss 1.8452 Accuracy 0.4525\n",
      "Epoch 14 Batch 9400 Loss 1.8452 Accuracy 0.4525\n",
      "Epoch 14 Batch 9450 Loss 1.8453 Accuracy 0.4525\n",
      "Epoch 14 Batch 9500 Loss 1.8455 Accuracy 0.4525\n",
      "Epoch 14 Batch 9550 Loss 1.8455 Accuracy 0.4524\n",
      "Epoch 14 Batch 9600 Loss 1.8455 Accuracy 0.4524\n",
      "Epoch 14 Batch 9650 Loss 1.8455 Accuracy 0.4524\n",
      "Epoch 14 Batch 9700 Loss 1.8456 Accuracy 0.4524\n",
      "Epoch 14 Batch 9750 Loss 1.8457 Accuracy 0.4524\n",
      "Epoch 14 Batch 9800 Loss 1.8454 Accuracy 0.4524\n",
      "Epoch 14 Batch 9850 Loss 1.8453 Accuracy 0.4524\n",
      "Epoch 14 Batch 9900 Loss 1.8452 Accuracy 0.4524\n",
      "Epoch 14 Batch 9950 Loss 1.8451 Accuracy 0.4524\n",
      "Epoch 14 Batch 10000 Loss 1.8450 Accuracy 0.4524\n",
      "Epoch 14 Batch 10050 Loss 1.8450 Accuracy 0.4524\n",
      "Epoch 14 Batch 10100 Loss 1.8451 Accuracy 0.4524\n",
      "Epoch 14 Batch 10150 Loss 1.8451 Accuracy 0.4524\n",
      "Epoch 14 Batch 10200 Loss 1.8452 Accuracy 0.4524\n",
      "Epoch 14 Loss 1.8451 Accuracy 0.4524\n",
      "Time taken for 1 epoch: 431.6509187221527 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.7454 Accuracy 0.4630\n",
      "Epoch 15 Batch 50 Loss 1.8090 Accuracy 0.4563\n",
      "Epoch 15 Batch 100 Loss 1.7990 Accuracy 0.4580\n",
      "Epoch 15 Batch 150 Loss 1.8224 Accuracy 0.4556\n",
      "Epoch 15 Batch 200 Loss 1.8197 Accuracy 0.4557\n",
      "Epoch 15 Batch 250 Loss 1.8212 Accuracy 0.4554\n",
      "Epoch 15 Batch 300 Loss 1.8297 Accuracy 0.4554\n",
      "Epoch 15 Batch 350 Loss 1.8278 Accuracy 0.4552\n",
      "Epoch 15 Batch 400 Loss 1.8300 Accuracy 0.4546\n",
      "Epoch 15 Batch 450 Loss 1.8349 Accuracy 0.4540\n",
      "Epoch 15 Batch 500 Loss 1.8348 Accuracy 0.4541\n",
      "Epoch 15 Batch 550 Loss 1.8357 Accuracy 0.4543\n",
      "Epoch 15 Batch 600 Loss 1.8350 Accuracy 0.4541\n",
      "Epoch 15 Batch 650 Loss 1.8357 Accuracy 0.4540\n",
      "Epoch 15 Batch 700 Loss 1.8376 Accuracy 0.4538\n",
      "Epoch 15 Batch 750 Loss 1.8383 Accuracy 0.4538\n",
      "Epoch 15 Batch 800 Loss 1.8385 Accuracy 0.4537\n",
      "Epoch 15 Batch 850 Loss 1.8395 Accuracy 0.4533\n",
      "Epoch 15 Batch 900 Loss 1.8394 Accuracy 0.4533\n",
      "Epoch 15 Batch 950 Loss 1.8395 Accuracy 0.4532\n",
      "Epoch 15 Batch 1000 Loss 1.8397 Accuracy 0.4532\n",
      "Epoch 15 Batch 1050 Loss 1.8399 Accuracy 0.4533\n",
      "Epoch 15 Batch 1100 Loss 1.8396 Accuracy 0.4536\n",
      "Epoch 15 Batch 1150 Loss 1.8403 Accuracy 0.4535\n",
      "Epoch 15 Batch 1200 Loss 1.8395 Accuracy 0.4534\n",
      "Epoch 15 Batch 1250 Loss 1.8401 Accuracy 0.4532\n",
      "Epoch 15 Batch 1300 Loss 1.8403 Accuracy 0.4532\n",
      "Epoch 15 Batch 1350 Loss 1.8402 Accuracy 0.4531\n",
      "Epoch 15 Batch 1400 Loss 1.8398 Accuracy 0.4532\n",
      "Epoch 15 Batch 1450 Loss 1.8378 Accuracy 0.4533\n",
      "Epoch 15 Batch 1500 Loss 1.8383 Accuracy 0.4533\n",
      "Epoch 15 Batch 1550 Loss 1.8381 Accuracy 0.4532\n",
      "Epoch 15 Batch 1600 Loss 1.8383 Accuracy 0.4533\n",
      "Epoch 15 Batch 1650 Loss 1.8375 Accuracy 0.4534\n",
      "Epoch 15 Batch 1700 Loss 1.8373 Accuracy 0.4534\n",
      "Epoch 15 Batch 1750 Loss 1.8373 Accuracy 0.4534\n",
      "Epoch 15 Batch 1800 Loss 1.8375 Accuracy 0.4533\n",
      "Epoch 15 Batch 1850 Loss 1.8364 Accuracy 0.4534\n",
      "Epoch 15 Batch 1900 Loss 1.8359 Accuracy 0.4534\n",
      "Epoch 15 Batch 1950 Loss 1.8355 Accuracy 0.4534\n",
      "Epoch 15 Batch 2000 Loss 1.8358 Accuracy 0.4535\n",
      "Epoch 15 Batch 2050 Loss 1.8371 Accuracy 0.4534\n",
      "Epoch 15 Batch 2100 Loss 1.8368 Accuracy 0.4534\n",
      "Epoch 15 Batch 2150 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 2200 Loss 1.8374 Accuracy 0.4533\n",
      "Epoch 15 Batch 2250 Loss 1.8380 Accuracy 0.4534\n",
      "Epoch 15 Batch 2300 Loss 1.8377 Accuracy 0.4534\n",
      "Epoch 15 Batch 2350 Loss 1.8375 Accuracy 0.4535\n",
      "Epoch 15 Batch 2400 Loss 1.8367 Accuracy 0.4535\n",
      "Epoch 15 Batch 2450 Loss 1.8367 Accuracy 0.4536\n",
      "Epoch 15 Batch 2500 Loss 1.8365 Accuracy 0.4536\n",
      "Epoch 15 Batch 2550 Loss 1.8363 Accuracy 0.4536\n",
      "Epoch 15 Batch 2600 Loss 1.8365 Accuracy 0.4536\n",
      "Epoch 15 Batch 2650 Loss 1.8366 Accuracy 0.4536\n",
      "Epoch 15 Batch 2700 Loss 1.8370 Accuracy 0.4536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 2750 Loss 1.8367 Accuracy 0.4536\n",
      "Epoch 15 Batch 2800 Loss 1.8366 Accuracy 0.4536\n",
      "Epoch 15 Batch 2850 Loss 1.8362 Accuracy 0.4536\n",
      "Epoch 15 Batch 2900 Loss 1.8361 Accuracy 0.4536\n",
      "Epoch 15 Batch 2950 Loss 1.8365 Accuracy 0.4535\n",
      "Epoch 15 Batch 3000 Loss 1.8366 Accuracy 0.4536\n",
      "Epoch 15 Batch 3050 Loss 1.8368 Accuracy 0.4536\n",
      "Epoch 15 Batch 3100 Loss 1.8373 Accuracy 0.4535\n",
      "Epoch 15 Batch 3150 Loss 1.8377 Accuracy 0.4535\n",
      "Epoch 15 Batch 3200 Loss 1.8376 Accuracy 0.4536\n",
      "Epoch 15 Batch 3250 Loss 1.8378 Accuracy 0.4536\n",
      "Epoch 15 Batch 3300 Loss 1.8375 Accuracy 0.4536\n",
      "Epoch 15 Batch 3350 Loss 1.8378 Accuracy 0.4536\n",
      "Epoch 15 Batch 3400 Loss 1.8377 Accuracy 0.4536\n",
      "Epoch 15 Batch 3450 Loss 1.8379 Accuracy 0.4536\n",
      "Epoch 15 Batch 3500 Loss 1.8377 Accuracy 0.4536\n",
      "Epoch 15 Batch 3550 Loss 1.8375 Accuracy 0.4536\n",
      "Epoch 15 Batch 3600 Loss 1.8379 Accuracy 0.4536\n",
      "Epoch 15 Batch 3650 Loss 1.8375 Accuracy 0.4536\n",
      "Epoch 15 Batch 3700 Loss 1.8376 Accuracy 0.4535\n",
      "Epoch 15 Batch 3750 Loss 1.8376 Accuracy 0.4535\n",
      "Epoch 15 Batch 3800 Loss 1.8374 Accuracy 0.4534\n",
      "Epoch 15 Batch 3850 Loss 1.8369 Accuracy 0.4535\n",
      "Epoch 15 Batch 3900 Loss 1.8370 Accuracy 0.4534\n",
      "Epoch 15 Batch 3950 Loss 1.8371 Accuracy 0.4535\n",
      "Epoch 15 Batch 4000 Loss 1.8368 Accuracy 0.4535\n",
      "Epoch 15 Batch 4050 Loss 1.8368 Accuracy 0.4534\n",
      "Epoch 15 Batch 4100 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 4150 Loss 1.8367 Accuracy 0.4535\n",
      "Epoch 15 Batch 4200 Loss 1.8373 Accuracy 0.4534\n",
      "Epoch 15 Batch 4250 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 4300 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 4350 Loss 1.8374 Accuracy 0.4534\n",
      "Epoch 15 Batch 4400 Loss 1.8373 Accuracy 0.4534\n",
      "Epoch 15 Batch 4450 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 4500 Loss 1.8371 Accuracy 0.4534\n",
      "Epoch 15 Batch 4550 Loss 1.8368 Accuracy 0.4534\n",
      "Epoch 15 Batch 4600 Loss 1.8367 Accuracy 0.4535\n",
      "Epoch 15 Batch 4650 Loss 1.8368 Accuracy 0.4535\n",
      "Epoch 15 Batch 4700 Loss 1.8370 Accuracy 0.4535\n",
      "Epoch 15 Batch 4750 Loss 1.8371 Accuracy 0.4535\n",
      "Epoch 15 Batch 4800 Loss 1.8371 Accuracy 0.4535\n",
      "Epoch 15 Batch 4850 Loss 1.8372 Accuracy 0.4535\n",
      "Epoch 15 Batch 4900 Loss 1.8375 Accuracy 0.4535\n",
      "Epoch 15 Batch 4950 Loss 1.8374 Accuracy 0.4534\n",
      "Epoch 15 Batch 5000 Loss 1.8371 Accuracy 0.4534\n",
      "Epoch 15 Batch 5050 Loss 1.8373 Accuracy 0.4535\n",
      "Epoch 15 Batch 5100 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 5150 Loss 1.8372 Accuracy 0.4534\n",
      "Epoch 15 Batch 5200 Loss 1.8369 Accuracy 0.4534\n",
      "Epoch 15 Batch 5250 Loss 1.8370 Accuracy 0.4534\n",
      "Epoch 15 Batch 5300 Loss 1.8369 Accuracy 0.4534\n",
      "Epoch 15 Batch 5350 Loss 1.8364 Accuracy 0.4534\n",
      "Epoch 15 Batch 5400 Loss 1.8363 Accuracy 0.4534\n",
      "Epoch 15 Batch 5450 Loss 1.8365 Accuracy 0.4533\n",
      "Epoch 15 Batch 5500 Loss 1.8368 Accuracy 0.4533\n",
      "Epoch 15 Batch 5550 Loss 1.8366 Accuracy 0.4533\n",
      "Epoch 15 Batch 5600 Loss 1.8365 Accuracy 0.4533\n",
      "Epoch 15 Batch 5650 Loss 1.8367 Accuracy 0.4533\n",
      "Epoch 15 Batch 5700 Loss 1.8367 Accuracy 0.4533\n",
      "Epoch 15 Batch 5750 Loss 1.8367 Accuracy 0.4533\n",
      "Epoch 15 Batch 5800 Loss 1.8366 Accuracy 0.4533\n",
      "Epoch 15 Batch 5850 Loss 1.8367 Accuracy 0.4533\n",
      "Epoch 15 Batch 5900 Loss 1.8366 Accuracy 0.4533\n",
      "Epoch 15 Batch 5950 Loss 1.8365 Accuracy 0.4533\n",
      "Epoch 15 Batch 6000 Loss 1.8364 Accuracy 0.4533\n",
      "Epoch 15 Batch 6050 Loss 1.8362 Accuracy 0.4533\n",
      "Epoch 15 Batch 6100 Loss 1.8362 Accuracy 0.4533\n",
      "Epoch 15 Batch 6150 Loss 1.8362 Accuracy 0.4533\n",
      "Epoch 15 Batch 6200 Loss 1.8361 Accuracy 0.4533\n",
      "Epoch 15 Batch 6250 Loss 1.8363 Accuracy 0.4533\n",
      "Epoch 15 Batch 6300 Loss 1.8363 Accuracy 0.4533\n",
      "Epoch 15 Batch 6350 Loss 1.8364 Accuracy 0.4533\n",
      "Epoch 15 Batch 6400 Loss 1.8360 Accuracy 0.4533\n",
      "Epoch 15 Batch 6450 Loss 1.8361 Accuracy 0.4533\n",
      "Epoch 15 Batch 6500 Loss 1.8361 Accuracy 0.4533\n",
      "Epoch 15 Batch 6550 Loss 1.8362 Accuracy 0.4533\n",
      "Epoch 15 Batch 6600 Loss 1.8362 Accuracy 0.4533\n",
      "Epoch 15 Batch 6650 Loss 1.8359 Accuracy 0.4533\n",
      "Epoch 15 Batch 6700 Loss 1.8356 Accuracy 0.4533\n",
      "Epoch 15 Batch 6750 Loss 1.8356 Accuracy 0.4533\n",
      "Epoch 15 Batch 6800 Loss 1.8356 Accuracy 0.4533\n",
      "Epoch 15 Batch 6850 Loss 1.8356 Accuracy 0.4533\n",
      "Epoch 15 Batch 6900 Loss 1.8355 Accuracy 0.4533\n",
      "Epoch 15 Batch 6950 Loss 1.8357 Accuracy 0.4533\n",
      "Epoch 15 Batch 7000 Loss 1.8358 Accuracy 0.4533\n",
      "Epoch 15 Batch 7050 Loss 1.8358 Accuracy 0.4533\n",
      "Epoch 15 Batch 7100 Loss 1.8358 Accuracy 0.4533\n",
      "Epoch 15 Batch 7150 Loss 1.8358 Accuracy 0.4533\n",
      "Epoch 15 Batch 7200 Loss 1.8358 Accuracy 0.4533\n",
      "Epoch 15 Batch 7250 Loss 1.8355 Accuracy 0.4533\n",
      "Epoch 15 Batch 7300 Loss 1.8357 Accuracy 0.4534\n",
      "Epoch 15 Batch 7350 Loss 1.8355 Accuracy 0.4534\n",
      "Epoch 15 Batch 7400 Loss 1.8354 Accuracy 0.4534\n",
      "Epoch 15 Batch 7450 Loss 1.8354 Accuracy 0.4534\n",
      "Epoch 15 Batch 7500 Loss 1.8354 Accuracy 0.4534\n",
      "Epoch 15 Batch 7550 Loss 1.8354 Accuracy 0.4534\n",
      "Epoch 15 Batch 7600 Loss 1.8354 Accuracy 0.4534\n",
      "Epoch 15 Batch 7650 Loss 1.8351 Accuracy 0.4534\n",
      "Epoch 15 Batch 7700 Loss 1.8352 Accuracy 0.4534\n",
      "Epoch 15 Batch 7750 Loss 1.8351 Accuracy 0.4534\n",
      "Epoch 15 Batch 7800 Loss 1.8351 Accuracy 0.4534\n",
      "Epoch 15 Batch 7850 Loss 1.8351 Accuracy 0.4534\n",
      "Epoch 15 Batch 7900 Loss 1.8349 Accuracy 0.4535\n",
      "Epoch 15 Batch 7950 Loss 1.8348 Accuracy 0.4535\n",
      "Epoch 15 Batch 8000 Loss 1.8345 Accuracy 0.4535\n",
      "Epoch 15 Batch 8050 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 8100 Loss 1.8340 Accuracy 0.4535\n",
      "Epoch 15 Batch 8150 Loss 1.8339 Accuracy 0.4536\n",
      "Epoch 15 Batch 8200 Loss 1.8341 Accuracy 0.4535\n",
      "Epoch 15 Batch 8250 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 8300 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 8350 Loss 1.8345 Accuracy 0.4535\n",
      "Epoch 15 Batch 8400 Loss 1.8344 Accuracy 0.4535\n",
      "Epoch 15 Batch 8450 Loss 1.8344 Accuracy 0.4535\n",
      "Epoch 15 Batch 8500 Loss 1.8342 Accuracy 0.4536\n",
      "Epoch 15 Batch 8550 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 8600 Loss 1.8341 Accuracy 0.4536\n",
      "Epoch 15 Batch 8650 Loss 1.8342 Accuracy 0.4536\n",
      "Epoch 15 Batch 8700 Loss 1.8342 Accuracy 0.4536\n",
      "Epoch 15 Batch 8750 Loss 1.8340 Accuracy 0.4536\n",
      "Epoch 15 Batch 8800 Loss 1.8340 Accuracy 0.4536\n",
      "Epoch 15 Batch 8850 Loss 1.8341 Accuracy 0.4536\n",
      "Epoch 15 Batch 8900 Loss 1.8340 Accuracy 0.4536\n",
      "Epoch 15 Batch 8950 Loss 1.8340 Accuracy 0.4536\n",
      "Epoch 15 Batch 9000 Loss 1.8339 Accuracy 0.4536\n",
      "Epoch 15 Batch 9050 Loss 1.8338 Accuracy 0.4536\n",
      "Epoch 15 Batch 9100 Loss 1.8339 Accuracy 0.4535\n",
      "Epoch 15 Batch 9150 Loss 1.8340 Accuracy 0.4535\n",
      "Epoch 15 Batch 9200 Loss 1.8341 Accuracy 0.4535\n",
      "Epoch 15 Batch 9250 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 9300 Loss 1.8344 Accuracy 0.4535\n",
      "Epoch 15 Batch 9350 Loss 1.8344 Accuracy 0.4535\n",
      "Epoch 15 Batch 9400 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 9450 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 9500 Loss 1.8343 Accuracy 0.4535\n",
      "Epoch 15 Batch 9550 Loss 1.8345 Accuracy 0.4535\n",
      "Epoch 15 Batch 9600 Loss 1.8345 Accuracy 0.4535\n",
      "Epoch 15 Batch 9650 Loss 1.8343 Accuracy 0.4535\n",
      "Epoch 15 Batch 9700 Loss 1.8343 Accuracy 0.4535\n",
      "Epoch 15 Batch 9750 Loss 1.8344 Accuracy 0.4534\n",
      "Epoch 15 Batch 9800 Loss 1.8344 Accuracy 0.4534\n",
      "Epoch 15 Batch 9850 Loss 1.8344 Accuracy 0.4534\n",
      "Epoch 15 Batch 9900 Loss 1.8343 Accuracy 0.4534\n",
      "Epoch 15 Batch 9950 Loss 1.8343 Accuracy 0.4535\n",
      "Epoch 15 Batch 10000 Loss 1.8342 Accuracy 0.4535\n",
      "Epoch 15 Batch 10050 Loss 1.8340 Accuracy 0.4535\n",
      "Epoch 15 Batch 10100 Loss 1.8340 Accuracy 0.4535\n",
      "Epoch 15 Batch 10150 Loss 1.8340 Accuracy 0.4535\n",
      "Epoch 15 Batch 10200 Loss 1.8340 Accuracy 0.4535\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Epoch 15 Loss 1.8339 Accuracy 0.4535\n",
      "Time taken for 1 epoch: 431.85983419418335 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.7597 Accuracy 0.4326\n",
      "Epoch 16 Batch 50 Loss 1.8394 Accuracy 0.4499\n",
      "Epoch 16 Batch 100 Loss 1.8120 Accuracy 0.4536\n",
      "Epoch 16 Batch 150 Loss 1.8100 Accuracy 0.4556\n",
      "Epoch 16 Batch 200 Loss 1.8110 Accuracy 0.4553\n",
      "Epoch 16 Batch 250 Loss 1.8199 Accuracy 0.4546\n",
      "Epoch 16 Batch 300 Loss 1.8177 Accuracy 0.4545\n",
      "Epoch 16 Batch 350 Loss 1.8200 Accuracy 0.4547\n",
      "Epoch 16 Batch 400 Loss 1.8214 Accuracy 0.4547\n",
      "Epoch 16 Batch 450 Loss 1.8258 Accuracy 0.4548\n",
      "Epoch 16 Batch 500 Loss 1.8301 Accuracy 0.4547\n",
      "Epoch 16 Batch 550 Loss 1.8311 Accuracy 0.4548\n",
      "Epoch 16 Batch 600 Loss 1.8300 Accuracy 0.4548\n",
      "Epoch 16 Batch 650 Loss 1.8309 Accuracy 0.4547\n",
      "Epoch 16 Batch 700 Loss 1.8288 Accuracy 0.4549\n",
      "Epoch 16 Batch 750 Loss 1.8275 Accuracy 0.4548\n",
      "Epoch 16 Batch 800 Loss 1.8284 Accuracy 0.4547\n",
      "Epoch 16 Batch 850 Loss 1.8303 Accuracy 0.4546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 900 Loss 1.8302 Accuracy 0.4545\n",
      "Epoch 16 Batch 950 Loss 1.8288 Accuracy 0.4545\n",
      "Epoch 16 Batch 1000 Loss 1.8294 Accuracy 0.4545\n",
      "Epoch 16 Batch 1050 Loss 1.8294 Accuracy 0.4543\n",
      "Epoch 16 Batch 1100 Loss 1.8316 Accuracy 0.4541\n",
      "Epoch 16 Batch 1150 Loss 1.8333 Accuracy 0.4538\n",
      "Epoch 16 Batch 1200 Loss 1.8320 Accuracy 0.4542\n",
      "Epoch 16 Batch 1250 Loss 1.8313 Accuracy 0.4543\n",
      "Epoch 16 Batch 1300 Loss 1.8302 Accuracy 0.4542\n",
      "Epoch 16 Batch 1350 Loss 1.8307 Accuracy 0.4542\n",
      "Epoch 16 Batch 1400 Loss 1.8294 Accuracy 0.4543\n",
      "Epoch 16 Batch 1450 Loss 1.8303 Accuracy 0.4541\n",
      "Epoch 16 Batch 1500 Loss 1.8297 Accuracy 0.4542\n",
      "Epoch 16 Batch 1550 Loss 1.8290 Accuracy 0.4541\n",
      "Epoch 16 Batch 1600 Loss 1.8281 Accuracy 0.4542\n",
      "Epoch 16 Batch 1650 Loss 1.8290 Accuracy 0.4543\n",
      "Epoch 16 Batch 1700 Loss 1.8277 Accuracy 0.4544\n",
      "Epoch 16 Batch 1750 Loss 1.8268 Accuracy 0.4545\n",
      "Epoch 16 Batch 1800 Loss 1.8270 Accuracy 0.4544\n",
      "Epoch 16 Batch 1850 Loss 1.8272 Accuracy 0.4543\n",
      "Epoch 16 Batch 1900 Loss 1.8274 Accuracy 0.4544\n",
      "Epoch 16 Batch 1950 Loss 1.8268 Accuracy 0.4544\n",
      "Epoch 16 Batch 2000 Loss 1.8261 Accuracy 0.4545\n",
      "Epoch 16 Batch 2050 Loss 1.8263 Accuracy 0.4544\n",
      "Epoch 16 Batch 2100 Loss 1.8254 Accuracy 0.4545\n",
      "Epoch 16 Batch 2150 Loss 1.8242 Accuracy 0.4547\n",
      "Epoch 16 Batch 2200 Loss 1.8252 Accuracy 0.4546\n",
      "Epoch 16 Batch 2250 Loss 1.8255 Accuracy 0.4545\n",
      "Epoch 16 Batch 2300 Loss 1.8258 Accuracy 0.4544\n",
      "Epoch 16 Batch 2350 Loss 1.8260 Accuracy 0.4544\n",
      "Epoch 16 Batch 2400 Loss 1.8259 Accuracy 0.4545\n",
      "Epoch 16 Batch 2450 Loss 1.8263 Accuracy 0.4544\n",
      "Epoch 16 Batch 2500 Loss 1.8265 Accuracy 0.4543\n",
      "Epoch 16 Batch 2550 Loss 1.8260 Accuracy 0.4544\n",
      "Epoch 16 Batch 2600 Loss 1.8259 Accuracy 0.4544\n",
      "Epoch 16 Batch 2650 Loss 1.8257 Accuracy 0.4544\n",
      "Epoch 16 Batch 2700 Loss 1.8256 Accuracy 0.4545\n",
      "Epoch 16 Batch 2750 Loss 1.8261 Accuracy 0.4544\n",
      "Epoch 16 Batch 2800 Loss 1.8260 Accuracy 0.4544\n",
      "Epoch 16 Batch 2850 Loss 1.8261 Accuracy 0.4544\n",
      "Epoch 16 Batch 2900 Loss 1.8266 Accuracy 0.4543\n",
      "Epoch 16 Batch 2950 Loss 1.8263 Accuracy 0.4544\n",
      "Epoch 16 Batch 3000 Loss 1.8264 Accuracy 0.4543\n",
      "Epoch 16 Batch 3050 Loss 1.8267 Accuracy 0.4543\n",
      "Epoch 16 Batch 3100 Loss 1.8270 Accuracy 0.4544\n",
      "Epoch 16 Batch 3150 Loss 1.8273 Accuracy 0.4544\n",
      "Epoch 16 Batch 3200 Loss 1.8271 Accuracy 0.4544\n",
      "Epoch 16 Batch 3250 Loss 1.8276 Accuracy 0.4543\n",
      "Epoch 16 Batch 3300 Loss 1.8272 Accuracy 0.4543\n",
      "Epoch 16 Batch 3350 Loss 1.8273 Accuracy 0.4543\n",
      "Epoch 16 Batch 3400 Loss 1.8269 Accuracy 0.4544\n",
      "Epoch 16 Batch 3450 Loss 1.8269 Accuracy 0.4544\n",
      "Epoch 16 Batch 3500 Loss 1.8269 Accuracy 0.4544\n",
      "Epoch 16 Batch 3550 Loss 1.8266 Accuracy 0.4544\n",
      "Epoch 16 Batch 3600 Loss 1.8261 Accuracy 0.4545\n",
      "Epoch 16 Batch 3650 Loss 1.8265 Accuracy 0.4544\n",
      "Epoch 16 Batch 3700 Loss 1.8262 Accuracy 0.4544\n",
      "Epoch 16 Batch 3750 Loss 1.8261 Accuracy 0.4544\n",
      "Epoch 16 Batch 3800 Loss 1.8260 Accuracy 0.4545\n",
      "Epoch 16 Batch 3850 Loss 1.8255 Accuracy 0.4545\n",
      "Epoch 16 Batch 3900 Loss 1.8257 Accuracy 0.4545\n",
      "Epoch 16 Batch 3950 Loss 1.8256 Accuracy 0.4544\n",
      "Epoch 16 Batch 4000 Loss 1.8257 Accuracy 0.4544\n",
      "Epoch 16 Batch 4050 Loss 1.8255 Accuracy 0.4545\n",
      "Epoch 16 Batch 4100 Loss 1.8255 Accuracy 0.4544\n",
      "Epoch 16 Batch 4150 Loss 1.8255 Accuracy 0.4544\n",
      "Epoch 16 Batch 4200 Loss 1.8260 Accuracy 0.4543\n",
      "Epoch 16 Batch 4250 Loss 1.8259 Accuracy 0.4544\n",
      "Epoch 16 Batch 4300 Loss 1.8264 Accuracy 0.4543\n",
      "Epoch 16 Batch 4350 Loss 1.8267 Accuracy 0.4543\n",
      "Epoch 16 Batch 4400 Loss 1.8269 Accuracy 0.4543\n",
      "Epoch 16 Batch 4450 Loss 1.8266 Accuracy 0.4543\n",
      "Epoch 16 Batch 4500 Loss 1.8267 Accuracy 0.4542\n",
      "Epoch 16 Batch 4550 Loss 1.8268 Accuracy 0.4542\n",
      "Epoch 16 Batch 4600 Loss 1.8267 Accuracy 0.4542\n",
      "Epoch 16 Batch 4650 Loss 1.8267 Accuracy 0.4543\n",
      "Epoch 16 Batch 4700 Loss 1.8267 Accuracy 0.4543\n",
      "Epoch 16 Batch 4750 Loss 1.8269 Accuracy 0.4543\n",
      "Epoch 16 Batch 4800 Loss 1.8269 Accuracy 0.4544\n",
      "Epoch 16 Batch 4850 Loss 1.8268 Accuracy 0.4544\n",
      "Epoch 16 Batch 4900 Loss 1.8268 Accuracy 0.4544\n",
      "Epoch 16 Batch 4950 Loss 1.8269 Accuracy 0.4544\n",
      "Epoch 16 Batch 5000 Loss 1.8266 Accuracy 0.4543\n",
      "Epoch 16 Batch 5050 Loss 1.8266 Accuracy 0.4543\n",
      "Epoch 16 Batch 5100 Loss 1.8269 Accuracy 0.4543\n",
      "Epoch 16 Batch 5150 Loss 1.8271 Accuracy 0.4543\n",
      "Epoch 16 Batch 5200 Loss 1.8272 Accuracy 0.4543\n",
      "Epoch 16 Batch 5250 Loss 1.8274 Accuracy 0.4543\n",
      "Epoch 16 Batch 5300 Loss 1.8274 Accuracy 0.4542\n",
      "Epoch 16 Batch 5350 Loss 1.8273 Accuracy 0.4542\n",
      "Epoch 16 Batch 5400 Loss 1.8277 Accuracy 0.4542\n",
      "Epoch 16 Batch 5450 Loss 1.8272 Accuracy 0.4542\n",
      "Epoch 16 Batch 5500 Loss 1.8274 Accuracy 0.4542\n",
      "Epoch 16 Batch 5550 Loss 1.8271 Accuracy 0.4542\n",
      "Epoch 16 Batch 5600 Loss 1.8268 Accuracy 0.4542\n",
      "Epoch 16 Batch 5650 Loss 1.8265 Accuracy 0.4542\n",
      "Epoch 16 Batch 5700 Loss 1.8263 Accuracy 0.4542\n",
      "Epoch 16 Batch 5750 Loss 1.8264 Accuracy 0.4542\n",
      "Epoch 16 Batch 5800 Loss 1.8265 Accuracy 0.4543\n",
      "Epoch 16 Batch 5850 Loss 1.8263 Accuracy 0.4543\n",
      "Epoch 16 Batch 5900 Loss 1.8265 Accuracy 0.4542\n",
      "Epoch 16 Batch 5950 Loss 1.8265 Accuracy 0.4542\n",
      "Epoch 16 Batch 6000 Loss 1.8265 Accuracy 0.4543\n",
      "Epoch 16 Batch 6050 Loss 1.8263 Accuracy 0.4543\n",
      "Epoch 16 Batch 6100 Loss 1.8260 Accuracy 0.4543\n",
      "Epoch 16 Batch 6150 Loss 1.8258 Accuracy 0.4543\n",
      "Epoch 16 Batch 6200 Loss 1.8259 Accuracy 0.4543\n",
      "Epoch 16 Batch 6250 Loss 1.8258 Accuracy 0.4543\n",
      "Epoch 16 Batch 6300 Loss 1.8258 Accuracy 0.4543\n",
      "Epoch 16 Batch 6350 Loss 1.8259 Accuracy 0.4543\n",
      "Epoch 16 Batch 6400 Loss 1.8259 Accuracy 0.4543\n",
      "Epoch 16 Batch 6450 Loss 1.8257 Accuracy 0.4543\n",
      "Epoch 16 Batch 6500 Loss 1.8256 Accuracy 0.4544\n",
      "Epoch 16 Batch 6550 Loss 1.8254 Accuracy 0.4544\n",
      "Epoch 16 Batch 6600 Loss 1.8253 Accuracy 0.4544\n",
      "Epoch 16 Batch 6650 Loss 1.8252 Accuracy 0.4545\n",
      "Epoch 16 Batch 6700 Loss 1.8251 Accuracy 0.4544\n",
      "Epoch 16 Batch 6750 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 6800 Loss 1.8248 Accuracy 0.4544\n",
      "Epoch 16 Batch 6850 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 6900 Loss 1.8248 Accuracy 0.4544\n",
      "Epoch 16 Batch 6950 Loss 1.8248 Accuracy 0.4545\n",
      "Epoch 16 Batch 7000 Loss 1.8249 Accuracy 0.4544\n",
      "Epoch 16 Batch 7050 Loss 1.8251 Accuracy 0.4544\n",
      "Epoch 16 Batch 7100 Loss 1.8251 Accuracy 0.4544\n",
      "Epoch 16 Batch 7150 Loss 1.8249 Accuracy 0.4544\n",
      "Epoch 16 Batch 7200 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7250 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7300 Loss 1.8251 Accuracy 0.4544\n",
      "Epoch 16 Batch 7350 Loss 1.8252 Accuracy 0.4544\n",
      "Epoch 16 Batch 7400 Loss 1.8252 Accuracy 0.4544\n",
      "Epoch 16 Batch 7450 Loss 1.8251 Accuracy 0.4544\n",
      "Epoch 16 Batch 7500 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7550 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7600 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7650 Loss 1.8249 Accuracy 0.4544\n",
      "Epoch 16 Batch 7700 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7750 Loss 1.8249 Accuracy 0.4544\n",
      "Epoch 16 Batch 7800 Loss 1.8248 Accuracy 0.4544\n",
      "Epoch 16 Batch 7850 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7900 Loss 1.8250 Accuracy 0.4544\n",
      "Epoch 16 Batch 7950 Loss 1.8246 Accuracy 0.4544\n",
      "Epoch 16 Batch 8000 Loss 1.8244 Accuracy 0.4545\n",
      "Epoch 16 Batch 8050 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 8100 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 8150 Loss 1.8240 Accuracy 0.4545\n",
      "Epoch 16 Batch 8200 Loss 1.8238 Accuracy 0.4545\n",
      "Epoch 16 Batch 8250 Loss 1.8238 Accuracy 0.4545\n",
      "Epoch 16 Batch 8300 Loss 1.8241 Accuracy 0.4545\n",
      "Epoch 16 Batch 8350 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 8400 Loss 1.8244 Accuracy 0.4545\n",
      "Epoch 16 Batch 8450 Loss 1.8244 Accuracy 0.4545\n",
      "Epoch 16 Batch 8500 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 8550 Loss 1.8241 Accuracy 0.4545\n",
      "Epoch 16 Batch 8600 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 8650 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 8700 Loss 1.8245 Accuracy 0.4545\n",
      "Epoch 16 Batch 8750 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 8800 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 8850 Loss 1.8241 Accuracy 0.4545\n",
      "Epoch 16 Batch 8900 Loss 1.8240 Accuracy 0.4546\n",
      "Epoch 16 Batch 8950 Loss 1.8242 Accuracy 0.4546\n",
      "Epoch 16 Batch 9000 Loss 1.8244 Accuracy 0.4546\n",
      "Epoch 16 Batch 9050 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9100 Loss 1.8244 Accuracy 0.4545\n",
      "Epoch 16 Batch 9150 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9200 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9250 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9300 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 9350 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9400 Loss 1.8242 Accuracy 0.4546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 9450 Loss 1.8243 Accuracy 0.4546\n",
      "Epoch 16 Batch 9500 Loss 1.8242 Accuracy 0.4546\n",
      "Epoch 16 Batch 9550 Loss 1.8243 Accuracy 0.4546\n",
      "Epoch 16 Batch 9600 Loss 1.8244 Accuracy 0.4545\n",
      "Epoch 16 Batch 9650 Loss 1.8244 Accuracy 0.4546\n",
      "Epoch 16 Batch 9700 Loss 1.8245 Accuracy 0.4546\n",
      "Epoch 16 Batch 9750 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9800 Loss 1.8243 Accuracy 0.4546\n",
      "Epoch 16 Batch 9850 Loss 1.8243 Accuracy 0.4545\n",
      "Epoch 16 Batch 9900 Loss 1.8242 Accuracy 0.4546\n",
      "Epoch 16 Batch 9950 Loss 1.8242 Accuracy 0.4546\n",
      "Epoch 16 Batch 10000 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 10050 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 10100 Loss 1.8242 Accuracy 0.4545\n",
      "Epoch 16 Batch 10150 Loss 1.8241 Accuracy 0.4545\n",
      "Epoch 16 Batch 10200 Loss 1.8239 Accuracy 0.4545\n",
      "Epoch 16 Loss 1.8240 Accuracy 0.4545\n",
      "Time taken for 1 epoch: 431.74508261680603 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.7983 Accuracy 0.4712\n",
      "Epoch 17 Batch 50 Loss 1.7728 Accuracy 0.4561\n",
      "Epoch 17 Batch 100 Loss 1.7633 Accuracy 0.4603\n",
      "Epoch 17 Batch 150 Loss 1.7771 Accuracy 0.4585\n",
      "Epoch 17 Batch 200 Loss 1.7913 Accuracy 0.4577\n",
      "Epoch 17 Batch 250 Loss 1.8033 Accuracy 0.4562\n",
      "Epoch 17 Batch 300 Loss 1.8071 Accuracy 0.4563\n",
      "Epoch 17 Batch 350 Loss 1.8077 Accuracy 0.4563\n",
      "Epoch 17 Batch 400 Loss 1.8110 Accuracy 0.4559\n",
      "Epoch 17 Batch 450 Loss 1.8114 Accuracy 0.4560\n",
      "Epoch 17 Batch 500 Loss 1.8108 Accuracy 0.4563\n",
      "Epoch 17 Batch 550 Loss 1.8125 Accuracy 0.4565\n",
      "Epoch 17 Batch 600 Loss 1.8141 Accuracy 0.4560\n",
      "Epoch 17 Batch 650 Loss 1.8164 Accuracy 0.4558\n",
      "Epoch 17 Batch 700 Loss 1.8178 Accuracy 0.4558\n",
      "Epoch 17 Batch 750 Loss 1.8210 Accuracy 0.4558\n",
      "Epoch 17 Batch 800 Loss 1.8209 Accuracy 0.4558\n",
      "Epoch 17 Batch 850 Loss 1.8207 Accuracy 0.4556\n",
      "Epoch 17 Batch 900 Loss 1.8194 Accuracy 0.4556\n",
      "Epoch 17 Batch 950 Loss 1.8204 Accuracy 0.4555\n",
      "Epoch 17 Batch 1000 Loss 1.8192 Accuracy 0.4558\n",
      "Epoch 17 Batch 1050 Loss 1.8203 Accuracy 0.4556\n",
      "Epoch 17 Batch 1100 Loss 1.8200 Accuracy 0.4557\n",
      "Epoch 17 Batch 1150 Loss 1.8200 Accuracy 0.4556\n",
      "Epoch 17 Batch 1200 Loss 1.8204 Accuracy 0.4555\n",
      "Epoch 17 Batch 1250 Loss 1.8207 Accuracy 0.4556\n",
      "Epoch 17 Batch 1300 Loss 1.8199 Accuracy 0.4556\n",
      "Epoch 17 Batch 1350 Loss 1.8184 Accuracy 0.4558\n",
      "Epoch 17 Batch 1400 Loss 1.8169 Accuracy 0.4558\n",
      "Epoch 17 Batch 1450 Loss 1.8166 Accuracy 0.4559\n",
      "Epoch 17 Batch 1500 Loss 1.8160 Accuracy 0.4558\n",
      "Epoch 17 Batch 1550 Loss 1.8168 Accuracy 0.4557\n",
      "Epoch 17 Batch 1600 Loss 1.8162 Accuracy 0.4556\n",
      "Epoch 17 Batch 1650 Loss 1.8162 Accuracy 0.4557\n",
      "Epoch 17 Batch 1700 Loss 1.8155 Accuracy 0.4557\n",
      "Epoch 17 Batch 1750 Loss 1.8154 Accuracy 0.4558\n",
      "Epoch 17 Batch 1800 Loss 1.8156 Accuracy 0.4558\n",
      "Epoch 17 Batch 1850 Loss 1.8148 Accuracy 0.4559\n",
      "Epoch 17 Batch 1900 Loss 1.8150 Accuracy 0.4558\n",
      "Epoch 17 Batch 1950 Loss 1.8155 Accuracy 0.4559\n",
      "Epoch 17 Batch 2000 Loss 1.8145 Accuracy 0.4560\n",
      "Epoch 17 Batch 2050 Loss 1.8138 Accuracy 0.4559\n",
      "Epoch 17 Batch 2100 Loss 1.8137 Accuracy 0.4559\n",
      "Epoch 17 Batch 2150 Loss 1.8131 Accuracy 0.4560\n",
      "Epoch 17 Batch 2200 Loss 1.8136 Accuracy 0.4560\n",
      "Epoch 17 Batch 2250 Loss 1.8132 Accuracy 0.4560\n",
      "Epoch 17 Batch 2300 Loss 1.8136 Accuracy 0.4560\n",
      "Epoch 17 Batch 2350 Loss 1.8136 Accuracy 0.4560\n",
      "Epoch 17 Batch 2400 Loss 1.8150 Accuracy 0.4559\n",
      "Epoch 17 Batch 2450 Loss 1.8149 Accuracy 0.4559\n",
      "Epoch 17 Batch 2500 Loss 1.8151 Accuracy 0.4559\n",
      "Epoch 17 Batch 2550 Loss 1.8147 Accuracy 0.4560\n",
      "Epoch 17 Batch 2600 Loss 1.8151 Accuracy 0.4559\n",
      "Epoch 17 Batch 2650 Loss 1.8150 Accuracy 0.4559\n",
      "Epoch 17 Batch 2700 Loss 1.8150 Accuracy 0.4559\n",
      "Epoch 17 Batch 2750 Loss 1.8154 Accuracy 0.4558\n",
      "Epoch 17 Batch 2800 Loss 1.8148 Accuracy 0.4558\n",
      "Epoch 17 Batch 2850 Loss 1.8151 Accuracy 0.4557\n",
      "Epoch 17 Batch 2900 Loss 1.8152 Accuracy 0.4558\n",
      "Epoch 17 Batch 2950 Loss 1.8160 Accuracy 0.4557\n",
      "Epoch 17 Batch 3000 Loss 1.8160 Accuracy 0.4558\n",
      "Epoch 17 Batch 3050 Loss 1.8158 Accuracy 0.4557\n",
      "Epoch 17 Batch 3100 Loss 1.8159 Accuracy 0.4558\n",
      "Epoch 17 Batch 3150 Loss 1.8163 Accuracy 0.4557\n",
      "Epoch 17 Batch 3200 Loss 1.8168 Accuracy 0.4557\n",
      "Epoch 17 Batch 3250 Loss 1.8174 Accuracy 0.4557\n",
      "Epoch 17 Batch 3300 Loss 1.8170 Accuracy 0.4556\n",
      "Epoch 17 Batch 3350 Loss 1.8168 Accuracy 0.4557\n",
      "Epoch 17 Batch 3400 Loss 1.8159 Accuracy 0.4558\n",
      "Epoch 17 Batch 3450 Loss 1.8156 Accuracy 0.4558\n",
      "Epoch 17 Batch 3500 Loss 1.8155 Accuracy 0.4558\n",
      "Epoch 17 Batch 3550 Loss 1.8158 Accuracy 0.4557\n",
      "Epoch 17 Batch 3600 Loss 1.8162 Accuracy 0.4557\n",
      "Epoch 17 Batch 3650 Loss 1.8164 Accuracy 0.4557\n",
      "Epoch 17 Batch 3700 Loss 1.8165 Accuracy 0.4557\n",
      "Epoch 17 Batch 3750 Loss 1.8167 Accuracy 0.4556\n",
      "Epoch 17 Batch 3800 Loss 1.8166 Accuracy 0.4556\n",
      "Epoch 17 Batch 3850 Loss 1.8165 Accuracy 0.4556\n",
      "Epoch 17 Batch 3900 Loss 1.8168 Accuracy 0.4556\n",
      "Epoch 17 Batch 3950 Loss 1.8168 Accuracy 0.4556\n",
      "Epoch 17 Batch 4000 Loss 1.8171 Accuracy 0.4555\n",
      "Epoch 17 Batch 4050 Loss 1.8172 Accuracy 0.4555\n",
      "Epoch 17 Batch 4100 Loss 1.8172 Accuracy 0.4555\n",
      "Epoch 17 Batch 4150 Loss 1.8168 Accuracy 0.4556\n",
      "Epoch 17 Batch 4200 Loss 1.8170 Accuracy 0.4556\n",
      "Epoch 17 Batch 4250 Loss 1.8170 Accuracy 0.4556\n",
      "Epoch 17 Batch 4300 Loss 1.8168 Accuracy 0.4556\n",
      "Epoch 17 Batch 4350 Loss 1.8169 Accuracy 0.4556\n",
      "Epoch 17 Batch 4400 Loss 1.8168 Accuracy 0.4556\n",
      "Epoch 17 Batch 4450 Loss 1.8166 Accuracy 0.4556\n",
      "Epoch 17 Batch 4500 Loss 1.8163 Accuracy 0.4556\n",
      "Epoch 17 Batch 4550 Loss 1.8164 Accuracy 0.4556\n",
      "Epoch 17 Batch 4600 Loss 1.8167 Accuracy 0.4555\n",
      "Epoch 17 Batch 4650 Loss 1.8163 Accuracy 0.4556\n",
      "Epoch 17 Batch 4700 Loss 1.8163 Accuracy 0.4556\n",
      "Epoch 17 Batch 4750 Loss 1.8164 Accuracy 0.4556\n",
      "Epoch 17 Batch 4800 Loss 1.8166 Accuracy 0.4556\n",
      "Epoch 17 Batch 4850 Loss 1.8170 Accuracy 0.4555\n",
      "Epoch 17 Batch 4900 Loss 1.8170 Accuracy 0.4555\n",
      "Epoch 17 Batch 4950 Loss 1.8171 Accuracy 0.4555\n",
      "Epoch 17 Batch 5000 Loss 1.8166 Accuracy 0.4555\n",
      "Epoch 17 Batch 5050 Loss 1.8170 Accuracy 0.4555\n",
      "Epoch 17 Batch 5100 Loss 1.8173 Accuracy 0.4555\n",
      "Epoch 17 Batch 5150 Loss 1.8170 Accuracy 0.4555\n",
      "Epoch 17 Batch 5200 Loss 1.8170 Accuracy 0.4555\n",
      "Epoch 17 Batch 5250 Loss 1.8168 Accuracy 0.4555\n",
      "Epoch 17 Batch 5300 Loss 1.8162 Accuracy 0.4555\n",
      "Epoch 17 Batch 5350 Loss 1.8162 Accuracy 0.4555\n",
      "Epoch 17 Batch 5400 Loss 1.8161 Accuracy 0.4555\n",
      "Epoch 17 Batch 5450 Loss 1.8162 Accuracy 0.4555\n",
      "Epoch 17 Batch 5500 Loss 1.8165 Accuracy 0.4555\n",
      "Epoch 17 Batch 5550 Loss 1.8165 Accuracy 0.4554\n",
      "Epoch 17 Batch 5600 Loss 1.8162 Accuracy 0.4554\n",
      "Epoch 17 Batch 5650 Loss 1.8160 Accuracy 0.4555\n",
      "Epoch 17 Batch 5700 Loss 1.8162 Accuracy 0.4555\n",
      "Epoch 17 Batch 5750 Loss 1.8161 Accuracy 0.4555\n",
      "Epoch 17 Batch 5800 Loss 1.8162 Accuracy 0.4555\n",
      "Epoch 17 Batch 5850 Loss 1.8160 Accuracy 0.4555\n",
      "Epoch 17 Batch 5900 Loss 1.8161 Accuracy 0.4554\n",
      "Epoch 17 Batch 5950 Loss 1.8159 Accuracy 0.4555\n",
      "Epoch 17 Batch 6000 Loss 1.8160 Accuracy 0.4555\n",
      "Epoch 17 Batch 6050 Loss 1.8160 Accuracy 0.4555\n",
      "Epoch 17 Batch 6100 Loss 1.8157 Accuracy 0.4555\n",
      "Epoch 17 Batch 6150 Loss 1.8159 Accuracy 0.4555\n",
      "Epoch 17 Batch 6200 Loss 1.8155 Accuracy 0.4555\n",
      "Epoch 17 Batch 6250 Loss 1.8157 Accuracy 0.4555\n",
      "Epoch 17 Batch 6300 Loss 1.8157 Accuracy 0.4555\n",
      "Epoch 17 Batch 6350 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 6400 Loss 1.8155 Accuracy 0.4555\n",
      "Epoch 17 Batch 6450 Loss 1.8155 Accuracy 0.4555\n",
      "Epoch 17 Batch 6500 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 6550 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 6600 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 6650 Loss 1.8155 Accuracy 0.4555\n",
      "Epoch 17 Batch 6700 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 6750 Loss 1.8151 Accuracy 0.4555\n",
      "Epoch 17 Batch 6800 Loss 1.8149 Accuracy 0.4556\n",
      "Epoch 17 Batch 6850 Loss 1.8150 Accuracy 0.4555\n",
      "Epoch 17 Batch 6900 Loss 1.8154 Accuracy 0.4555\n",
      "Epoch 17 Batch 6950 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7000 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7050 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7100 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7150 Loss 1.8155 Accuracy 0.4555\n",
      "Epoch 17 Batch 7200 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 7250 Loss 1.8157 Accuracy 0.4555\n",
      "Epoch 17 Batch 7300 Loss 1.8156 Accuracy 0.4555\n",
      "Epoch 17 Batch 7350 Loss 1.8154 Accuracy 0.4555\n",
      "Epoch 17 Batch 7400 Loss 1.8154 Accuracy 0.4555\n",
      "Epoch 17 Batch 7450 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7500 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7550 Loss 1.8153 Accuracy 0.4556\n",
      "Epoch 17 Batch 7600 Loss 1.8153 Accuracy 0.4556\n",
      "Epoch 17 Batch 7650 Loss 1.8154 Accuracy 0.4555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 7700 Loss 1.8153 Accuracy 0.4555\n",
      "Epoch 17 Batch 7750 Loss 1.8152 Accuracy 0.4556\n",
      "Epoch 17 Batch 7800 Loss 1.8153 Accuracy 0.4556\n",
      "Epoch 17 Batch 7850 Loss 1.8152 Accuracy 0.4556\n",
      "Epoch 17 Batch 7900 Loss 1.8149 Accuracy 0.4556\n",
      "Epoch 17 Batch 7950 Loss 1.8149 Accuracy 0.4556\n",
      "Epoch 17 Batch 8000 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8050 Loss 1.8145 Accuracy 0.4556\n",
      "Epoch 17 Batch 8100 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8150 Loss 1.8147 Accuracy 0.4556\n",
      "Epoch 17 Batch 8200 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8250 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8300 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8350 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8400 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 8450 Loss 1.8145 Accuracy 0.4556\n",
      "Epoch 17 Batch 8500 Loss 1.8144 Accuracy 0.4556\n",
      "Epoch 17 Batch 8550 Loss 1.8144 Accuracy 0.4556\n",
      "Epoch 17 Batch 8600 Loss 1.8143 Accuracy 0.4556\n",
      "Epoch 17 Batch 8650 Loss 1.8141 Accuracy 0.4557\n",
      "Epoch 17 Batch 8700 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 8750 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 8800 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 8850 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 8900 Loss 1.8142 Accuracy 0.4557\n",
      "Epoch 17 Batch 8950 Loss 1.8144 Accuracy 0.4556\n",
      "Epoch 17 Batch 9000 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9050 Loss 1.8146 Accuracy 0.4557\n",
      "Epoch 17 Batch 9100 Loss 1.8145 Accuracy 0.4556\n",
      "Epoch 17 Batch 9150 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 9200 Loss 1.8147 Accuracy 0.4556\n",
      "Epoch 17 Batch 9250 Loss 1.8148 Accuracy 0.4556\n",
      "Epoch 17 Batch 9300 Loss 1.8145 Accuracy 0.4556\n",
      "Epoch 17 Batch 9350 Loss 1.8146 Accuracy 0.4557\n",
      "Epoch 17 Batch 9400 Loss 1.8145 Accuracy 0.4557\n",
      "Epoch 17 Batch 9450 Loss 1.8145 Accuracy 0.4557\n",
      "Epoch 17 Batch 9500 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9550 Loss 1.8143 Accuracy 0.4557\n",
      "Epoch 17 Batch 9600 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9650 Loss 1.8145 Accuracy 0.4557\n",
      "Epoch 17 Batch 9700 Loss 1.8145 Accuracy 0.4557\n",
      "Epoch 17 Batch 9750 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9800 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9850 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9900 Loss 1.8144 Accuracy 0.4557\n",
      "Epoch 17 Batch 9950 Loss 1.8145 Accuracy 0.4556\n",
      "Epoch 17 Batch 10000 Loss 1.8146 Accuracy 0.4556\n",
      "Epoch 17 Batch 10050 Loss 1.8148 Accuracy 0.4556\n",
      "Epoch 17 Batch 10100 Loss 1.8147 Accuracy 0.4556\n",
      "Epoch 17 Batch 10150 Loss 1.8144 Accuracy 0.4556\n",
      "Epoch 17 Batch 10200 Loss 1.8144 Accuracy 0.4556\n",
      "Epoch 17 Loss 1.8144 Accuracy 0.4556\n",
      "Time taken for 1 epoch: 431.74244117736816 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.9251 Accuracy 0.4400\n",
      "Epoch 18 Batch 50 Loss 1.8544 Accuracy 0.4536\n",
      "Epoch 18 Batch 100 Loss 1.8175 Accuracy 0.4551\n",
      "Epoch 18 Batch 150 Loss 1.8152 Accuracy 0.4559\n",
      "Epoch 18 Batch 200 Loss 1.8117 Accuracy 0.4572\n",
      "Epoch 18 Batch 250 Loss 1.8092 Accuracy 0.4578\n",
      "Epoch 18 Batch 300 Loss 1.8095 Accuracy 0.4570\n",
      "Epoch 18 Batch 350 Loss 1.8084 Accuracy 0.4569\n",
      "Epoch 18 Batch 400 Loss 1.8097 Accuracy 0.4571\n",
      "Epoch 18 Batch 450 Loss 1.8088 Accuracy 0.4568\n",
      "Epoch 18 Batch 500 Loss 1.8083 Accuracy 0.4568\n",
      "Epoch 18 Batch 550 Loss 1.8127 Accuracy 0.4563\n",
      "Epoch 18 Batch 600 Loss 1.8135 Accuracy 0.4563\n",
      "Epoch 18 Batch 650 Loss 1.8126 Accuracy 0.4567\n",
      "Epoch 18 Batch 700 Loss 1.8113 Accuracy 0.4565\n",
      "Epoch 18 Batch 750 Loss 1.8100 Accuracy 0.4567\n",
      "Epoch 18 Batch 800 Loss 1.8113 Accuracy 0.4566\n",
      "Epoch 18 Batch 850 Loss 1.8117 Accuracy 0.4567\n",
      "Epoch 18 Batch 900 Loss 1.8100 Accuracy 0.4568\n",
      "Epoch 18 Batch 950 Loss 1.8114 Accuracy 0.4565\n",
      "Epoch 18 Batch 1000 Loss 1.8119 Accuracy 0.4566\n",
      "Epoch 18 Batch 1050 Loss 1.8116 Accuracy 0.4566\n",
      "Epoch 18 Batch 1100 Loss 1.8120 Accuracy 0.4567\n",
      "Epoch 18 Batch 1150 Loss 1.8126 Accuracy 0.4566\n",
      "Epoch 18 Batch 1200 Loss 1.8120 Accuracy 0.4567\n",
      "Epoch 18 Batch 1250 Loss 1.8118 Accuracy 0.4566\n",
      "Epoch 18 Batch 1300 Loss 1.8118 Accuracy 0.4567\n",
      "Epoch 18 Batch 1350 Loss 1.8108 Accuracy 0.4567\n",
      "Epoch 18 Batch 1400 Loss 1.8097 Accuracy 0.4566\n",
      "Epoch 18 Batch 1450 Loss 1.8089 Accuracy 0.4567\n",
      "Epoch 18 Batch 1500 Loss 1.8096 Accuracy 0.4567\n",
      "Epoch 18 Batch 1550 Loss 1.8088 Accuracy 0.4567\n",
      "Epoch 18 Batch 1600 Loss 1.8089 Accuracy 0.4567\n",
      "Epoch 18 Batch 1650 Loss 1.8084 Accuracy 0.4567\n",
      "Epoch 18 Batch 1700 Loss 1.8091 Accuracy 0.4565\n",
      "Epoch 18 Batch 1750 Loss 1.8090 Accuracy 0.4565\n",
      "Epoch 18 Batch 1800 Loss 1.8091 Accuracy 0.4565\n",
      "Epoch 18 Batch 1850 Loss 1.8098 Accuracy 0.4564\n",
      "Epoch 18 Batch 1900 Loss 1.8099 Accuracy 0.4564\n",
      "Epoch 18 Batch 1950 Loss 1.8087 Accuracy 0.4565\n",
      "Epoch 18 Batch 2000 Loss 1.8086 Accuracy 0.4565\n",
      "Epoch 18 Batch 2050 Loss 1.8088 Accuracy 0.4565\n",
      "Epoch 18 Batch 2100 Loss 1.8088 Accuracy 0.4564\n",
      "Epoch 18 Batch 2150 Loss 1.8087 Accuracy 0.4566\n",
      "Epoch 18 Batch 2200 Loss 1.8085 Accuracy 0.4566\n",
      "Epoch 18 Batch 2250 Loss 1.8084 Accuracy 0.4566\n",
      "Epoch 18 Batch 2300 Loss 1.8088 Accuracy 0.4565\n",
      "Epoch 18 Batch 2350 Loss 1.8080 Accuracy 0.4566\n",
      "Epoch 18 Batch 2400 Loss 1.8081 Accuracy 0.4566\n",
      "Epoch 18 Batch 2450 Loss 1.8070 Accuracy 0.4566\n",
      "Epoch 18 Batch 2500 Loss 1.8075 Accuracy 0.4566\n",
      "Epoch 18 Batch 2550 Loss 1.8083 Accuracy 0.4566\n",
      "Epoch 18 Batch 2600 Loss 1.8082 Accuracy 0.4565\n",
      "Epoch 18 Batch 2650 Loss 1.8086 Accuracy 0.4565\n",
      "Epoch 18 Batch 2700 Loss 1.8089 Accuracy 0.4565\n",
      "Epoch 18 Batch 2750 Loss 1.8088 Accuracy 0.4565\n",
      "Epoch 18 Batch 2800 Loss 1.8091 Accuracy 0.4565\n",
      "Epoch 18 Batch 2850 Loss 1.8093 Accuracy 0.4565\n",
      "Epoch 18 Batch 2900 Loss 1.8094 Accuracy 0.4565\n",
      "Epoch 18 Batch 2950 Loss 1.8099 Accuracy 0.4565\n",
      "Epoch 18 Batch 3000 Loss 1.8096 Accuracy 0.4564\n",
      "Epoch 18 Batch 3050 Loss 1.8099 Accuracy 0.4563\n",
      "Epoch 18 Batch 3100 Loss 1.8097 Accuracy 0.4563\n",
      "Epoch 18 Batch 3150 Loss 1.8098 Accuracy 0.4563\n",
      "Epoch 18 Batch 3200 Loss 1.8094 Accuracy 0.4564\n",
      "Epoch 18 Batch 3250 Loss 1.8092 Accuracy 0.4565\n",
      "Epoch 18 Batch 3300 Loss 1.8092 Accuracy 0.4565\n",
      "Epoch 18 Batch 3350 Loss 1.8096 Accuracy 0.4565\n",
      "Epoch 18 Batch 3400 Loss 1.8091 Accuracy 0.4564\n",
      "Epoch 18 Batch 3450 Loss 1.8089 Accuracy 0.4565\n",
      "Epoch 18 Batch 3500 Loss 1.8089 Accuracy 0.4564\n",
      "Epoch 18 Batch 3550 Loss 1.8091 Accuracy 0.4564\n",
      "Epoch 18 Batch 3600 Loss 1.8096 Accuracy 0.4563\n",
      "Epoch 18 Batch 3650 Loss 1.8092 Accuracy 0.4563\n",
      "Epoch 18 Batch 3700 Loss 1.8096 Accuracy 0.4563\n",
      "Epoch 18 Batch 3750 Loss 1.8091 Accuracy 0.4563\n",
      "Epoch 18 Batch 3800 Loss 1.8089 Accuracy 0.4563\n",
      "Epoch 18 Batch 3850 Loss 1.8091 Accuracy 0.4562\n",
      "Epoch 18 Batch 3900 Loss 1.8096 Accuracy 0.4562\n",
      "Epoch 18 Batch 3950 Loss 1.8095 Accuracy 0.4562\n",
      "Epoch 18 Batch 4000 Loss 1.8094 Accuracy 0.4562\n",
      "Epoch 18 Batch 4050 Loss 1.8094 Accuracy 0.4562\n",
      "Epoch 18 Batch 4100 Loss 1.8093 Accuracy 0.4563\n",
      "Epoch 18 Batch 4150 Loss 1.8095 Accuracy 0.4563\n",
      "Epoch 18 Batch 4200 Loss 1.8094 Accuracy 0.4563\n",
      "Epoch 18 Batch 4250 Loss 1.8092 Accuracy 0.4563\n",
      "Epoch 18 Batch 4300 Loss 1.8091 Accuracy 0.4563\n",
      "Epoch 18 Batch 4350 Loss 1.8090 Accuracy 0.4563\n",
      "Epoch 18 Batch 4400 Loss 1.8095 Accuracy 0.4562\n",
      "Epoch 18 Batch 4450 Loss 1.8100 Accuracy 0.4562\n",
      "Epoch 18 Batch 4500 Loss 1.8099 Accuracy 0.4562\n",
      "Epoch 18 Batch 4550 Loss 1.8096 Accuracy 0.4562\n",
      "Epoch 18 Batch 4600 Loss 1.8099 Accuracy 0.4562\n",
      "Epoch 18 Batch 4650 Loss 1.8099 Accuracy 0.4562\n",
      "Epoch 18 Batch 4700 Loss 1.8095 Accuracy 0.4561\n",
      "Epoch 18 Batch 4750 Loss 1.8093 Accuracy 0.4562\n",
      "Epoch 18 Batch 4800 Loss 1.8091 Accuracy 0.4562\n",
      "Epoch 18 Batch 4850 Loss 1.8091 Accuracy 0.4563\n",
      "Epoch 18 Batch 4900 Loss 1.8086 Accuracy 0.4563\n",
      "Epoch 18 Batch 4950 Loss 1.8091 Accuracy 0.4562\n",
      "Epoch 18 Batch 5000 Loss 1.8090 Accuracy 0.4562\n",
      "Epoch 18 Batch 5050 Loss 1.8089 Accuracy 0.4563\n",
      "Epoch 18 Batch 5100 Loss 1.8089 Accuracy 0.4563\n",
      "Epoch 18 Batch 5150 Loss 1.8094 Accuracy 0.4562\n",
      "Epoch 18 Batch 5200 Loss 1.8095 Accuracy 0.4562\n",
      "Epoch 18 Batch 5250 Loss 1.8096 Accuracy 0.4562\n",
      "Epoch 18 Batch 5300 Loss 1.8096 Accuracy 0.4562\n",
      "Epoch 18 Batch 5350 Loss 1.8096 Accuracy 0.4561\n",
      "Epoch 18 Batch 5400 Loss 1.8095 Accuracy 0.4561\n",
      "Epoch 18 Batch 5450 Loss 1.8095 Accuracy 0.4561\n",
      "Epoch 18 Batch 5500 Loss 1.8091 Accuracy 0.4561\n",
      "Epoch 18 Batch 5550 Loss 1.8091 Accuracy 0.4561\n",
      "Epoch 18 Batch 5600 Loss 1.8091 Accuracy 0.4561\n",
      "Epoch 18 Batch 5650 Loss 1.8091 Accuracy 0.4561\n",
      "Epoch 18 Batch 5700 Loss 1.8088 Accuracy 0.4561\n",
      "Epoch 18 Batch 5750 Loss 1.8089 Accuracy 0.4561\n",
      "Epoch 18 Batch 5800 Loss 1.8088 Accuracy 0.4561\n",
      "Epoch 18 Batch 5850 Loss 1.8090 Accuracy 0.4561\n",
      "Epoch 18 Batch 5900 Loss 1.8092 Accuracy 0.4561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 5950 Loss 1.8089 Accuracy 0.4561\n",
      "Epoch 18 Batch 6000 Loss 1.8087 Accuracy 0.4562\n",
      "Epoch 18 Batch 6050 Loss 1.8086 Accuracy 0.4561\n",
      "Epoch 18 Batch 6100 Loss 1.8084 Accuracy 0.4561\n",
      "Epoch 18 Batch 6150 Loss 1.8087 Accuracy 0.4561\n",
      "Epoch 18 Batch 6200 Loss 1.8087 Accuracy 0.4562\n",
      "Epoch 18 Batch 6250 Loss 1.8086 Accuracy 0.4562\n",
      "Epoch 18 Batch 6300 Loss 1.8086 Accuracy 0.4562\n",
      "Epoch 18 Batch 6350 Loss 1.8083 Accuracy 0.4562\n",
      "Epoch 18 Batch 6400 Loss 1.8083 Accuracy 0.4562\n",
      "Epoch 18 Batch 6450 Loss 1.8082 Accuracy 0.4562\n",
      "Epoch 18 Batch 6500 Loss 1.8084 Accuracy 0.4562\n",
      "Epoch 18 Batch 6550 Loss 1.8082 Accuracy 0.4563\n",
      "Epoch 18 Batch 6600 Loss 1.8079 Accuracy 0.4563\n",
      "Epoch 18 Batch 6650 Loss 1.8079 Accuracy 0.4563\n",
      "Epoch 18 Batch 6700 Loss 1.8078 Accuracy 0.4563\n",
      "Epoch 18 Batch 6750 Loss 1.8077 Accuracy 0.4563\n",
      "Epoch 18 Batch 6800 Loss 1.8076 Accuracy 0.4563\n",
      "Epoch 18 Batch 6850 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 6900 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 6950 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 7000 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 7050 Loss 1.8075 Accuracy 0.4562\n",
      "Epoch 18 Batch 7100 Loss 1.8077 Accuracy 0.4562\n",
      "Epoch 18 Batch 7150 Loss 1.8078 Accuracy 0.4562\n",
      "Epoch 18 Batch 7200 Loss 1.8078 Accuracy 0.4562\n",
      "Epoch 18 Batch 7250 Loss 1.8077 Accuracy 0.4562\n",
      "Epoch 18 Batch 7300 Loss 1.8076 Accuracy 0.4562\n",
      "Epoch 18 Batch 7350 Loss 1.8078 Accuracy 0.4562\n",
      "Epoch 18 Batch 7400 Loss 1.8079 Accuracy 0.4562\n",
      "Epoch 18 Batch 7450 Loss 1.8078 Accuracy 0.4562\n",
      "Epoch 18 Batch 7500 Loss 1.8077 Accuracy 0.4563\n",
      "Epoch 18 Batch 7550 Loss 1.8078 Accuracy 0.4563\n",
      "Epoch 18 Batch 7600 Loss 1.8076 Accuracy 0.4563\n",
      "Epoch 18 Batch 7650 Loss 1.8078 Accuracy 0.4563\n",
      "Epoch 18 Batch 7700 Loss 1.8081 Accuracy 0.4563\n",
      "Epoch 18 Batch 7750 Loss 1.8080 Accuracy 0.4562\n",
      "Epoch 18 Batch 7800 Loss 1.8079 Accuracy 0.4563\n",
      "Epoch 18 Batch 7850 Loss 1.8079 Accuracy 0.4563\n",
      "Epoch 18 Batch 7900 Loss 1.8079 Accuracy 0.4563\n",
      "Epoch 18 Batch 7950 Loss 1.8077 Accuracy 0.4563\n",
      "Epoch 18 Batch 8000 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 8050 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 8100 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 8150 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8200 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8250 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8300 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 8350 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8400 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8450 Loss 1.8073 Accuracy 0.4564\n",
      "Epoch 18 Batch 8500 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 8550 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 8600 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 8650 Loss 1.8071 Accuracy 0.4563\n",
      "Epoch 18 Batch 8700 Loss 1.8073 Accuracy 0.4564\n",
      "Epoch 18 Batch 8750 Loss 1.8073 Accuracy 0.4564\n",
      "Epoch 18 Batch 8800 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 8850 Loss 1.8073 Accuracy 0.4564\n",
      "Epoch 18 Batch 8900 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 8950 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 9000 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 9050 Loss 1.8075 Accuracy 0.4563\n",
      "Epoch 18 Batch 9100 Loss 1.8076 Accuracy 0.4563\n",
      "Epoch 18 Batch 9150 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 9200 Loss 1.8074 Accuracy 0.4563\n",
      "Epoch 18 Batch 9250 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 9300 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 9350 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 9400 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 9450 Loss 1.8071 Accuracy 0.4564\n",
      "Epoch 18 Batch 9500 Loss 1.8070 Accuracy 0.4564\n",
      "Epoch 18 Batch 9550 Loss 1.8071 Accuracy 0.4564\n",
      "Epoch 18 Batch 9600 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 9650 Loss 1.8071 Accuracy 0.4564\n",
      "Epoch 18 Batch 9700 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 9750 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 9800 Loss 1.8073 Accuracy 0.4563\n",
      "Epoch 18 Batch 9850 Loss 1.8072 Accuracy 0.4563\n",
      "Epoch 18 Batch 9900 Loss 1.8072 Accuracy 0.4563\n",
      "Epoch 18 Batch 9950 Loss 1.8071 Accuracy 0.4563\n",
      "Epoch 18 Batch 10000 Loss 1.8070 Accuracy 0.4564\n",
      "Epoch 18 Batch 10050 Loss 1.8071 Accuracy 0.4563\n",
      "Epoch 18 Batch 10100 Loss 1.8072 Accuracy 0.4563\n",
      "Epoch 18 Batch 10150 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Batch 10200 Loss 1.8072 Accuracy 0.4564\n",
      "Epoch 18 Loss 1.8070 Accuracy 0.4564\n",
      "Time taken for 1 epoch: 431.6106460094452 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.8527 Accuracy 0.4786\n",
      "Epoch 19 Batch 50 Loss 1.7872 Accuracy 0.4564\n",
      "Epoch 19 Batch 100 Loss 1.8159 Accuracy 0.4558\n",
      "Epoch 19 Batch 150 Loss 1.8036 Accuracy 0.4575\n",
      "Epoch 19 Batch 200 Loss 1.7983 Accuracy 0.4579\n",
      "Epoch 19 Batch 250 Loss 1.7978 Accuracy 0.4578\n",
      "Epoch 19 Batch 300 Loss 1.7930 Accuracy 0.4582\n",
      "Epoch 19 Batch 350 Loss 1.7935 Accuracy 0.4584\n",
      "Epoch 19 Batch 400 Loss 1.7898 Accuracy 0.4584\n",
      "Epoch 19 Batch 450 Loss 1.7920 Accuracy 0.4580\n",
      "Epoch 19 Batch 500 Loss 1.7920 Accuracy 0.4582\n",
      "Epoch 19 Batch 550 Loss 1.7969 Accuracy 0.4580\n",
      "Epoch 19 Batch 600 Loss 1.7988 Accuracy 0.4575\n",
      "Epoch 19 Batch 650 Loss 1.8028 Accuracy 0.4572\n",
      "Epoch 19 Batch 700 Loss 1.8035 Accuracy 0.4573\n",
      "Epoch 19 Batch 750 Loss 1.8048 Accuracy 0.4571\n",
      "Epoch 19 Batch 800 Loss 1.8040 Accuracy 0.4569\n",
      "Epoch 19 Batch 850 Loss 1.8049 Accuracy 0.4569\n",
      "Epoch 19 Batch 900 Loss 1.8064 Accuracy 0.4569\n",
      "Epoch 19 Batch 950 Loss 1.8050 Accuracy 0.4570\n",
      "Epoch 19 Batch 1000 Loss 1.8049 Accuracy 0.4570\n",
      "Epoch 19 Batch 1050 Loss 1.8047 Accuracy 0.4570\n",
      "Epoch 19 Batch 1100 Loss 1.8046 Accuracy 0.4571\n",
      "Epoch 19 Batch 1150 Loss 1.8048 Accuracy 0.4570\n",
      "Epoch 19 Batch 1200 Loss 1.8050 Accuracy 0.4570\n",
      "Epoch 19 Batch 1250 Loss 1.8053 Accuracy 0.4569\n",
      "Epoch 19 Batch 1300 Loss 1.8044 Accuracy 0.4570\n",
      "Epoch 19 Batch 1350 Loss 1.8040 Accuracy 0.4570\n",
      "Epoch 19 Batch 1400 Loss 1.8034 Accuracy 0.4570\n",
      "Epoch 19 Batch 1450 Loss 1.8025 Accuracy 0.4569\n",
      "Epoch 19 Batch 1500 Loss 1.8010 Accuracy 0.4570\n",
      "Epoch 19 Batch 1550 Loss 1.8001 Accuracy 0.4571\n",
      "Epoch 19 Batch 1600 Loss 1.8012 Accuracy 0.4570\n",
      "Epoch 19 Batch 1650 Loss 1.8007 Accuracy 0.4572\n",
      "Epoch 19 Batch 1700 Loss 1.7993 Accuracy 0.4573\n",
      "Epoch 19 Batch 1750 Loss 1.7979 Accuracy 0.4573\n",
      "Epoch 19 Batch 1800 Loss 1.7978 Accuracy 0.4574\n",
      "Epoch 19 Batch 1850 Loss 1.7985 Accuracy 0.4573\n",
      "Epoch 19 Batch 1900 Loss 1.7990 Accuracy 0.4572\n",
      "Epoch 19 Batch 1950 Loss 1.7992 Accuracy 0.4572\n",
      "Epoch 19 Batch 2000 Loss 1.7992 Accuracy 0.4572\n",
      "Epoch 19 Batch 2050 Loss 1.7986 Accuracy 0.4572\n",
      "Epoch 19 Batch 2100 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 2150 Loss 1.7976 Accuracy 0.4573\n",
      "Epoch 19 Batch 2200 Loss 1.7975 Accuracy 0.4573\n",
      "Epoch 19 Batch 2250 Loss 1.7976 Accuracy 0.4573\n",
      "Epoch 19 Batch 2300 Loss 1.7980 Accuracy 0.4573\n",
      "Epoch 19 Batch 2350 Loss 1.7986 Accuracy 0.4573\n",
      "Epoch 19 Batch 2400 Loss 1.7993 Accuracy 0.4573\n",
      "Epoch 19 Batch 2450 Loss 1.7989 Accuracy 0.4574\n",
      "Epoch 19 Batch 2500 Loss 1.7991 Accuracy 0.4575\n",
      "Epoch 19 Batch 2550 Loss 1.7991 Accuracy 0.4575\n",
      "Epoch 19 Batch 2600 Loss 1.7982 Accuracy 0.4575\n",
      "Epoch 19 Batch 2650 Loss 1.7991 Accuracy 0.4575\n",
      "Epoch 19 Batch 2700 Loss 1.7989 Accuracy 0.4575\n",
      "Epoch 19 Batch 2750 Loss 1.7990 Accuracy 0.4575\n",
      "Epoch 19 Batch 2800 Loss 1.7988 Accuracy 0.4576\n",
      "Epoch 19 Batch 2850 Loss 1.7990 Accuracy 0.4575\n",
      "Epoch 19 Batch 2900 Loss 1.7991 Accuracy 0.4575\n",
      "Epoch 19 Batch 2950 Loss 1.7993 Accuracy 0.4574\n",
      "Epoch 19 Batch 3000 Loss 1.7994 Accuracy 0.4574\n",
      "Epoch 19 Batch 3050 Loss 1.7998 Accuracy 0.4574\n",
      "Epoch 19 Batch 3100 Loss 1.8002 Accuracy 0.4574\n",
      "Epoch 19 Batch 3150 Loss 1.8006 Accuracy 0.4574\n",
      "Epoch 19 Batch 3200 Loss 1.8005 Accuracy 0.4574\n",
      "Epoch 19 Batch 3250 Loss 1.8008 Accuracy 0.4574\n",
      "Epoch 19 Batch 3300 Loss 1.8006 Accuracy 0.4574\n",
      "Epoch 19 Batch 3350 Loss 1.8003 Accuracy 0.4574\n",
      "Epoch 19 Batch 3400 Loss 1.8004 Accuracy 0.4573\n",
      "Epoch 19 Batch 3450 Loss 1.7999 Accuracy 0.4573\n",
      "Epoch 19 Batch 3500 Loss 1.7996 Accuracy 0.4574\n",
      "Epoch 19 Batch 3550 Loss 1.7998 Accuracy 0.4574\n",
      "Epoch 19 Batch 3600 Loss 1.7996 Accuracy 0.4574\n",
      "Epoch 19 Batch 3650 Loss 1.7993 Accuracy 0.4574\n",
      "Epoch 19 Batch 3700 Loss 1.7990 Accuracy 0.4575\n",
      "Epoch 19 Batch 3750 Loss 1.7994 Accuracy 0.4574\n",
      "Epoch 19 Batch 3800 Loss 1.7996 Accuracy 0.4574\n",
      "Epoch 19 Batch 3850 Loss 1.7996 Accuracy 0.4574\n",
      "Epoch 19 Batch 3900 Loss 1.7996 Accuracy 0.4573\n",
      "Epoch 19 Batch 3950 Loss 1.7998 Accuracy 0.4573\n",
      "Epoch 19 Batch 4000 Loss 1.8000 Accuracy 0.4572\n",
      "Epoch 19 Batch 4050 Loss 1.8004 Accuracy 0.4572\n",
      "Epoch 19 Batch 4100 Loss 1.8005 Accuracy 0.4571\n",
      "Epoch 19 Batch 4150 Loss 1.8004 Accuracy 0.4572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 4200 Loss 1.8007 Accuracy 0.4571\n",
      "Epoch 19 Batch 4250 Loss 1.8004 Accuracy 0.4571\n",
      "Epoch 19 Batch 4300 Loss 1.8003 Accuracy 0.4571\n",
      "Epoch 19 Batch 4350 Loss 1.8001 Accuracy 0.4571\n",
      "Epoch 19 Batch 4400 Loss 1.8003 Accuracy 0.4571\n",
      "Epoch 19 Batch 4450 Loss 1.8005 Accuracy 0.4571\n",
      "Epoch 19 Batch 4500 Loss 1.8008 Accuracy 0.4570\n",
      "Epoch 19 Batch 4550 Loss 1.8007 Accuracy 0.4570\n",
      "Epoch 19 Batch 4600 Loss 1.8007 Accuracy 0.4570\n",
      "Epoch 19 Batch 4650 Loss 1.8007 Accuracy 0.4571\n",
      "Epoch 19 Batch 4700 Loss 1.8006 Accuracy 0.4571\n",
      "Epoch 19 Batch 4750 Loss 1.8007 Accuracy 0.4571\n",
      "Epoch 19 Batch 4800 Loss 1.8007 Accuracy 0.4571\n",
      "Epoch 19 Batch 4850 Loss 1.8008 Accuracy 0.4571\n",
      "Epoch 19 Batch 4900 Loss 1.8008 Accuracy 0.4571\n",
      "Epoch 19 Batch 4950 Loss 1.8010 Accuracy 0.4571\n",
      "Epoch 19 Batch 5000 Loss 1.8008 Accuracy 0.4571\n",
      "Epoch 19 Batch 5050 Loss 1.8010 Accuracy 0.4571\n",
      "Epoch 19 Batch 5100 Loss 1.8010 Accuracy 0.4571\n",
      "Epoch 19 Batch 5150 Loss 1.8008 Accuracy 0.4571\n",
      "Epoch 19 Batch 5200 Loss 1.8009 Accuracy 0.4571\n",
      "Epoch 19 Batch 5250 Loss 1.8007 Accuracy 0.4571\n",
      "Epoch 19 Batch 5300 Loss 1.8005 Accuracy 0.4571\n",
      "Epoch 19 Batch 5350 Loss 1.8006 Accuracy 0.4570\n",
      "Epoch 19 Batch 5400 Loss 1.8008 Accuracy 0.4570\n",
      "Epoch 19 Batch 5450 Loss 1.8009 Accuracy 0.4570\n",
      "Epoch 19 Batch 5500 Loss 1.8007 Accuracy 0.4569\n",
      "Epoch 19 Batch 5550 Loss 1.8007 Accuracy 0.4570\n",
      "Epoch 19 Batch 5600 Loss 1.8002 Accuracy 0.4570\n",
      "Epoch 19 Batch 5650 Loss 1.8001 Accuracy 0.4570\n",
      "Epoch 19 Batch 5700 Loss 1.7997 Accuracy 0.4571\n",
      "Epoch 19 Batch 5750 Loss 1.7997 Accuracy 0.4571\n",
      "Epoch 19 Batch 5800 Loss 1.7997 Accuracy 0.4571\n",
      "Epoch 19 Batch 5850 Loss 1.7996 Accuracy 0.4571\n",
      "Epoch 19 Batch 5900 Loss 1.7996 Accuracy 0.4571\n",
      "Epoch 19 Batch 5950 Loss 1.7992 Accuracy 0.4571\n",
      "Epoch 19 Batch 6000 Loss 1.7993 Accuracy 0.4571\n",
      "Epoch 19 Batch 6050 Loss 1.7991 Accuracy 0.4572\n",
      "Epoch 19 Batch 6100 Loss 1.7992 Accuracy 0.4572\n",
      "Epoch 19 Batch 6150 Loss 1.7994 Accuracy 0.4571\n",
      "Epoch 19 Batch 6200 Loss 1.7992 Accuracy 0.4571\n",
      "Epoch 19 Batch 6250 Loss 1.7994 Accuracy 0.4571\n",
      "Epoch 19 Batch 6300 Loss 1.7992 Accuracy 0.4572\n",
      "Epoch 19 Batch 6350 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 6400 Loss 1.7993 Accuracy 0.4571\n",
      "Epoch 19 Batch 6450 Loss 1.7995 Accuracy 0.4571\n",
      "Epoch 19 Batch 6500 Loss 1.7999 Accuracy 0.4571\n",
      "Epoch 19 Batch 6550 Loss 1.7997 Accuracy 0.4571\n",
      "Epoch 19 Batch 6600 Loss 1.7996 Accuracy 0.4571\n",
      "Epoch 19 Batch 6650 Loss 1.7998 Accuracy 0.4571\n",
      "Epoch 19 Batch 6700 Loss 1.7995 Accuracy 0.4571\n",
      "Epoch 19 Batch 6750 Loss 1.7994 Accuracy 0.4571\n",
      "Epoch 19 Batch 6800 Loss 1.7995 Accuracy 0.4571\n",
      "Epoch 19 Batch 6850 Loss 1.7995 Accuracy 0.4571\n",
      "Epoch 19 Batch 6900 Loss 1.7995 Accuracy 0.4571\n",
      "Epoch 19 Batch 6950 Loss 1.7994 Accuracy 0.4571\n",
      "Epoch 19 Batch 7000 Loss 1.7993 Accuracy 0.4571\n",
      "Epoch 19 Batch 7050 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 7100 Loss 1.7989 Accuracy 0.4571\n",
      "Epoch 19 Batch 7150 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 7200 Loss 1.7989 Accuracy 0.4571\n",
      "Epoch 19 Batch 7250 Loss 1.7987 Accuracy 0.4572\n",
      "Epoch 19 Batch 7300 Loss 1.7990 Accuracy 0.4571\n",
      "Epoch 19 Batch 7350 Loss 1.7990 Accuracy 0.4571\n",
      "Epoch 19 Batch 7400 Loss 1.7988 Accuracy 0.4571\n",
      "Epoch 19 Batch 7450 Loss 1.7990 Accuracy 0.4571\n",
      "Epoch 19 Batch 7500 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 7550 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 7600 Loss 1.7992 Accuracy 0.4572\n",
      "Epoch 19 Batch 7650 Loss 1.7992 Accuracy 0.4571\n",
      "Epoch 19 Batch 7700 Loss 1.7991 Accuracy 0.4571\n",
      "Epoch 19 Batch 7750 Loss 1.7992 Accuracy 0.4571\n",
      "Epoch 19 Batch 7800 Loss 1.7990 Accuracy 0.4571\n",
      "Epoch 19 Batch 7850 Loss 1.7987 Accuracy 0.4572\n",
      "Epoch 19 Batch 7900 Loss 1.7987 Accuracy 0.4572\n",
      "Epoch 19 Batch 7950 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 8000 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 8050 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 8100 Loss 1.7981 Accuracy 0.4573\n",
      "Epoch 19 Batch 8150 Loss 1.7981 Accuracy 0.4572\n",
      "Epoch 19 Batch 8200 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 8250 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 8300 Loss 1.7981 Accuracy 0.4573\n",
      "Epoch 19 Batch 8350 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 8400 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 8450 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 8500 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 8550 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 8600 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 8650 Loss 1.7985 Accuracy 0.4572\n",
      "Epoch 19 Batch 8700 Loss 1.7983 Accuracy 0.4573\n",
      "Epoch 19 Batch 8750 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 8800 Loss 1.7984 Accuracy 0.4573\n",
      "Epoch 19 Batch 8850 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 8900 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 8950 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 9000 Loss 1.7985 Accuracy 0.4572\n",
      "Epoch 19 Batch 9050 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9100 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9150 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9200 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9250 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 9300 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9350 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9400 Loss 1.7983 Accuracy 0.4573\n",
      "Epoch 19 Batch 9450 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 9500 Loss 1.7982 Accuracy 0.4573\n",
      "Epoch 19 Batch 9550 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 9600 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Batch 9650 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 9700 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 9750 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 9800 Loss 1.7985 Accuracy 0.4572\n",
      "Epoch 19 Batch 9850 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 9900 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 9950 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 10000 Loss 1.7984 Accuracy 0.4572\n",
      "Epoch 19 Batch 10050 Loss 1.7985 Accuracy 0.4572\n",
      "Epoch 19 Batch 10100 Loss 1.7985 Accuracy 0.4572\n",
      "Epoch 19 Batch 10150 Loss 1.7983 Accuracy 0.4572\n",
      "Epoch 19 Batch 10200 Loss 1.7982 Accuracy 0.4572\n",
      "Epoch 19 Loss 1.7983 Accuracy 0.4572\n",
      "Time taken for 1 epoch: 431.745641708374 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.7474 Accuracy 0.4753\n",
      "Epoch 20 Batch 50 Loss 1.8043 Accuracy 0.4575\n",
      "Epoch 20 Batch 100 Loss 1.7834 Accuracy 0.4602\n",
      "Epoch 20 Batch 150 Loss 1.7856 Accuracy 0.4601\n",
      "Epoch 20 Batch 200 Loss 1.7876 Accuracy 0.4600\n",
      "Epoch 20 Batch 250 Loss 1.7841 Accuracy 0.4591\n",
      "Epoch 20 Batch 300 Loss 1.7865 Accuracy 0.4585\n",
      "Epoch 20 Batch 350 Loss 1.7921 Accuracy 0.4584\n",
      "Epoch 20 Batch 400 Loss 1.7931 Accuracy 0.4583\n",
      "Epoch 20 Batch 450 Loss 1.7990 Accuracy 0.4581\n",
      "Epoch 20 Batch 500 Loss 1.7971 Accuracy 0.4584\n",
      "Epoch 20 Batch 550 Loss 1.7962 Accuracy 0.4583\n",
      "Epoch 20 Batch 600 Loss 1.7952 Accuracy 0.4583\n",
      "Epoch 20 Batch 650 Loss 1.7967 Accuracy 0.4581\n",
      "Epoch 20 Batch 700 Loss 1.7964 Accuracy 0.4579\n",
      "Epoch 20 Batch 750 Loss 1.7958 Accuracy 0.4579\n",
      "Epoch 20 Batch 800 Loss 1.7962 Accuracy 0.4576\n",
      "Epoch 20 Batch 850 Loss 1.7973 Accuracy 0.4576\n",
      "Epoch 20 Batch 900 Loss 1.7985 Accuracy 0.4574\n",
      "Epoch 20 Batch 950 Loss 1.7989 Accuracy 0.4572\n",
      "Epoch 20 Batch 1000 Loss 1.7983 Accuracy 0.4574\n",
      "Epoch 20 Batch 1050 Loss 1.7996 Accuracy 0.4570\n",
      "Epoch 20 Batch 1100 Loss 1.7997 Accuracy 0.4569\n",
      "Epoch 20 Batch 1150 Loss 1.8003 Accuracy 0.4568\n",
      "Epoch 20 Batch 1200 Loss 1.7989 Accuracy 0.4571\n",
      "Epoch 20 Batch 1250 Loss 1.7980 Accuracy 0.4572\n",
      "Epoch 20 Batch 1300 Loss 1.7985 Accuracy 0.4571\n",
      "Epoch 20 Batch 1350 Loss 1.7972 Accuracy 0.4572\n",
      "Epoch 20 Batch 1400 Loss 1.7964 Accuracy 0.4574\n",
      "Epoch 20 Batch 1450 Loss 1.7966 Accuracy 0.4573\n",
      "Epoch 20 Batch 1500 Loss 1.7954 Accuracy 0.4576\n",
      "Epoch 20 Batch 1550 Loss 1.7948 Accuracy 0.4575\n",
      "Epoch 20 Batch 1600 Loss 1.7945 Accuracy 0.4575\n",
      "Epoch 20 Batch 1650 Loss 1.7939 Accuracy 0.4577\n",
      "Epoch 20 Batch 1700 Loss 1.7933 Accuracy 0.4577\n",
      "Epoch 20 Batch 1750 Loss 1.7932 Accuracy 0.4577\n",
      "Epoch 20 Batch 1800 Loss 1.7927 Accuracy 0.4578\n",
      "Epoch 20 Batch 1850 Loss 1.7928 Accuracy 0.4578\n",
      "Epoch 20 Batch 1900 Loss 1.7927 Accuracy 0.4579\n",
      "Epoch 20 Batch 1950 Loss 1.7923 Accuracy 0.4578\n",
      "Epoch 20 Batch 2000 Loss 1.7924 Accuracy 0.4578\n",
      "Epoch 20 Batch 2050 Loss 1.7929 Accuracy 0.4578\n",
      "Epoch 20 Batch 2100 Loss 1.7925 Accuracy 0.4578\n",
      "Epoch 20 Batch 2150 Loss 1.7920 Accuracy 0.4579\n",
      "Epoch 20 Batch 2200 Loss 1.7922 Accuracy 0.4579\n",
      "Epoch 20 Batch 2250 Loss 1.7920 Accuracy 0.4579\n",
      "Epoch 20 Batch 2300 Loss 1.7927 Accuracy 0.4579\n",
      "Epoch 20 Batch 2350 Loss 1.7920 Accuracy 0.4580\n",
      "Epoch 20 Batch 2400 Loss 1.7915 Accuracy 0.4580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 2450 Loss 1.7918 Accuracy 0.4580\n",
      "Epoch 20 Batch 2500 Loss 1.7913 Accuracy 0.4581\n",
      "Epoch 20 Batch 2550 Loss 1.7923 Accuracy 0.4580\n",
      "Epoch 20 Batch 2600 Loss 1.7922 Accuracy 0.4580\n",
      "Epoch 20 Batch 2650 Loss 1.7920 Accuracy 0.4580\n",
      "Epoch 20 Batch 2700 Loss 1.7920 Accuracy 0.4580\n",
      "Epoch 20 Batch 2750 Loss 1.7921 Accuracy 0.4580\n",
      "Epoch 20 Batch 2800 Loss 1.7926 Accuracy 0.4580\n",
      "Epoch 20 Batch 2850 Loss 1.7928 Accuracy 0.4579\n",
      "Epoch 20 Batch 2900 Loss 1.7930 Accuracy 0.4579\n",
      "Epoch 20 Batch 2950 Loss 1.7929 Accuracy 0.4580\n",
      "Epoch 20 Batch 3000 Loss 1.7928 Accuracy 0.4580\n",
      "Epoch 20 Batch 3050 Loss 1.7929 Accuracy 0.4580\n",
      "Epoch 20 Batch 3100 Loss 1.7934 Accuracy 0.4580\n",
      "Epoch 20 Batch 3150 Loss 1.7934 Accuracy 0.4579\n",
      "Epoch 20 Batch 3200 Loss 1.7935 Accuracy 0.4579\n",
      "Epoch 20 Batch 3250 Loss 1.7937 Accuracy 0.4579\n",
      "Epoch 20 Batch 3300 Loss 1.7937 Accuracy 0.4579\n",
      "Epoch 20 Batch 3350 Loss 1.7938 Accuracy 0.4579\n",
      "Epoch 20 Batch 3400 Loss 1.7934 Accuracy 0.4580\n",
      "Epoch 20 Batch 3450 Loss 1.7934 Accuracy 0.4580\n",
      "Epoch 20 Batch 3500 Loss 1.7932 Accuracy 0.4580\n",
      "Epoch 20 Batch 3550 Loss 1.7932 Accuracy 0.4579\n",
      "Epoch 20 Batch 3600 Loss 1.7934 Accuracy 0.4579\n",
      "Epoch 20 Batch 3650 Loss 1.7935 Accuracy 0.4579\n",
      "Epoch 20 Batch 3700 Loss 1.7932 Accuracy 0.4579\n",
      "Epoch 20 Batch 3750 Loss 1.7932 Accuracy 0.4579\n",
      "Epoch 20 Batch 3800 Loss 1.7927 Accuracy 0.4579\n",
      "Epoch 20 Batch 3850 Loss 1.7926 Accuracy 0.4579\n",
      "Epoch 20 Batch 3900 Loss 1.7923 Accuracy 0.4578\n",
      "Epoch 20 Batch 3950 Loss 1.7925 Accuracy 0.4578\n",
      "Epoch 20 Batch 4000 Loss 1.7926 Accuracy 0.4578\n",
      "Epoch 20 Batch 4050 Loss 1.7928 Accuracy 0.4578\n",
      "Epoch 20 Batch 4100 Loss 1.7930 Accuracy 0.4578\n",
      "Epoch 20 Batch 4150 Loss 1.7932 Accuracy 0.4578\n",
      "Epoch 20 Batch 4200 Loss 1.7931 Accuracy 0.4578\n",
      "Epoch 20 Batch 4250 Loss 1.7931 Accuracy 0.4578\n",
      "Epoch 20 Batch 4300 Loss 1.7933 Accuracy 0.4579\n",
      "Epoch 20 Batch 4350 Loss 1.7937 Accuracy 0.4578\n",
      "Epoch 20 Batch 4400 Loss 1.7939 Accuracy 0.4578\n",
      "Epoch 20 Batch 4450 Loss 1.7939 Accuracy 0.4578\n",
      "Epoch 20 Batch 4500 Loss 1.7940 Accuracy 0.4577\n",
      "Epoch 20 Batch 4550 Loss 1.7937 Accuracy 0.4578\n",
      "Epoch 20 Batch 4600 Loss 1.7936 Accuracy 0.4578\n",
      "Epoch 20 Batch 4650 Loss 1.7936 Accuracy 0.4578\n",
      "Epoch 20 Batch 4700 Loss 1.7933 Accuracy 0.4578\n",
      "Epoch 20 Batch 4750 Loss 1.7935 Accuracy 0.4577\n",
      "Epoch 20 Batch 4800 Loss 1.7934 Accuracy 0.4578\n",
      "Epoch 20 Batch 4850 Loss 1.7935 Accuracy 0.4578\n",
      "Epoch 20 Batch 4900 Loss 1.7939 Accuracy 0.4577\n",
      "Epoch 20 Batch 4950 Loss 1.7939 Accuracy 0.4577\n",
      "Epoch 20 Batch 5000 Loss 1.7937 Accuracy 0.4577\n",
      "Epoch 20 Batch 5050 Loss 1.7937 Accuracy 0.4577\n",
      "Epoch 20 Batch 5100 Loss 1.7938 Accuracy 0.4577\n",
      "Epoch 20 Batch 5150 Loss 1.7937 Accuracy 0.4577\n",
      "Epoch 20 Batch 5200 Loss 1.7936 Accuracy 0.4577\n",
      "Epoch 20 Batch 5250 Loss 1.7936 Accuracy 0.4576\n",
      "Epoch 20 Batch 5300 Loss 1.7933 Accuracy 0.4576\n",
      "Epoch 20 Batch 5350 Loss 1.7931 Accuracy 0.4576\n",
      "Epoch 20 Batch 5400 Loss 1.7932 Accuracy 0.4576\n",
      "Epoch 20 Batch 5450 Loss 1.7935 Accuracy 0.4576\n",
      "Epoch 20 Batch 5500 Loss 1.7932 Accuracy 0.4576\n",
      "Epoch 20 Batch 5550 Loss 1.7930 Accuracy 0.4576\n",
      "Epoch 20 Batch 5600 Loss 1.7927 Accuracy 0.4576\n",
      "Epoch 20 Batch 5650 Loss 1.7931 Accuracy 0.4576\n",
      "Epoch 20 Batch 5700 Loss 1.7927 Accuracy 0.4576\n",
      "Epoch 20 Batch 5750 Loss 1.7925 Accuracy 0.4577\n",
      "Epoch 20 Batch 5800 Loss 1.7925 Accuracy 0.4577\n",
      "Epoch 20 Batch 5850 Loss 1.7927 Accuracy 0.4577\n",
      "Epoch 20 Batch 5900 Loss 1.7928 Accuracy 0.4576\n",
      "Epoch 20 Batch 5950 Loss 1.7926 Accuracy 0.4576\n",
      "Epoch 20 Batch 6000 Loss 1.7927 Accuracy 0.4576\n",
      "Epoch 20 Batch 6050 Loss 1.7925 Accuracy 0.4576\n",
      "Epoch 20 Batch 6100 Loss 1.7928 Accuracy 0.4576\n",
      "Epoch 20 Batch 6150 Loss 1.7929 Accuracy 0.4576\n",
      "Epoch 20 Batch 6200 Loss 1.7930 Accuracy 0.4576\n",
      "Epoch 20 Batch 6250 Loss 1.7930 Accuracy 0.4576\n",
      "Epoch 20 Batch 6300 Loss 1.7929 Accuracy 0.4576\n",
      "Epoch 20 Batch 6350 Loss 1.7929 Accuracy 0.4577\n",
      "Epoch 20 Batch 6400 Loss 1.7926 Accuracy 0.4576\n",
      "Epoch 20 Batch 6450 Loss 1.7926 Accuracy 0.4576\n",
      "Epoch 20 Batch 6500 Loss 1.7926 Accuracy 0.4576\n",
      "Epoch 20 Batch 6550 Loss 1.7925 Accuracy 0.4576\n",
      "Epoch 20 Batch 6600 Loss 1.7924 Accuracy 0.4577\n",
      "Epoch 20 Batch 6650 Loss 1.7924 Accuracy 0.4576\n",
      "Epoch 20 Batch 6700 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 6750 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 6800 Loss 1.7922 Accuracy 0.4576\n",
      "Epoch 20 Batch 6850 Loss 1.7922 Accuracy 0.4576\n",
      "Epoch 20 Batch 6900 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 6950 Loss 1.7923 Accuracy 0.4577\n",
      "Epoch 20 Batch 7000 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 7050 Loss 1.7920 Accuracy 0.4577\n",
      "Epoch 20 Batch 7100 Loss 1.7918 Accuracy 0.4577\n",
      "Epoch 20 Batch 7150 Loss 1.7920 Accuracy 0.4577\n",
      "Epoch 20 Batch 7200 Loss 1.7917 Accuracy 0.4577\n",
      "Epoch 20 Batch 7250 Loss 1.7918 Accuracy 0.4577\n",
      "Epoch 20 Batch 7300 Loss 1.7919 Accuracy 0.4577\n",
      "Epoch 20 Batch 7350 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 7400 Loss 1.7920 Accuracy 0.4577\n",
      "Epoch 20 Batch 7450 Loss 1.7920 Accuracy 0.4577\n",
      "Epoch 20 Batch 7500 Loss 1.7921 Accuracy 0.4577\n",
      "Epoch 20 Batch 7550 Loss 1.7922 Accuracy 0.4577\n",
      "Epoch 20 Batch 7600 Loss 1.7925 Accuracy 0.4577\n",
      "Epoch 20 Batch 7650 Loss 1.7924 Accuracy 0.4578\n",
      "Epoch 20 Batch 7700 Loss 1.7923 Accuracy 0.4578\n",
      "Epoch 20 Batch 7750 Loss 1.7923 Accuracy 0.4578\n",
      "Epoch 20 Batch 7800 Loss 1.7922 Accuracy 0.4578\n",
      "Epoch 20 Batch 7850 Loss 1.7921 Accuracy 0.4578\n",
      "Epoch 20 Batch 7900 Loss 1.7920 Accuracy 0.4578\n",
      "Epoch 20 Batch 7950 Loss 1.7920 Accuracy 0.4578\n",
      "Epoch 20 Batch 8000 Loss 1.7919 Accuracy 0.4578\n",
      "Epoch 20 Batch 8050 Loss 1.7919 Accuracy 0.4578\n",
      "Epoch 20 Batch 8100 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 8150 Loss 1.7915 Accuracy 0.4578\n",
      "Epoch 20 Batch 8200 Loss 1.7915 Accuracy 0.4578\n",
      "Epoch 20 Batch 8250 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 8300 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 8350 Loss 1.7914 Accuracy 0.4578\n",
      "Epoch 20 Batch 8400 Loss 1.7914 Accuracy 0.4578\n",
      "Epoch 20 Batch 8450 Loss 1.7914 Accuracy 0.4578\n",
      "Epoch 20 Batch 8500 Loss 1.7912 Accuracy 0.4579\n",
      "Epoch 20 Batch 8550 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 8600 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 8650 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 8700 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 8750 Loss 1.7912 Accuracy 0.4579\n",
      "Epoch 20 Batch 8800 Loss 1.7912 Accuracy 0.4579\n",
      "Epoch 20 Batch 8850 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 8900 Loss 1.7914 Accuracy 0.4579\n",
      "Epoch 20 Batch 8950 Loss 1.7916 Accuracy 0.4579\n",
      "Epoch 20 Batch 9000 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 9050 Loss 1.7919 Accuracy 0.4578\n",
      "Epoch 20 Batch 9100 Loss 1.7918 Accuracy 0.4578\n",
      "Epoch 20 Batch 9150 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 9200 Loss 1.7915 Accuracy 0.4578\n",
      "Epoch 20 Batch 9250 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 9300 Loss 1.7916 Accuracy 0.4578\n",
      "Epoch 20 Batch 9350 Loss 1.7918 Accuracy 0.4578\n",
      "Epoch 20 Batch 9400 Loss 1.7917 Accuracy 0.4578\n",
      "Epoch 20 Batch 9450 Loss 1.7918 Accuracy 0.4578\n",
      "Epoch 20 Batch 9500 Loss 1.7919 Accuracy 0.4578\n",
      "Epoch 20 Batch 9550 Loss 1.7920 Accuracy 0.4578\n",
      "Epoch 20 Batch 9600 Loss 1.7919 Accuracy 0.4578\n",
      "Epoch 20 Batch 9650 Loss 1.7917 Accuracy 0.4578\n",
      "Epoch 20 Batch 9700 Loss 1.7917 Accuracy 0.4578\n",
      "Epoch 20 Batch 9750 Loss 1.7915 Accuracy 0.4579\n",
      "Epoch 20 Batch 9800 Loss 1.7915 Accuracy 0.4579\n",
      "Epoch 20 Batch 9850 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 9900 Loss 1.7913 Accuracy 0.4579\n",
      "Epoch 20 Batch 9950 Loss 1.7911 Accuracy 0.4579\n",
      "Epoch 20 Batch 10000 Loss 1.7910 Accuracy 0.4579\n",
      "Epoch 20 Batch 10050 Loss 1.7910 Accuracy 0.4579\n",
      "Epoch 20 Batch 10100 Loss 1.7910 Accuracy 0.4579\n",
      "Epoch 20 Batch 10150 Loss 1.7911 Accuracy 0.4579\n",
      "Epoch 20 Batch 10200 Loss 1.7911 Accuracy 0.4579\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Epoch 20 Loss 1.7911 Accuracy 0.4579\n",
      "Time taken for 1 epoch: 431.90456533432007 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  batch = 0\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  k = 4\n",
    "  start_token = [tokenizer_de.vocab_size]\n",
    "  end_token = [tokenizer_de.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_de.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  encoder_input_k = tf.tile(encoder_input, multiples=[k, 1])\n",
    "#  print(encoder_input)\n",
    "#  print(encoder_input_k)\n",
    "  assert(tf.reduce_all(tf.math.equal(encoder_input[0], encoder_input_k[1])))\n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    \n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "  output_k = tf.tile(output, multiples=[k, 1])\n",
    "\n",
    "  output_cost = tf.zeros((k,1))\n",
    "  \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input_k, output_k)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input_k, \n",
    "                                                 output_k,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "    #print(predictions)\n",
    "    log_probs=tf.math.log(tf.nn.softmax(predictions))\n",
    "       \n",
    "    \n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    (values, indices) = tf.math.top_k(log_probs, k=k)\n",
    "    print(values)\n",
    "    print(indices)\n",
    "    print(tf.transpose(indices[0]))\n",
    "    print(predicted_id)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    # COMMENTED OUT, since we can't easily do this early stopping while refactoring for k best\n",
    "    #if predicted_id == tokenizer_en.vocab_size+1:\n",
    "    #  return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    # hack to make it work\n",
    "    output_k = tf.concat([output_k, tf.transpose(indices[0])], axis=-1)\n",
    "\n",
    "  #print(output)\n",
    "  print(output_k)\n",
    "  return output_k[0], attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_de.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_de.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.20258223 -4.1952353  -4.3418875  -4.452239  ]]\n",
      "\n",
      " [[-0.20258223 -4.1952353  -4.3418875  -4.452239  ]]\n",
      "\n",
      " [[-0.20258223 -4.1952353  -4.3418875  -4.452239  ]]\n",
      "\n",
      " [[-0.20258223 -4.1952353  -4.3418875  -4.452239  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[6394  123  254   60]]\n",
      "\n",
      " [[6394  123  254   60]]\n",
      "\n",
      " [[6394  123  254   60]]\n",
      "\n",
      " [[6394  123  254   60]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[6394]\n",
      " [ 123]\n",
      " [ 254]\n",
      " [  60]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[6394]\n",
      " [6394]\n",
      " [6394]\n",
      " [6394]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-0.00209766 -7.5707088  -7.593559   -8.724556  ]]\n",
      "\n",
      " [[-0.07401307 -3.933499   -4.044572   -5.694874  ]]\n",
      "\n",
      " [[-0.12349532 -3.033398   -4.159415   -5.4839535 ]]\n",
      "\n",
      " [[-0.4275606  -1.6971195  -3.0823247  -3.342327  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7957    2   47 7971]]\n",
      "\n",
      " [[ 185  229   69  367]]\n",
      "\n",
      " [[   9   43 6018  185]]\n",
      "\n",
      " [[   9 6018 6916   43]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7957]\n",
      " [   2]\n",
      " [  47]\n",
      " [7971]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7957]\n",
      " [ 185]\n",
      " [   9]\n",
      " [   9]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -1.0993569   -2.3721902   -2.9252307   -3.177351  ]]\n",
      "\n",
      " [[ -0.38400483  -2.890711    -4.007666    -4.118712  ]]\n",
      "\n",
      " [[ -0.3130298   -3.3125548   -3.7975488   -4.2397265 ]]\n",
      "\n",
      " [[ -0.00081889  -9.210499   -10.215627   -10.267471  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[6018 6775 6916  185]]\n",
      "\n",
      " [[ 185   69    7  648]]\n",
      "\n",
      " [[   9   43 6251  214]]\n",
      "\n",
      " [[8182  361   14 7958]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[6018]\n",
      " [6775]\n",
      " [6916]\n",
      " [ 185]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[6018]\n",
      " [ 185]\n",
      " [   9]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-1.4467442  -2.7928803  -2.9977868  -3.286751  ]]\n",
      "\n",
      " [[-1.4782518  -1.6599268  -2.36856    -3.222652  ]]\n",
      "\n",
      " [[-0.15729089 -2.392425   -3.606296   -4.8796535 ]]\n",
      "\n",
      " [[-2.3504868  -2.851525   -3.4641104  -3.4925835 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1156  346    4 2311]]\n",
      "\n",
      " [[   4    9    3    6]]\n",
      "\n",
      " [[7971    2    8 7958]]\n",
      "\n",
      " [[6018 6775  648 7826]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1156]\n",
      " [ 346]\n",
      " [   4]\n",
      " [2311]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1156]\n",
      " [   4]\n",
      " [7971]\n",
      " [6018]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-0.7130925  -1.6965176  -2.922878   -2.941238  ]]\n",
      "\n",
      " [[-0.36258122 -1.301939   -4.847616   -5.094982  ]]\n",
      "\n",
      " [[-2.4518569  -2.4921474  -2.7984374  -3.1909544 ]]\n",
      "\n",
      " [[-1.5601035  -2.2440214  -2.577634   -2.9758275 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 405 2285 4543 1309]]\n",
      "\n",
      " [[7971    2 7958   37]]\n",
      "\n",
      " [[1156    1  280  346]]\n",
      "\n",
      " [[   9   35    6   43]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 405]\n",
      " [2285]\n",
      " [4543]\n",
      " [1309]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 405]\n",
      " [7971]\n",
      " [1156]\n",
      " [   9]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-0.01130377 -5.3985033  -5.723588   -7.4930315 ]]\n",
      "\n",
      " [[-0.11524666 -2.3600435  -5.1087847  -6.218335  ]]\n",
      "\n",
      " [[-0.06341687 -3.2161572  -5.1470733  -5.9149356 ]]\n",
      "\n",
      " [[-0.43806273 -2.575365   -2.5908527  -3.7901096 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7971    2 7958 7964]]\n",
      "\n",
      " [[7971    2 7958   37]]\n",
      "\n",
      " [[7971    2 8040   37]]\n",
      "\n",
      " [[7971    2 4551  125]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [   2]\n",
      " [7958]\n",
      " [7964]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [7971]\n",
      " [7971]\n",
      " [7971]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.00002766 -11.365371   -12.8892565  -13.040093  ]]\n",
      "\n",
      " [[ -1.4026419   -2.3707664   -2.7073028   -3.1468666 ]]\n",
      "\n",
      " [[ -0.00040903  -9.381513    -9.490289    -9.745362  ]]\n",
      "\n",
      " [[ -0.00508988  -6.6990786   -6.7504435   -7.4726496 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182  361 7964 7966]]\n",
      "\n",
      " [[   9   11   18    5]]\n",
      "\n",
      " [[8182 7958 7971  361]]\n",
      "\n",
      " [[8182 7958 7971  361]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [ 361]\n",
      " [7964]\n",
      " [7966]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [   9]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-1.572696   -2.1208947  -2.6163054  -3.7512238 ]]\n",
      "\n",
      " [[-2.1523657  -3.170497   -3.9426966  -3.9991236 ]]\n",
      "\n",
      " [[-0.00235901 -6.97524    -7.6309733  -8.545404  ]]\n",
      "\n",
      " [[-0.02432615 -4.1544495  -5.207157   -7.099716  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7971 7957 8025  588]]\n",
      "\n",
      " [[ 361   88  601  494]]\n",
      "\n",
      " [[8182 7971 7958  361]]\n",
      "\n",
      " [[8182 7958 7971 7957]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [7957]\n",
      " [8025]\n",
      " [ 588]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [ 361]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.0000577  -10.691407   -11.64594    -12.050431  ]]\n",
      "\n",
      " [[ -2.257742    -2.32755     -2.330346    -2.76962   ]]\n",
      "\n",
      " [[ -0.06625037  -3.8019872   -4.232594    -4.7678494 ]]\n",
      "\n",
      " [[ -0.9445013   -2.2320018   -2.4093537   -3.12738   ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182  361 7964 7966]]\n",
      "\n",
      " [[   9  698 7988  346]]\n",
      "\n",
      " [[7971 7958    2 8182]]\n",
      "\n",
      " [[8045 3856 1949 1761]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [ 361]\n",
      " [7964]\n",
      " [7966]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [   9]\n",
      " [7971]\n",
      " [8045]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-0.99484533 -2.7029867  -2.982152   -3.9313116 ]]\n",
      "\n",
      " [[-2.6872616  -3.2515795  -3.2691996  -4.082449  ]]\n",
      "\n",
      " [[-0.00761817 -5.993111   -6.149594   -7.465693  ]]\n",
      "\n",
      " [[-0.01620829 -4.5461726  -5.5923343  -7.6520686 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7957 7971 8025    8]]\n",
      "\n",
      " [[ 361 6394 1400  730]]\n",
      "\n",
      " [[8182 7971 7958  361]]\n",
      "\n",
      " [[8182 7958 7971 7988]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7957]\n",
      " [7971]\n",
      " [8025]\n",
      " [   8]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7957]\n",
      " [ 361]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -2.6512291   -3.3516285   -3.3760865   -3.5618966 ]]\n",
      "\n",
      " [[ -0.00028143 -10.123298   -10.384982   -10.553993  ]]\n",
      "\n",
      " [[ -0.13524292  -3.0620542   -3.7574391   -4.1842084 ]]\n",
      "\n",
      " [[ -1.6930047   -2.747059    -3.0244715   -3.0932047 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1871  698 1435    6]]\n",
      "\n",
      " [[8182 8034  361 7964]]\n",
      "\n",
      " [[7971 7958    2 8182]]\n",
      "\n",
      " [[   5    6   12    7]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1871]\n",
      " [ 698]\n",
      " [1435]\n",
      " [   6]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1871]\n",
      " [8182]\n",
      " [7971]\n",
      " [   5]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-1.1352643 -2.1235454 -2.1927326 -2.9752932]]\n",
      "\n",
      " [[-0.5539368 -2.8010185 -3.7248237 -3.8780067]]\n",
      "\n",
      " [[-1.8597984 -1.9408952 -2.9506645 -2.974369 ]]\n",
      "\n",
      " [[-1.0352365 -1.7458861 -3.142254  -3.4410079]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1223  988  306 1290]]\n",
      "\n",
      " [[7971 8025 7958 6370]]\n",
      "\n",
      " [[5190 1387 2383  100]]\n",
      "\n",
      " [[   1   19  280    7]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1223]\n",
      " [ 988]\n",
      " [ 306]\n",
      " [1290]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1223]\n",
      " [7971]\n",
      " [5190]\n",
      " [   1]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-1.2268873  -2.6680303  -2.8112798  -3.1858528 ]]\n",
      "\n",
      " [[-0.70152575 -2.3215668  -2.9341002  -3.3164341 ]]\n",
      "\n",
      " [[-0.19663459 -3.132746   -3.921427   -4.8313622 ]]\n",
      "\n",
      " [[-0.05504331 -5.120616   -5.9169436  -6.411252  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7971  118  251   81]]\n",
      "\n",
      " [[ 483  217 8026  418]]\n",
      "\n",
      " [[7971 7958    2 8182]]\n",
      "\n",
      " [[8026 1784 8030 7971]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [ 118]\n",
      " [ 251]\n",
      " [  81]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [ 483]\n",
      " [7971]\n",
      " [8026]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.00007177 -10.754691   -11.316352   -11.615632  ]]\n",
      "\n",
      " [[ -0.17653854  -3.1539023   -4.115553    -4.309614  ]]\n",
      "\n",
      " [[ -0.25441164  -2.8563786   -3.0647373   -4.084032  ]]\n",
      "\n",
      " [[ -2.7145603   -3.2104914   -3.3505273   -3.4028308 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182  361 7966 7964]]\n",
      "\n",
      " [[7971 8182 7958 7964]]\n",
      "\n",
      " [[7971 8182 7958 8046]]\n",
      "\n",
      " [[ 596  802 3439  523]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [ 361]\n",
      " [7966]\n",
      " [7964]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [7971]\n",
      " [7971]\n",
      " [ 596]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-1.6561437  -1.8623776  -2.6622093  -3.1299634 ]]\n",
      "\n",
      " [[-0.02199736 -4.153534   -8.133785   -8.657976  ]]\n",
      "\n",
      " [[-0.01854905 -4.541731   -5.0178547  -7.797121  ]]\n",
      "\n",
      " [[-0.00480463 -6.08572    -6.410736   -9.110876  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8025 7971 7957 8040]]\n",
      "\n",
      " [[8182  361 7958 8017]]\n",
      "\n",
      " [[8182 7958 7971 7988]]\n",
      "\n",
      " [[8182 7958 7971  361]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8025]\n",
      " [7971]\n",
      " [7957]\n",
      " [8040]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8025]\n",
      " [8182]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ -0.06833186  -3.7964125   -3.897426    -5.578662  ]]\n",
      "\n",
      " [[ -0.00010992 -10.673865   -11.014789   -11.046569  ]]\n",
      "\n",
      " [[ -0.2778604   -1.9001234   -3.9369605   -4.0756793 ]]\n",
      "\n",
      " [[ -0.42270067  -2.0391405   -3.4403055   -3.8805673 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7971 8182 7958 4159]]\n",
      "\n",
      " [[8182  361 7964 7966]]\n",
      "\n",
      " [[7988 8182 7958 7971]]\n",
      "\n",
      " [[7971 7958 8182 7988]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [8182]\n",
      " [7958]\n",
      " [4159]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [8182]\n",
      " [7988]\n",
      " [7971]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.00005675 -11.11507    -11.688499   -11.892848  ]]\n",
      "\n",
      " [[ -1.0776081   -1.7499332   -3.0798817   -3.1284046 ]]\n",
      "\n",
      " [[ -0.00040533  -8.4682455   -9.787677   -11.135672  ]]\n",
      "\n",
      " [[ -0.00034183  -8.786888   -10.603842   -11.216458  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182  361 7966 7964]]\n",
      "\n",
      " [[7971 8025  582 7958]]\n",
      "\n",
      " [[8182 7958 7971 3209]]\n",
      "\n",
      " [[8182 7958 7971 7964]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [ 361]\n",
      " [7966]\n",
      " [7964]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [7971]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.54639137  -1.5188683   -2.5485466   -4.308942  ]]\n",
      "\n",
      " [[ -0.00028585  -8.752195   -11.079266   -11.098873  ]]\n",
      "\n",
      " [[ -0.005412    -5.5497046   -6.7867937   -9.3517885 ]]\n",
      "\n",
      " [[ -0.00116828  -7.4431124   -7.6806726  -10.371249  ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7971 8182 7958 8025]]\n",
      "\n",
      " [[8182  361 7964 7966]]\n",
      "\n",
      " [[8182 7958 7971 7988]]\n",
      "\n",
      " [[8182 7971 7958 7988]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [8182]\n",
      " [7958]\n",
      " [8025]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[7971]\n",
      " [8182]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.00004768 -11.305313   -12.011026   -12.130255  ]]\n",
      "\n",
      " [[ -0.43402833  -1.4259717   -2.9367597   -5.526912  ]]\n",
      "\n",
      " [[ -0.00045811  -8.385387    -9.946524   -10.735154  ]]\n",
      "\n",
      " [[ -0.1759128   -2.6560574   -2.7889729   -4.4964924 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182  361 7966 7964]]\n",
      "\n",
      " [[7971 8182 7958 8025]]\n",
      "\n",
      " [[8182 7958 7971 3209]]\n",
      "\n",
      " [[7971 8182 7958 7988]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [ 361]\n",
      " [7966]\n",
      " [7964]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [7971]\n",
      " [8182]\n",
      " [7971]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ -0.4465254   -1.1541784   -3.5461347   -5.798641  ]]\n",
      "\n",
      " [[ -0.00011886  -9.799079   -11.7931795  -11.947427  ]]\n",
      "\n",
      " [[ -0.00226599  -6.4105854   -7.98547     -9.699476  ]]\n",
      "\n",
      " [[ -0.00038321  -8.64213     -8.990904   -11.01123   ]]], shape=(4, 1, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[8182 7971 7958 7988]]\n",
      "\n",
      " [[8182  361 7966 7964]]\n",
      "\n",
      " [[8182 7958 7971 7988]]\n",
      "\n",
      " [[8182 7971 7958 7988]]], shape=(4, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [7971]\n",
      " [7958]\n",
      " [7988]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8182]\n",
      " [8182]\n",
      " [8182]\n",
      " [8182]], shape=(4, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8181 6394 7957 6018 1156  405 7971 8182 7971 8182 7957 1871 1223 7971\n",
      "  8182 8025 7971 8182 7971 8182 8182]\n",
      " [8181  123    2 6775  346 2285    2  361 7957  361 7971  698  988  118\n",
      "   361 7971 8182  361 8182  361 7971]\n",
      " [8181  254   47 6916    4 4543 7958 7964 8025 7964 8025 1435  306  251\n",
      "  7966 7957 7958 7966 7958 7966 7958]\n",
      " [8181   60 7971  185 2311 1309 7964 7966  588 7966    8    6 1290   81\n",
      "  7964 8040 4159 7964 8025 7964 7988]], shape=(4, 21), dtype=int32)\n",
      "Input: guten Morgen allerseits.\n",
      "Predicted translation: A , tomorrow allthing,    .fortuer .  .\n"
     ]
    }
   ],
   "source": [
    "translate(\"guten Morgen allerseits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: guten morgen allerseits.\n",
      "Predicted translation: Good tomorrow and everyone.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"guten morgen allerseits.\", plot='decoder_layer4_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
